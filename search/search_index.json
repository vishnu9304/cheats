{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"MY CHEATS"},{"location":"crypto/network_security/","text":"Basic Concepts Encryption: Conversion of plain text to cipher text. (Sender) Decryption: Conversion of cipher text to plain text. (Receiver) Cryptography: Study of Encryption Cryptanalysis: Study of Decryption Cryptology: Encryption + Decryption Encryption can be done in two ways: Stream Cipher: Conversion takes place bit by bit. (Recommended for short length messages Block Cipher: Conversion takes place block by block (grouping of bits), block size is determined by the type of algorithm. Two type of Encryption mechanism: Symmetric Encryption: Same key will be used for Encryption process and Decryption process (Secret key). Asymmetric Encryption: Two keys will be used that are mathematically linked (Public and Private keys). Note: It is possible to encrypt data with private key and decrypt it with public key. Types of Attacks Passive attacks: No modification will be done to the data by unauthorized user. Active attacks: Data will modified by unauthorized user. Passive attacks types: Eavesdropping - No modification will be done to the data, the attacker will just listen to the messages. Traffic Analysis - Just Analysing the traffic. Based on the traffic flow access will be gained. Active attacks types: Masquerade attacks - Receiver will receive the data from 3rd party in the name of sender. Replay attacks - Alice (A) sends her hashed password to Bob (B). Eve (E) sniffs the hash and replays it pretending like alice. Data Modification attacks - Sender sends the data, the attacker takes it and modifies it send it to receiver. Denial Of Service attacks - Disrupts the services provided by the server. Security Services Authentication - Acess control. Prevent un-authorized access. Authorization - Verification of user identity. Confidentiality - Securely sharing the data. Integrity - Make sure the data is not modified during transmission. Non-Repudiation. Substitution Techniques - The characters are replaced with corresponding cipher texts which are calulated mathematically (Vulnerable to attacks, suitable only for short length methods). Example: Caesar cipher Transposition Techniques - Re-arrange the order of bit positions. Example: Rail fence Cipher Columnar transposition FEISTEL STRUCTURE Most of the block cipher techniques follows a strcture called as FEISTEL STRUCTURE. FEISTEL STRUCTURE - Surf online for more info. Block ciper design priciples: block size key size number of rounds subkeys Symmentric Algorithms DES (Data Encyption Standard) AES (Advanced Encyption Standard) ASymmentric Algorithms RSA Diffe Hellman Key Exchange. This is not an Encryption algorithm. It is used only to exchange the secret/symmetric keys.","title":"Network Security"},{"location":"crypto/network_security/#basic-concepts","text":"Encryption: Conversion of plain text to cipher text. (Sender) Decryption: Conversion of cipher text to plain text. (Receiver) Cryptography: Study of Encryption Cryptanalysis: Study of Decryption Cryptology: Encryption + Decryption","title":"Basic Concepts"},{"location":"crypto/network_security/#encryption-can-be-done-in-two-ways","text":"Stream Cipher: Conversion takes place bit by bit. (Recommended for short length messages Block Cipher: Conversion takes place block by block (grouping of bits), block size is determined by the type of algorithm.","title":"Encryption can be done in two ways:"},{"location":"crypto/network_security/#two-type-of-encryption-mechanism","text":"Symmetric Encryption: Same key will be used for Encryption process and Decryption process (Secret key). Asymmetric Encryption: Two keys will be used that are mathematically linked (Public and Private keys). Note: It is possible to encrypt data with private key and decrypt it with public key.","title":"Two type of Encryption mechanism:"},{"location":"crypto/network_security/#types-of-attacks","text":"Passive attacks: No modification will be done to the data by unauthorized user. Active attacks: Data will modified by unauthorized user. Passive attacks types: Eavesdropping - No modification will be done to the data, the attacker will just listen to the messages. Traffic Analysis - Just Analysing the traffic. Based on the traffic flow access will be gained. Active attacks types: Masquerade attacks - Receiver will receive the data from 3rd party in the name of sender. Replay attacks - Alice (A) sends her hashed password to Bob (B). Eve (E) sniffs the hash and replays it pretending like alice. Data Modification attacks - Sender sends the data, the attacker takes it and modifies it send it to receiver. Denial Of Service attacks - Disrupts the services provided by the server.","title":"Types of Attacks"},{"location":"crypto/network_security/#security-services","text":"Authentication - Acess control. Prevent un-authorized access. Authorization - Verification of user identity. Confidentiality - Securely sharing the data. Integrity - Make sure the data is not modified during transmission. Non-Repudiation. Substitution Techniques - The characters are replaced with corresponding cipher texts which are calulated mathematically (Vulnerable to attacks, suitable only for short length methods). Example: Caesar cipher Transposition Techniques - Re-arrange the order of bit positions. Example: Rail fence Cipher Columnar transposition","title":"Security Services"},{"location":"crypto/network_security/#feistel-structure","text":"Most of the block cipher techniques follows a strcture called as FEISTEL STRUCTURE. FEISTEL STRUCTURE - Surf online for more info. Block ciper design priciples: block size key size number of rounds subkeys","title":"FEISTEL STRUCTURE"},{"location":"crypto/network_security/#symmentric-algorithms","text":"DES (Data Encyption Standard) AES (Advanced Encyption Standard)","title":"Symmentric Algorithms"},{"location":"crypto/network_security/#asymmentric-algorithms","text":"RSA Diffe Hellman Key Exchange. This is not an Encryption algorithm. It is used only to exchange the secret/symmetric keys.","title":"ASymmentric Algorithms"},{"location":"crypto/ssl/","text":"Certificate extensions An SSL Certificate is essentially an X.509 certificate, it defines the structure of the certificate. These certificate files will have different extensions based on the format and encoding they use. Types of encoding Binary Base64 ASCII Formats PEM uses Base64 ASCII encoding DER uses Binary encoding PEM file extensions (pem, crt, key, cer) The .pem file can include the server certificate, the intermediate certificate and the private key in a single file The server certificate and intermediate certificate can also be in a separate .crt or .cer file The private key can be in a .key file Each certificate in the PEM file is contained between the ---- BEGIN CERTIFICATE---- and ----END CERTIFICATE---- statements The private key is contained between the ---- BEGIN RSA PRIVATE KEY----- and -----END RSA PRIVATE KEY----- statements The CSR is contained between the -----BEGIN CERTIFICATE REQUEST----- and -----END CERTIFICATE REQUEST----- statements DER file extensions (der, cer) The DER certificates are in binary form, contained in .der or .cer files. These certificates are mainly used in Java-based web servers. OpenSSL commands Print the certificate in text form and don't print certificate output. openssl x509 -in server.crt -text -noout Verify Whether a Certificate and Private Key Match Verify Whether a Certificate and Private Key Match.To verify you need to print out md5 checksums and compare them. openssl x509 -noout -modulus -in server.crt| openssl md5 openssl rsa -noout -modulus -in server.key| openssl md5 Verify key and it's validity sudo openssl rsa -in server.key -noout -check RSA key ok Verify a Certificate was Signed by a CA openssl verify -verbose -CAFile ca.crt server.crt Convert PEM to DER openssl x509 -in server.crt -outform der -out server.der Convert DER to PEM openssl x509 -inform der -in server.der -out server.crt","title":"SSL"},{"location":"crypto/ssl/#certificate-extensions","text":"An SSL Certificate is essentially an X.509 certificate, it defines the structure of the certificate. These certificate files will have different extensions based on the format and encoding they use.","title":"Certificate extensions"},{"location":"crypto/ssl/#types-of-encoding","text":"Binary Base64 ASCII","title":"Types of encoding"},{"location":"crypto/ssl/#formats","text":"PEM uses Base64 ASCII encoding DER uses Binary encoding","title":"Formats"},{"location":"crypto/ssl/#pem-file-extensions-pem-crt-key-cer","text":"The .pem file can include the server certificate, the intermediate certificate and the private key in a single file The server certificate and intermediate certificate can also be in a separate .crt or .cer file The private key can be in a .key file Each certificate in the PEM file is contained between the ---- BEGIN CERTIFICATE---- and ----END CERTIFICATE---- statements The private key is contained between the ---- BEGIN RSA PRIVATE KEY----- and -----END RSA PRIVATE KEY----- statements The CSR is contained between the -----BEGIN CERTIFICATE REQUEST----- and -----END CERTIFICATE REQUEST----- statements","title":"PEM file extensions (pem, crt, key, cer)"},{"location":"crypto/ssl/#der-file-extensions-der-cer","text":"The DER certificates are in binary form, contained in .der or .cer files. These certificates are mainly used in Java-based web servers.","title":"DER file extensions (der, cer)"},{"location":"crypto/ssl/#openssl-commands","text":"Print the certificate in text form and don't print certificate output. openssl x509 -in server.crt -text -noout","title":"OpenSSL commands"},{"location":"crypto/ssl/#verify-whether-a-certificate-and-private-key-match","text":"Verify Whether a Certificate and Private Key Match.To verify you need to print out md5 checksums and compare them. openssl x509 -noout -modulus -in server.crt| openssl md5 openssl rsa -noout -modulus -in server.key| openssl md5","title":"Verify Whether a Certificate and Private Key Match"},{"location":"crypto/ssl/#verify-key-and-its-validity","text":"sudo openssl rsa -in server.key -noout -check RSA key ok","title":"Verify key and it's validity"},{"location":"crypto/ssl/#verify-a-certificate-was-signed-by-a-ca","text":"openssl verify -verbose -CAFile ca.crt server.crt","title":"Verify a Certificate was Signed by a CA"},{"location":"crypto/ssl/#convert-pem-to-der","text":"openssl x509 -in server.crt -outform der -out server.der","title":"Convert PEM to DER"},{"location":"crypto/ssl/#convert-der-to-pem","text":"openssl x509 -inform der -in server.der -out server.crt","title":"Convert DER to PEM"},{"location":"docker/docker_ca/","text":"Docker Edition Community Edition - Free Opensource Enterprise Edition - Paid version Installing Docker CE 1 - sudo yum install -y device-mapper-persistent-data lvm2 2 - sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 3 - sudo yum install -y docker-ce-18.09.5 docker-ce-cli-18.09.5 containerd.io 4 - sudo systemctl start docker 5 - sudo systemctl enable docker 6 - sudo useradd -a -G docker username // For allowing normal users to execute docker commands, add them under docker group. Selcting Storage Driver Command to find the storage driver: docker info Changing the default storage driver: 1 - Change it under \"/usr/lib/systemd/system/docker.service\" and add this flag: --storage-driver devicemapper ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ExecStart=/usr/bin/dockerd -H fd:// --storage-driver devicemapper --containerd=/run/container containerd.sock 2 - Daemon config file. (Recommended Way) /etc/docker/daemon.json { \"storage-driver\": \"devicemapper\" } When you start a container, a thin writable container layer is added on top of the other layers. Any changes the container makes to the filesystem are stored here. The major difference between a container and an image is the top writable layer. When the container is deleted, the writable layer is also deleted. The underlying image remains unchanged. Container Size On Disk: docker ps -s // https://docs.docker.com/storage/storagedriver/ Copy On Write: Copy the files to the writable layer only if the files are modifed. Running the docker Container: (docker run command) Some of the common falgs used with \"docker run\" command. Syntax: docker run [options] image[:tag] [command] [args] -d Run container in detached mode. --name For providing a descriptive name to the container. --restart options no - Never restart the container. on-failure - If container exits with non-zero exit code. always - Always restart the container whether it is successful or not. Also start the container on daemon restart. unless-stopped - It is similar to \"always\", Container will not be restarted, if you stop it explicitly. -p host_port : container_port Publish, expose a port inside the container by mapping it with host port. --rm Remove the container automatically when it exits. Not compatible with \"--restart\" option. --memory Memory Hard Limit. --memory-reservation A soft limit on memory usage. Container will be restricted within this memory if docker detects memory contention on the host. Logging Drivers https://docs.docker.com/config/containers/logging/configure/ Logging Drivers are a pluggable framework for accessing log data from services and container in docker. Configure the default logging driver under /etc/docker/daemon.json using options \"log-driver\" and \"log-opts\" (System Wide) { \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"15m\" } } How to override the settings at the container level. docker run --log-driver json-file --log-opt max-size=50m nginx Image Creation, Management, and Registry Docker Images: The image consists of one or more read-only layers, while the container adds one additional layer. The layered filesystem allows multiple images and containers to share the same layers. This results in 1 - Small overall footprint 2 - Faster image transfer 3 - Faster image build docker image pull IMAGE[:TAG] To pull the docker image. docker image history IMAGE To list the layers used by the image. Components Of Dockerfile https://docs.docker.com/engine/reference/builder/ A docker file is set of instructions which are used to construct docker image. These instructions are called directives. FROM Starts with a new build stage and sets the base image. ENV To set environment variables. RUN Create new filesystem layer by running a command. CMD Default command to run when the container is executed. EXPOSE Technically it will not expose any ports. Documents which ports are intended to be published when running a container. WORKDIR Sets the current working directory for subsequent directives such as ADD,COPY,CMD,ENTRYPOINT We can have multiple WORKDIR directives inside the docker file. WORKDIR /var WORKDIR www //Relative path WORKDIR html The above three WORKDIR directives are equivalent to /var/www/html //Absolute path COPY Copy files from local machine to the image ADD Similar to copy, but little advanced than COPY, like pulling files using URL and extract an archive into loose files in the image. STOPSIGNAL Specify the signal that will be used to stop the container. When you run docker container stop, this signal will be passed. HEALTHCHECK Used to specify a custom health check, to verify the container is running fine. HEALTHCHECK CMD curl localhost:80 docker build -t custom-nginx . docker run -d custom-nginx -p 8080:80 Example: FROM Centos7:latest ENV NGINX_VERSION=1.0.8 RUN yum update -y yum install -y curl RUN yum update -y yum install -y nginx=$NGINX_VERSION [ why \"yum update -y\" is added twice? When you rebuild the image by changing nginx version, it will first look for the line \"RUN yum update -y yum install -y curl\" since there is not change in the RUN directive it will use the same old layer. Note: Inorder to trigger a change to any layer, we should modify the RUN directive. ] CMD [\"nginx\", \"-g\", \"daemon off;\"] Building Efficient Images General tips: - Put things that are less likely to change on lower level layers. - Don't create unnecessary layers. - Avoid including any unnecessary files, packages, etc.. Docker Multistage builds: Docker supports ablity to perform multistage builds.It will have more than one FROM directive in the docker file with each FROM directive starting a new stage. Each stage begins with a completely new set of layers, allowing you to selectively copy only the files needed from previous layer. Example: Below steps will create an image size of 774MB FROM golang:1.12.4 WORKDIR /helloworld COPY helloworld.go . RUN GOOS=linux go build -a -installsuffix cgo -o helloworld . CMD [\"./helloworld\"] Example: Multistate build.This will produce only image size of 7MB. Idea is to keep only the required files not all, in our case we don't need the entire to go image to run our program. All we need is a binary. Create the binay in STAGE1 and move it to STAGE2 image with smaller size. FROM golang:1.12.4 AS compiler //STAGE1 WORKDIR /helloworld COPY helloworld.go . RUN GOOS=linux go build -a -installsuffix cgo -o helloworld . FROM alpine:3.9.3 //STAGE2 WORKDIR /root COPY --from=compiler /helloworld/helloworld . CMD [\"./helloworld\"] REPOSITORY TAG IMAGE ID CREATED SIZE gostage latest 3b3816104992 7 seconds ago 7.53MB go-custom latest 9802cc0d3ab8 10 minutes ago 776MB Managing Images docker pull To pull the images from registry, if not found locally. docker image ls To list images. docker image ls -a To list images including intermediate images. docker inspect image name To get more info about the images. Provides json output. docker inspect --format \"{{.Arch}} {{.Os}}\" --format (go template) to extract specific fields. docker image rm image name / docker rmi image name To remove the image. docker container ls -a / docker ps -a To list the containers. Dangling Images: Dangling Images are something which doesn't have tags and no containers referencing them. When we delete a container, it doesn't necessarily delete the uderlying image. It will delete only the tags not the image. So that image is called danglig image. Cleaning up the Dangling Images: docker image prune If you have any image which is not referenced by anything or any containers. This command will do a clean up. Flattening an Image: Docker doesn't provide an official way to do this. Run a container - docker export (export the container to an archive) - docker import (Import the archive as new image) docker export container flat.tar cat flat.tar | docker import - flat:latest Docker Storage: https://docs.docker.com/storage/ Storage drivers are also known as Graph Drivers. The proper storage driver to use often depends on your operating system. overlay2 Centos8 and RHEL versions aufs Ubuntu device mapper Centos7 and earlier Storage Models: Persistent data can be managed using several storage models. Filesystem storage: Data stored in the form of a file system. Used by overlay and aufs. Efficient use of memory. Inefficient with write-heavy workloads. Block storage: Stores data in block. Used by device mapper. Efficient with write-heavy workloads. Object storage: Stores data in an external object based store. Application must be designed to use object based storage. Flexible and scalable.","title":"Docker CA"},{"location":"docker/docker_ca/#docker-edition","text":"Community Edition - Free Opensource Enterprise Edition - Paid version","title":"Docker Edition"},{"location":"docker/docker_ca/#installing-docker-ce","text":"1 - sudo yum install -y device-mapper-persistent-data lvm2 2 - sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 3 - sudo yum install -y docker-ce-18.09.5 docker-ce-cli-18.09.5 containerd.io 4 - sudo systemctl start docker 5 - sudo systemctl enable docker 6 - sudo useradd -a -G docker username // For allowing normal users to execute docker commands, add them under docker group.","title":"Installing Docker CE"},{"location":"docker/docker_ca/#selcting-storage-driver","text":"Command to find the storage driver: docker info Changing the default storage driver: 1 - Change it under \"/usr/lib/systemd/system/docker.service\" and add this flag: --storage-driver devicemapper ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ExecStart=/usr/bin/dockerd -H fd:// --storage-driver devicemapper --containerd=/run/container containerd.sock 2 - Daemon config file. (Recommended Way) /etc/docker/daemon.json { \"storage-driver\": \"devicemapper\" } When you start a container, a thin writable container layer is added on top of the other layers. Any changes the container makes to the filesystem are stored here. The major difference between a container and an image is the top writable layer. When the container is deleted, the writable layer is also deleted. The underlying image remains unchanged. Container Size On Disk: docker ps -s // https://docs.docker.com/storage/storagedriver/ Copy On Write: Copy the files to the writable layer only if the files are modifed.","title":"Selcting Storage Driver"},{"location":"docker/docker_ca/#running-the-docker-container-docker-run-command","text":"Some of the common falgs used with \"docker run\" command. Syntax: docker run [options] image[:tag] [command] [args] -d Run container in detached mode. --name For providing a descriptive name to the container. --restart options no - Never restart the container. on-failure - If container exits with non-zero exit code. always - Always restart the container whether it is successful or not. Also start the container on daemon restart. unless-stopped - It is similar to \"always\", Container will not be restarted, if you stop it explicitly. -p host_port : container_port Publish, expose a port inside the container by mapping it with host port. --rm Remove the container automatically when it exits. Not compatible with \"--restart\" option. --memory Memory Hard Limit. --memory-reservation A soft limit on memory usage. Container will be restricted within this memory if docker detects memory contention on the host.","title":"Running the docker Container: (docker run command)"},{"location":"docker/docker_ca/#logging-drivers","text":"https://docs.docker.com/config/containers/logging/configure/ Logging Drivers are a pluggable framework for accessing log data from services and container in docker. Configure the default logging driver under /etc/docker/daemon.json using options \"log-driver\" and \"log-opts\" (System Wide) { \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"15m\" } } How to override the settings at the container level. docker run --log-driver json-file --log-opt max-size=50m nginx","title":"Logging Drivers"},{"location":"docker/docker_ca/#image-creation-management-and-registry","text":"Docker Images: The image consists of one or more read-only layers, while the container adds one additional layer. The layered filesystem allows multiple images and containers to share the same layers. This results in 1 - Small overall footprint 2 - Faster image transfer 3 - Faster image build docker image pull IMAGE[:TAG] To pull the docker image. docker image history IMAGE To list the layers used by the image.","title":"Image Creation, Management, and Registry"},{"location":"docker/docker_ca/#components-of-dockerfile","text":"https://docs.docker.com/engine/reference/builder/ A docker file is set of instructions which are used to construct docker image. These instructions are called directives. FROM Starts with a new build stage and sets the base image. ENV To set environment variables. RUN Create new filesystem layer by running a command. CMD Default command to run when the container is executed. EXPOSE Technically it will not expose any ports. Documents which ports are intended to be published when running a container. WORKDIR Sets the current working directory for subsequent directives such as ADD,COPY,CMD,ENTRYPOINT We can have multiple WORKDIR directives inside the docker file. WORKDIR /var WORKDIR www //Relative path WORKDIR html The above three WORKDIR directives are equivalent to /var/www/html //Absolute path COPY Copy files from local machine to the image ADD Similar to copy, but little advanced than COPY, like pulling files using URL and extract an archive into loose files in the image. STOPSIGNAL Specify the signal that will be used to stop the container. When you run docker container stop, this signal will be passed. HEALTHCHECK Used to specify a custom health check, to verify the container is running fine. HEALTHCHECK CMD curl localhost:80 docker build -t custom-nginx . docker run -d custom-nginx -p 8080:80 Example: FROM Centos7:latest ENV NGINX_VERSION=1.0.8 RUN yum update -y yum install -y curl RUN yum update -y yum install -y nginx=$NGINX_VERSION [ why \"yum update -y\" is added twice? When you rebuild the image by changing nginx version, it will first look for the line \"RUN yum update -y yum install -y curl\" since there is not change in the RUN directive it will use the same old layer. Note: Inorder to trigger a change to any layer, we should modify the RUN directive. ] CMD [\"nginx\", \"-g\", \"daemon off;\"]","title":"Components Of Dockerfile"},{"location":"docker/docker_ca/#building-efficient-images","text":"General tips: - Put things that are less likely to change on lower level layers. - Don't create unnecessary layers. - Avoid including any unnecessary files, packages, etc.. Docker Multistage builds: Docker supports ablity to perform multistage builds.It will have more than one FROM directive in the docker file with each FROM directive starting a new stage. Each stage begins with a completely new set of layers, allowing you to selectively copy only the files needed from previous layer. Example: Below steps will create an image size of 774MB FROM golang:1.12.4 WORKDIR /helloworld COPY helloworld.go . RUN GOOS=linux go build -a -installsuffix cgo -o helloworld . CMD [\"./helloworld\"] Example: Multistate build.This will produce only image size of 7MB. Idea is to keep only the required files not all, in our case we don't need the entire to go image to run our program. All we need is a binary. Create the binay in STAGE1 and move it to STAGE2 image with smaller size. FROM golang:1.12.4 AS compiler //STAGE1 WORKDIR /helloworld COPY helloworld.go . RUN GOOS=linux go build -a -installsuffix cgo -o helloworld . FROM alpine:3.9.3 //STAGE2 WORKDIR /root COPY --from=compiler /helloworld/helloworld . CMD [\"./helloworld\"] REPOSITORY TAG IMAGE ID CREATED SIZE gostage latest 3b3816104992 7 seconds ago 7.53MB go-custom latest 9802cc0d3ab8 10 minutes ago 776MB","title":"Building Efficient Images"},{"location":"docker/docker_ca/#managing-images","text":"docker pull To pull the images from registry, if not found locally. docker image ls To list images. docker image ls -a To list images including intermediate images. docker inspect image name To get more info about the images. Provides json output. docker inspect --format \"{{.Arch}} {{.Os}}\" --format (go template) to extract specific fields. docker image rm image name / docker rmi image name To remove the image. docker container ls -a / docker ps -a To list the containers.","title":"Managing Images"},{"location":"docker/docker_ca/#dangling-images","text":"Dangling Images are something which doesn't have tags and no containers referencing them. When we delete a container, it doesn't necessarily delete the uderlying image. It will delete only the tags not the image. So that image is called danglig image.","title":"Dangling Images:"},{"location":"docker/docker_ca/#cleaning-up-the-dangling-images","text":"docker image prune If you have any image which is not referenced by anything or any containers. This command will do a clean up.","title":"Cleaning up the Dangling Images:"},{"location":"docker/docker_ca/#flattening-an-image-docker-doesnt-provide-an-official-way-to-do-this","text":"Run a container - docker export (export the container to an archive) - docker import (Import the archive as new image) docker export container flat.tar cat flat.tar | docker import - flat:latest","title":"Flattening an Image: Docker doesn't provide an official way to do this."},{"location":"docker/docker_ca/#docker-storage-httpsdocsdockercomstorage","text":"Storage drivers are also known as Graph Drivers. The proper storage driver to use often depends on your operating system. overlay2 Centos8 and RHEL versions aufs Ubuntu device mapper Centos7 and earlier Storage Models: Persistent data can be managed using several storage models.","title":"Docker Storage: https://docs.docker.com/storage/"},{"location":"docker/docker_ca/#filesystem-storage","text":"Data stored in the form of a file system. Used by overlay and aufs. Efficient use of memory. Inefficient with write-heavy workloads.","title":"Filesystem storage:"},{"location":"docker/docker_ca/#block-storage","text":"Stores data in block. Used by device mapper. Efficient with write-heavy workloads.","title":"Block storage:"},{"location":"docker/docker_ca/#object-storage","text":"Stores data in an external object based store. Application must be designed to use object based storage. Flexible and scalable.","title":"Object storage:"},{"location":"golang/defer/","text":"Defer statement is used to execute a function call just before the function where the defer statement is present returns. Example: package main import \"fmt\" func Finished() { fmt.Println(\"Finished finding largest\") } func FindLargest(nums []int) { defer Finished() max := nums[0] for _, v := range nums { if v max { max = v } } fmt.Printf(\"The largest number in the given slice is %d\\n\", max) } func main() { var nums = []int{1, 2, 4, 5, 6} FindLargest(nums) } Stack of defers When a function has multiple defer calls, they are they are added on to a stack and executed in Last In First Out (LIFO) order. Example: package main import \"fmt\" func main() { name := \"vishnu\" fmt.Printf(\"Before Reversing string: %s\\n\", name) for _, v := range rune[](name) { defer fmt.Printf(\"%c\",v) } } The defere statement defer fmt.Printf(\"%c\",v) will be added to a stack like below and printed in LIFO order at the end of the main function execution. fmt.Printf(u) fmt.Printf(n) fmt.Printf(h) fmt.Printf(s) fmt.Printf(i) fmt.Printf(v) Hence the output of the above program will be a reversed string unshiv . Arguments evaluation The arguments of a deferred function are evaluated when the defer statement is executed and not when the actual function call is done. package main import ( \"fmt\" ) func printA(a int) { fmt.Println(\"value of a in deferred function\", a) } func main() { a := 5 defer printA(a) a = 10 fmt.Println(\"value of a before deferred function call\", a) } Output: value of a before deferred function call 10 value of a in deferred function 5 From the above output it can be understood that although the value of a changes to 10 after the defer statement is executed, the actual deferred function call printA(a) still prints 5.","title":"defer"},{"location":"golang/defer/#stack-of-defers","text":"When a function has multiple defer calls, they are they are added on to a stack and executed in Last In First Out (LIFO) order. Example: package main import \"fmt\" func main() { name := \"vishnu\" fmt.Printf(\"Before Reversing string: %s\\n\", name) for _, v := range rune[](name) { defer fmt.Printf(\"%c\",v) } } The defere statement defer fmt.Printf(\"%c\",v) will be added to a stack like below and printed in LIFO order at the end of the main function execution. fmt.Printf(u) fmt.Printf(n) fmt.Printf(h) fmt.Printf(s) fmt.Printf(i) fmt.Printf(v) Hence the output of the above program will be a reversed string unshiv .","title":"Stack of defers"},{"location":"golang/defer/#arguments-evaluation","text":"The arguments of a deferred function are evaluated when the defer statement is executed and not when the actual function call is done. package main import ( \"fmt\" ) func printA(a int) { fmt.Println(\"value of a in deferred function\", a) } func main() { a := 5 defer printA(a) a = 10 fmt.Println(\"value of a before deferred function call\", a) } Output: value of a before deferred function call 10 value of a in deferred function 5 From the above output it can be understood that although the value of a changes to 10 after the defer statement is executed, the actual deferred function call printA(a) still prints 5.","title":"Arguments evaluation"},{"location":"golang/error/","text":"Handling Errors The idiomatic way of handling error in Go is to compare the returned error to nil. A nil value indicates that no error has occurred and a non nil value indicates the presence of an error. Error type representation Error is an interface type with below definition. type error interface { Error() string } Note: To assert the underlying struct. Follow the below steps. When any method or function returns an error type. Check the underlying type to get more info fmt.Printf(\"%T\\n\", err) . package main import ( \"fmt\" \"os\" ) func main() { f, err := os.Open(\"/test.txt\") if err != nil { fmt.Println(err) fmt.Printf(\"%T\\n\", err) } fmt.Println(f.Name(), \"opened successfully\") }","title":"error"},{"location":"golang/error/#error-type-representation","text":"Error is an interface type with below definition. type error interface { Error() string } Note: To assert the underlying struct. Follow the below steps. When any method or function returns an error type. Check the underlying type to get more info fmt.Printf(\"%T\\n\", err) . package main import ( \"fmt\" \"os\" ) func main() { f, err := os.Open(\"/test.txt\") if err != nil { fmt.Println(err) fmt.Printf(\"%T\\n\", err) } fmt.Println(f.Name(), \"opened successfully\") }","title":"Error type representation"},{"location":"golang/go_tools/","text":"How to format the code? go fmt main.go go fmt -d main.go This command will show the difference. How to build a binary for windows (Cross compilation) architecture from Mac? GOOS=windows go build main.go What will go install do ? It will compile the code and install the resulting binary under $GOPATH/bin directory. What will go get do? go get github.com/golang/example/hello It will go to github.com download the code compliles it and place the binary under $GOPATH/bin Extracting info from go packages. go doc and go list Vishnus-Mac:test v01$ ls main.go Vishnus-Mac:test v01$ go list -f '{{ .Name }}: {{ .Doc }}: {{ .Imports }}' main: This is a demo!: [fmt] Vishnus-Mac:test v01$ go list -f '{{ join .Imports \"\\n\" }}' fmt errors internal/fmtsort io math os reflect strconv sync unicode/utf8 go list -f '{{ .Doc }}' fmt // To list the \"fmt\" package documentation. go doc go doc fmt // To check documentation for fmt package. godoc -http :6060 // To run http doc server. errcheck tool Run errcheck command under the directory where the go files are kept. It will display any unhandled errors. Refer: https://youtu.be/uBjoTxosSys","title":"gotools"},{"location":"golang/go_tools/#how-to-format-the-code","text":"go fmt main.go go fmt -d main.go This command will show the difference.","title":"How to format the code?"},{"location":"golang/go_tools/#how-to-build-a-binary-for-windows-cross-compilation-architecture-from-mac","text":"GOOS=windows go build main.go","title":"How to build a binary for windows (Cross compilation) architecture from Mac?"},{"location":"golang/go_tools/#what-will-go-install-do","text":"It will compile the code and install the resulting binary under $GOPATH/bin directory.","title":"What will go install do ?"},{"location":"golang/go_tools/#what-will-go-get-do","text":"go get github.com/golang/example/hello It will go to github.com download the code compliles it and place the binary under $GOPATH/bin","title":"What will go get do?"},{"location":"golang/go_tools/#extracting-info-from-go-packages-go-doc-and-go-list","text":"Vishnus-Mac:test v01$ ls main.go Vishnus-Mac:test v01$ go list -f '{{ .Name }}: {{ .Doc }}: {{ .Imports }}' main: This is a demo!: [fmt] Vishnus-Mac:test v01$ go list -f '{{ join .Imports \"\\n\" }}' fmt errors internal/fmtsort io math os reflect strconv sync unicode/utf8 go list -f '{{ .Doc }}' fmt // To list the \"fmt\" package documentation.","title":"Extracting info from go packages. go doc and go list"},{"location":"golang/go_tools/#go-doc","text":"go doc fmt // To check documentation for fmt package. godoc -http :6060 // To run http doc server.","title":"go doc"},{"location":"golang/go_tools/#errcheck-tool","text":"Run errcheck command under the directory where the go files are kept. It will display any unhandled errors. Refer: https://youtu.be/uBjoTxosSys","title":"errcheck tool"},{"location":"golang/gomod/","text":"What is a go module? A module is a way of packaging the software as simple as that. A go module is not atomic and it can contain many packages inside like a repository and it is versioned . Go follows something called Semantic Versioning to version the packages. Also called as semver. oh wait, then what is the difference between a package and a repository? A package cannot be sub divided in to working pieces. If you divide the package in to two different files, still you need those two files to represent the whole pacakge. A package is an atomic thing. A repository contains many packages that can move around independently. What is versioning? Why do we use it? How do we use it? Software changes over time. We need some kind of standard way of tracking these changes over period of time. That is where Semantic Versioning comes in to picture. It is a pre straight forward convention of how to name versions. Example: v1.0.0 v1 - Major Version for every single non backward compatible versions. v1.0 - Minor Version. We increase it when we add a new feature and not broken any backward Compatiblity. v1.0.0 - Patch updates. fixing things that has broken, like security bug fixes. Writing programs outside the GOPATH Command to check default gopath: go env GOPATH Example: package main import ( \"fmt\" \"log\" \"os\" \"github.com/sirupsen/logrus\" ) func main() { _, err := io.Copy(os.Stdout, os.Stdin) if err != { logrus.Fatal(err) } } Here github.com/sirupsen/logrus is a third party package. When you do go get it will complain that it is not able to find the gopath. It cannot store code outside gopath. Vishnus-Mac:mycat v01$ go get go get: no install location for directory /Users/v01/dev/mycat outside GOPATH For more details see: 'go help gopath' Go module comes to rescuse the above scenario. We are going to go from a directory having go code to directory having go module Vishnus-Mac:mycat v01$ go mod init go: cannot determine module path for source directory /Users/v01/dev/mycat (outside GOPATH, no import comments) Again it will complain as you are outside the gopath and it doesn't know what module you are initalizing. We need to provide the import path Vishnus-Mac:mycat v01$ go mod init github.com/vishnu9304/mycat go: creating new go.mod: module github.com/vishnu9304/mycat Vishnus-Mac:mycat v01$ cat go.mod module github.com/vishnu9304/mycat go 1.12 When you run go build it will pull all the dependencies and mentioned in our go code. Vishnus-Mac:mycat v01$ go build Vishnus-Mac:mycat v01$ ls go.mod go.sum main.go mycat Vishnus-Mac:mycat v01$ cat go.mod module github.com/vishnu9304/mycat go 1.12 require github.com/sirupsen/logrus v1.4.2 Vishnus-Mac:mycat v01$ cat go.sum github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38= github.com/konsorten/go-windows-terminal-sequences v1.0.1/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ= github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4= github.com/sirupsen/logrus v1.4.2 h1:SPIRibHv4MatM3XXNO2BJeFLZwZ2LvZgfQ5+UNI2im4= github.com/sirupsen/logrus v1.4.2/go.mod h1:tLMulIdttU9McNUspp0xgXVQah82FyeX6MwdIuYE2rE= github.com/stretchr/objx v0.1.1/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME= github.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs= golang.org/x/sys v0.0.0-20190422165155-953cdadca894 h1:Cz4ceDQGXuKRnVBDTS23GTn/pU5OE2C0WrNTOYK1Uuc= golang.org/x/sys v0.0.0-20190422165155-953cdadca894/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs= Under go.mod you can see a new line require github.com/sirupsen/logrus v1.4.2 . Our module depends on that module. You will see one more additional file go.sum which contains a lot of info. On top of the version we are using we also have a HASH J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38= It's a cryptographic hash to track if something that got changed over time. Commonly used commands Command to fetch a module with the latest tag: go get -u github.com/sirupsen/logrus Let's do some clean up: go mod tidy When you do a go ge t or go build your test files are ignored. go mod tidy will also find the modules that your tests are depending on. Where are these downloaded modules stores? It will be stored under GOPATH /go/pkg/mod How to identify why we are depending on some module? Vishnus-Mac:mycat v01$ go mod why github.com/pmezard/go-difflib # github.com/pmezard/go-difflib (main module does not need package github.com/pmezard/go-difflib) Actually we are not using github.com/pmezard/go-difflib directly. We are using some module inside it. To identify that we need to add -m flag. Find something that is in github.com/pmezard/go-difflib Vishnus-Mac:mycat v01$ go mod why -m github.com/pmezard/go-difflib # github.com/pmezard/go-difflib github.com/vishnu9304/mycat github.com/sirupsen/logrus github.com/sirupsen/logrus.test github.com/stretchr/testify/assert github.com/pmezard/go-difflib/difflib Migrating modules to use go mod. Step 1: Clone the repo. Step 2: go mod init Step 3: go mod tidy Step 4: go get ./...","title":"gomod"},{"location":"golang/gomod/#what-is-a-go-module","text":"A module is a way of packaging the software as simple as that. A go module is not atomic and it can contain many packages inside like a repository and it is versioned . Go follows something called Semantic Versioning to version the packages. Also called as semver.","title":"What is a go module?"},{"location":"golang/gomod/#oh-wait-then-what-is-the-difference-between-a-package-and-a-repository","text":"A package cannot be sub divided in to working pieces. If you divide the package in to two different files, still you need those two files to represent the whole pacakge. A package is an atomic thing. A repository contains many packages that can move around independently.","title":"oh wait, then what is the difference between a package and a repository?"},{"location":"golang/gomod/#what-is-versioning-why-do-we-use-it-how-do-we-use-it","text":"Software changes over time. We need some kind of standard way of tracking these changes over period of time. That is where Semantic Versioning comes in to picture. It is a pre straight forward convention of how to name versions.","title":"What is versioning? Why do we use it? How do we use it?"},{"location":"golang/gomod/#example","text":"v1.0.0 v1 - Major Version for every single non backward compatible versions. v1.0 - Minor Version. We increase it when we add a new feature and not broken any backward Compatiblity. v1.0.0 - Patch updates. fixing things that has broken, like security bug fixes.","title":"Example:"},{"location":"golang/gomod/#writing-programs-outside-the-gopath","text":"Command to check default gopath: go env GOPATH","title":"Writing programs outside the GOPATH"},{"location":"golang/gomod/#example_1","text":"package main import ( \"fmt\" \"log\" \"os\" \"github.com/sirupsen/logrus\" ) func main() { _, err := io.Copy(os.Stdout, os.Stdin) if err != { logrus.Fatal(err) } } Here github.com/sirupsen/logrus is a third party package. When you do go get it will complain that it is not able to find the gopath. It cannot store code outside gopath. Vishnus-Mac:mycat v01$ go get go get: no install location for directory /Users/v01/dev/mycat outside GOPATH For more details see: 'go help gopath' Go module comes to rescuse the above scenario. We are going to go from a directory having go code to directory having go module Vishnus-Mac:mycat v01$ go mod init go: cannot determine module path for source directory /Users/v01/dev/mycat (outside GOPATH, no import comments) Again it will complain as you are outside the gopath and it doesn't know what module you are initalizing. We need to provide the import path Vishnus-Mac:mycat v01$ go mod init github.com/vishnu9304/mycat go: creating new go.mod: module github.com/vishnu9304/mycat Vishnus-Mac:mycat v01$ cat go.mod module github.com/vishnu9304/mycat go 1.12 When you run go build it will pull all the dependencies and mentioned in our go code. Vishnus-Mac:mycat v01$ go build Vishnus-Mac:mycat v01$ ls go.mod go.sum main.go mycat Vishnus-Mac:mycat v01$ cat go.mod module github.com/vishnu9304/mycat go 1.12 require github.com/sirupsen/logrus v1.4.2 Vishnus-Mac:mycat v01$ cat go.sum github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38= github.com/konsorten/go-windows-terminal-sequences v1.0.1/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ= github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4= github.com/sirupsen/logrus v1.4.2 h1:SPIRibHv4MatM3XXNO2BJeFLZwZ2LvZgfQ5+UNI2im4= github.com/sirupsen/logrus v1.4.2/go.mod h1:tLMulIdttU9McNUspp0xgXVQah82FyeX6MwdIuYE2rE= github.com/stretchr/objx v0.1.1/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME= github.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs= golang.org/x/sys v0.0.0-20190422165155-953cdadca894 h1:Cz4ceDQGXuKRnVBDTS23GTn/pU5OE2C0WrNTOYK1Uuc= golang.org/x/sys v0.0.0-20190422165155-953cdadca894/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs= Under go.mod you can see a new line require github.com/sirupsen/logrus v1.4.2 . Our module depends on that module. You will see one more additional file go.sum which contains a lot of info. On top of the version we are using we also have a HASH J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38= It's a cryptographic hash to track if something that got changed over time.","title":"Example:"},{"location":"golang/gomod/#commonly-used-commands","text":"Command to fetch a module with the latest tag: go get -u github.com/sirupsen/logrus Let's do some clean up: go mod tidy When you do a go ge t or go build your test files are ignored. go mod tidy will also find the modules that your tests are depending on.","title":"Commonly used commands"},{"location":"golang/gomod/#where-are-these-downloaded-modules-stores","text":"It will be stored under GOPATH /go/pkg/mod","title":"Where are these downloaded modules stores?"},{"location":"golang/gomod/#how-to-identify-why-we-are-depending-on-some-module","text":"Vishnus-Mac:mycat v01$ go mod why github.com/pmezard/go-difflib # github.com/pmezard/go-difflib (main module does not need package github.com/pmezard/go-difflib) Actually we are not using github.com/pmezard/go-difflib directly. We are using some module inside it. To identify that we need to add -m flag. Find something that is in github.com/pmezard/go-difflib Vishnus-Mac:mycat v01$ go mod why -m github.com/pmezard/go-difflib # github.com/pmezard/go-difflib github.com/vishnu9304/mycat github.com/sirupsen/logrus github.com/sirupsen/logrus.test github.com/stretchr/testify/assert github.com/pmezard/go-difflib/difflib","title":"How to identify why we are depending on some module?"},{"location":"golang/gomod/#migrating-modules-to-use-go-mod","text":"Step 1: Clone the repo. Step 2: go mod init Step 3: go mod tidy Step 4: go get ./...","title":"Migrating modules to use go mod."},{"location":"linux/CPU and Load Average/","text":"CPU's and Load Averages Key points about CPU's: CPU's comes with a rated speed and it is calculates in Hertz (1 Hertz=1 Cycles Per Second). Example: 1.6GHZ (1.6 Billons Per Second). Motherboard crystals play a major role in driving the CPU speed. Key point to note here is, the mother board crystal oscillates at some frequency (in MHZ mostly). CPU clock speed = cpu multiplier x Mother Board Clock Speed. Overclocking is the process of pushing the CPU beyond it's rated speed. -bash-4.1$ lscpu Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian CPU(s): 64 On-line CPU(s) list: 0-63 Thread(s) per core: 2 Core(s) per socket: 16 Socket(s): 2 NUMA node(s): 2 Vendor ID: GenuineIntel CPU family: 6 Model: 79 Model name: Intel(R) Xeon(R) CPU E5-2683 v4 @ 2.10GHz From above data, we have 2 cpu sockets , each processor is having 16 cores and 2 threads per core (hyperthreading enabled). Note: hyperthreading is only for intel processors. CPU Caches: Programs resides in Hard drive. When you double click on any program some of the files will be copied from Hard drive to RAM. Accessing data from RAM is lot faster than Hard drive but it is still quite slower than CPU itself. There is a static RAM inside the CPU itself, we call it as caches (L1, L2, L3). L1 caches are the smallest and fastest cache. Understanding Linux CPU Load averages: Source: https://scoutapm.com/blog/understanding-load-averages cpu load average: The number of processes currently running plus the number of processes yet to be processed. Unix refers to this as the run-queue length: the sum of the number of processes that are currently running plus the number that are waiting (queued) to run. \"Need to Look into it\" If your load average is staying above 0.70, it's time to investigate before things get worse. \"Fix this now\" If your load average stays above 1.00, find the problem and fix it now. \"Arrgh, it's 3AM WTF?\" If your load average is above 5.00, you could be in serious trouble, your box is either hanging or slowing way down, and this will (inexplicably) happen in the worst possible time like in the middle of the night or when you're presenting at a conference. On multi-processor system, the load is relative to the number of processor cores available. The \"100% utilization\" mark is 1.00 on a single-core system, 2.00, on a dual-core, 4.00 on a quad-core, etc. Rule of thumb: The \"number of cores = max load\" Rule of Thumb: on a multicore system, your load should not exceed the number of cores available. # of cores is important to interpreting load averages ... how do I know how many cores my system has? cat /proc/cpuinfo to get info on each processor in your system.","title":"CPU's"},{"location":"linux/CPU and Load Average/#cpus-and-load-averages","text":"","title":"CPU's and Load Averages"},{"location":"linux/CPU and Load Average/#key-points-about-cpus","text":"CPU's comes with a rated speed and it is calculates in Hertz (1 Hertz=1 Cycles Per Second). Example: 1.6GHZ (1.6 Billons Per Second). Motherboard crystals play a major role in driving the CPU speed. Key point to note here is, the mother board crystal oscillates at some frequency (in MHZ mostly). CPU clock speed = cpu multiplier x Mother Board Clock Speed. Overclocking is the process of pushing the CPU beyond it's rated speed. -bash-4.1$ lscpu Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian CPU(s): 64 On-line CPU(s) list: 0-63 Thread(s) per core: 2 Core(s) per socket: 16 Socket(s): 2 NUMA node(s): 2 Vendor ID: GenuineIntel CPU family: 6 Model: 79 Model name: Intel(R) Xeon(R) CPU E5-2683 v4 @ 2.10GHz From above data, we have 2 cpu sockets , each processor is having 16 cores and 2 threads per core (hyperthreading enabled). Note: hyperthreading is only for intel processors. CPU Caches: Programs resides in Hard drive. When you double click on any program some of the files will be copied from Hard drive to RAM. Accessing data from RAM is lot faster than Hard drive but it is still quite slower than CPU itself. There is a static RAM inside the CPU itself, we call it as caches (L1, L2, L3). L1 caches are the smallest and fastest cache.","title":"Key points about CPU's:"},{"location":"linux/CPU and Load Average/#understanding-linux-cpu-load-averages","text":"Source: https://scoutapm.com/blog/understanding-load-averages cpu load average: The number of processes currently running plus the number of processes yet to be processed. Unix refers to this as the run-queue length: the sum of the number of processes that are currently running plus the number that are waiting (queued) to run. \"Need to Look into it\" If your load average is staying above 0.70, it's time to investigate before things get worse. \"Fix this now\" If your load average stays above 1.00, find the problem and fix it now. \"Arrgh, it's 3AM WTF?\" If your load average is above 5.00, you could be in serious trouble, your box is either hanging or slowing way down, and this will (inexplicably) happen in the worst possible time like in the middle of the night or when you're presenting at a conference. On multi-processor system, the load is relative to the number of processor cores available. The \"100% utilization\" mark is 1.00 on a single-core system, 2.00, on a dual-core, 4.00 on a quad-core, etc. Rule of thumb: The \"number of cores = max load\" Rule of Thumb: on a multicore system, your load should not exceed the number of cores available. # of cores is important to interpreting load averages ... how do I know how many cores my system has? cat /proc/cpuinfo to get info on each processor in your system.","title":"Understanding Linux CPU Load averages:"},{"location":"linux/Filesystem/","text":"Filesystems Display filesystem mounting information How to display mount information? mount /dev/vda1 on / type ext4 (rw) proc on /proc type proc (rw) sysfs on /sys type sysfs (rw) devpts on /dev/pts type devpts (rw,gid=5,mode=620) tmpfs on /dev/shm type tmpfs (rw) none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw) Where does the mount command gets the above information? I get the details from the file /etc/mtab This file will get updated whenever a mount point gets mounted and un-mounted. How to skip updating /etc/mtab file? mount -n (-n option) Note: /etc/mtab file will not have the up-to-date information about the mount points. The updated information about the mount points will be present under /proc/mounts Mounting and Un-Mounting Filesystems How to check which process is using files and directories on a mounted partitions? fuser -v / (-v verbose) fuser -k /mnt/data (identify and kill all the processes using the mount point \"/mnt/data\") How to un-mount and force un-mount a file system? unmount /mnt/data (Clean unmount) unmount -f /mnt/data (Force unmount) How to mount filesystem? mount -t ext3 /dev/xvdj1 /mnt/data How to remount a filesystem? mount -o remount,ro /mnt/data Mounting and Unmounting filesystems Automatically with /etc/fstab Entry inside fstab source destination fstype options /dev/xvdj1 /mnt/data ext3 defaults 0 0 How to label the drives? sudo e2label /dev/xvdj1 mydata go to /etc/fstab and add an entry like below. LABEL=mydata /mnt/data ext3 defaults 0 0","title":"Filesystems"},{"location":"linux/Filesystem/#filesystems","text":"","title":"Filesystems"},{"location":"linux/Filesystem/#display-filesystem-mounting-information","text":"How to display mount information? mount /dev/vda1 on / type ext4 (rw) proc on /proc type proc (rw) sysfs on /sys type sysfs (rw) devpts on /dev/pts type devpts (rw,gid=5,mode=620) tmpfs on /dev/shm type tmpfs (rw) none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw) Where does the mount command gets the above information? I get the details from the file /etc/mtab This file will get updated whenever a mount point gets mounted and un-mounted. How to skip updating /etc/mtab file? mount -n (-n option) Note: /etc/mtab file will not have the up-to-date information about the mount points. The updated information about the mount points will be present under /proc/mounts","title":"Display filesystem mounting information"},{"location":"linux/Filesystem/#mounting-and-un-mounting-filesystems","text":"How to check which process is using files and directories on a mounted partitions? fuser -v / (-v verbose) fuser -k /mnt/data (identify and kill all the processes using the mount point \"/mnt/data\") How to un-mount and force un-mount a file system? unmount /mnt/data (Clean unmount) unmount -f /mnt/data (Force unmount) How to mount filesystem? mount -t ext3 /dev/xvdj1 /mnt/data How to remount a filesystem? mount -o remount,ro /mnt/data","title":"Mounting and Un-Mounting Filesystems"},{"location":"linux/Filesystem/#mounting-and-unmounting-filesystems-automatically-with-etcfstab","text":"Entry inside fstab source destination fstype options /dev/xvdj1 /mnt/data ext3 defaults 0 0 How to label the drives? sudo e2label /dev/xvdj1 mydata go to /etc/fstab and add an entry like below. LABEL=mydata /mnt/data ext3 defaults 0 0","title":"Mounting and Unmounting filesystems Automatically with /etc/fstab"},{"location":"linux/Measure and Troubleshoot Resource Utilization/","text":"Measure and Troubleshoot Resource Utilization uptime The current time, how long the system has been running, how many users are currently logged on, and the system load averages for the past 1, 5, and 15 minutes. 05:25:34 up 0 min, 1 user, load average: 0.39, 0.10, 0.03 Note: Uptime depends on binary formatted file \"/var/run/utmp\". iostat Report Central Processing Unit (CPU) statistics and input/output statistics for devices, partitions and network filesystems (NFS). The iostat command generates three types of reports: CPU Utilization report Device Utilization report Network Filesystem report Display only CPU Utilization report: iostat -c Display only disk/block device stats: iostat -d Display CPU Utilization report at regular interval for 'n' number of times: iostat -c 2 5 (Every 2 seconds for 5 times) Note: refer man pages if you don't understand the keywords displayed in the output. Example: %iowait, the percentage of time that the CPU or CPUs were idle during which the system had an outstanding disk I/O request. sar sar - Collect, report, or save system activity information. sar provides historical stats on disk utilization for block devices and CPU. Display CPU stats: sar | head Display block devices stats: sar -d Note: sar command is provided by sysstat package (provides iostat aswell). When you install sysstat you have to start and enable it. When you start sysstat utility it creates a file under /var/log/sa/sa[num] . This is where it stores all the stats for every 10mins. [num] two digit number represents the date. It will be recycled every 30days . /var/log/sa/sa[num] is binay formatted. sar is desined to read this file and generate reports. Memory: free free Display amount of free and used memory in the system. free -m -h -s 1 Display the output in MB / Shorten / Display the results for seconds until three iterations. free -m [v01@equatescrates ~]$ free -m total used free shared buffers cached Mem: 3705 807 2897 0 61 438 -/+ buffers/cache: 306 3398 Swap: 0 0 0 According to above output we have only have 2897 mb of memory free. But, actual available free memory is free(2897) + cached(438) + buffers(61) = 3398 Major Page Fault: is a request done to fetch pages from the hard disk and buffer it to RAM. Minor Page Fault: If the data is present in the RAM buffer cache, the processor can issue a Minor Page fault. https://www.slashroot.in/linux-system-io-monitoring article explains the logic behind the free command. Memory: vmstat vmstat provides full summary of all memory utilization (idle memory/buffer memory etc..) on our system. procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 1991612 262060 871784 0 0 35 30 178 263 2 1 95 2 0 Display active/inactive memory: vmstat -a Active Memory: The memory that is being used by a particular process. Inactive Memory: The pages which have not been accessed \"recently\". Run every 5 seconds: vmstat 5 Run every 5 seconds with a timestamp: vmstat 5 -t Display a table of various event counters and memory statistics: vmstat -s Disk and Files: lsof Example: Let's say you have identified busy disk or high cpu utilization using above discussed command. To find what files, utilities or applications are open and might be using that. We can look at that by system and end users. Display number of file currently open: lsof | wc -l Display number of files that are opened by a particular user: lsof -u [user] | wc -l ps, pstree, top How do I get detailed information about process running on our system? Display all running processes on the system: ps -e Diplay full format: ps -ef Display the process hierarchy: pstree Display the process hierarchy and highlight the current process: pstree -h Display the process hierarchy along with pid's and highlight the current process: pstree -h -p Display the process hierarchy along with pid's and highlight the current process along with command line parameters: pstree -h -p -a Note: All of the commands are great. But they are not interactive. top command used to get process information in an interactive way. Before that we need to understand what a load average is? and what are tasks? Notes for top command : There are flags that are used to change the behaviour of the top command. Search online for more flags. Press \"f\" to add or remove fields \"shift + R\" to sort it in reverse, the process with less cpu time first. Press \"n\" then 5 for top five process. Press \"d\" to change the update duration. Press \"k\" to kill and enter the \"PID\" number to kill a process. You can pass any valid linux kill signal (default is 15). iotop, ss, netstat ss command is similar to netstat, the ss command is used to dump the socket statistics. netstat uses /proc/net/tcp to gather system network information, while ss queries the kernel directly using the Netlink socket interface.s iotop: The top command shows the Memory and CPU usage for open programs, iotop shows disk usage information for open processes. To run on non-interactive mode: iotop -b (or) iotop --batch To set the number of interactions before quitting: iotop -N 10 (or) iotop --iter=10 To set the delay between interactions: iotop -d 10 (or) iotop --delay=10 To monitor particular processes: iotop -p 1234 2345 3456 (or) iotop --pid=1234 To monitor a list of users: iotop -u mike mark (or) iotop --user=mike To show only processes: iotop -P (or) iotop --processes To Show accumulated I/O instead of bandwidth: iotop -a (or) iotop --accumulated To Use kilobytes instead of a human friendly unit: iotop -k (or) iotop --kilobytes To add a time stamp on each line: iotop -t (or) iotop --time To suppress some lines of header: iotop -q (or) iotop --quiet","title":"Troubleshoot Resource Utilization"},{"location":"linux/Measure and Troubleshoot Resource Utilization/#measure-and-troubleshoot-resource-utilization","text":"","title":"Measure and Troubleshoot Resource Utilization"},{"location":"linux/Measure and Troubleshoot Resource Utilization/#uptime","text":"The current time, how long the system has been running, how many users are currently logged on, and the system load averages for the past 1, 5, and 15 minutes. 05:25:34 up 0 min, 1 user, load average: 0.39, 0.10, 0.03 Note: Uptime depends on binary formatted file \"/var/run/utmp\".","title":"uptime"},{"location":"linux/Measure and Troubleshoot Resource Utilization/#iostat","text":"Report Central Processing Unit (CPU) statistics and input/output statistics for devices, partitions and network filesystems (NFS). The iostat command generates three types of reports: CPU Utilization report Device Utilization report Network Filesystem report Display only CPU Utilization report: iostat -c Display only disk/block device stats: iostat -d Display CPU Utilization report at regular interval for 'n' number of times: iostat -c 2 5 (Every 2 seconds for 5 times) Note: refer man pages if you don't understand the keywords displayed in the output. Example: %iowait, the percentage of time that the CPU or CPUs were idle during which the system had an outstanding disk I/O request.","title":"iostat"},{"location":"linux/Measure and Troubleshoot Resource Utilization/#sar","text":"sar - Collect, report, or save system activity information. sar provides historical stats on disk utilization for block devices and CPU. Display CPU stats: sar | head Display block devices stats: sar -d Note: sar command is provided by sysstat package (provides iostat aswell). When you install sysstat you have to start and enable it. When you start sysstat utility it creates a file under /var/log/sa/sa[num] . This is where it stores all the stats for every 10mins. [num] two digit number represents the date. It will be recycled every 30days . /var/log/sa/sa[num] is binay formatted. sar is desined to read this file and generate reports.","title":"sar"},{"location":"linux/Measure and Troubleshoot Resource Utilization/#memory-free","text":"free Display amount of free and used memory in the system. free -m -h -s 1 Display the output in MB / Shorten / Display the results for seconds until three iterations. free -m [v01@equatescrates ~]$ free -m total used free shared buffers cached Mem: 3705 807 2897 0 61 438 -/+ buffers/cache: 306 3398 Swap: 0 0 0 According to above output we have only have 2897 mb of memory free. But, actual available free memory is free(2897) + cached(438) + buffers(61) = 3398 Major Page Fault: is a request done to fetch pages from the hard disk and buffer it to RAM. Minor Page Fault: If the data is present in the RAM buffer cache, the processor can issue a Minor Page fault. https://www.slashroot.in/linux-system-io-monitoring article explains the logic behind the free command.","title":"Memory: free"},{"location":"linux/Measure and Troubleshoot Resource Utilization/#memory-vmstat","text":"vmstat provides full summary of all memory utilization (idle memory/buffer memory etc..) on our system. procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 1991612 262060 871784 0 0 35 30 178 263 2 1 95 2 0 Display active/inactive memory: vmstat -a Active Memory: The memory that is being used by a particular process. Inactive Memory: The pages which have not been accessed \"recently\". Run every 5 seconds: vmstat 5 Run every 5 seconds with a timestamp: vmstat 5 -t Display a table of various event counters and memory statistics: vmstat -s","title":"Memory: vmstat"},{"location":"linux/Measure and Troubleshoot Resource Utilization/#disk-and-files-lsof","text":"Example: Let's say you have identified busy disk or high cpu utilization using above discussed command. To find what files, utilities or applications are open and might be using that. We can look at that by system and end users. Display number of file currently open: lsof | wc -l Display number of files that are opened by a particular user: lsof -u [user] | wc -l","title":"Disk and Files: lsof"},{"location":"linux/Measure and Troubleshoot Resource Utilization/#ps-pstree-top","text":"How do I get detailed information about process running on our system? Display all running processes on the system: ps -e Diplay full format: ps -ef Display the process hierarchy: pstree Display the process hierarchy and highlight the current process: pstree -h Display the process hierarchy along with pid's and highlight the current process: pstree -h -p Display the process hierarchy along with pid's and highlight the current process along with command line parameters: pstree -h -p -a Note: All of the commands are great. But they are not interactive. top command used to get process information in an interactive way. Before that we need to understand what a load average is? and what are tasks? Notes for top command : There are flags that are used to change the behaviour of the top command. Search online for more flags. Press \"f\" to add or remove fields \"shift + R\" to sort it in reverse, the process with less cpu time first. Press \"n\" then 5 for top five process. Press \"d\" to change the update duration. Press \"k\" to kill and enter the \"PID\" number to kill a process. You can pass any valid linux kill signal (default is 15).","title":"ps, pstree, top"},{"location":"linux/Measure and Troubleshoot Resource Utilization/#iotop-ss-netstat","text":"ss command is similar to netstat, the ss command is used to dump the socket statistics. netstat uses /proc/net/tcp to gather system network information, while ss queries the kernel directly using the Netlink socket interface.s iotop: The top command shows the Memory and CPU usage for open programs, iotop shows disk usage information for open processes. To run on non-interactive mode: iotop -b (or) iotop --batch To set the number of interactions before quitting: iotop -N 10 (or) iotop --iter=10 To set the delay between interactions: iotop -d 10 (or) iotop --delay=10 To monitor particular processes: iotop -p 1234 2345 3456 (or) iotop --pid=1234 To monitor a list of users: iotop -u mike mark (or) iotop --user=mike To show only processes: iotop -P (or) iotop --processes To Show accumulated I/O instead of bandwidth: iotop -a (or) iotop --accumulated To Use kilobytes instead of a human friendly unit: iotop -k (or) iotop --kilobytes To add a time stamp on each line: iotop -t (or) iotop --time To suppress some lines of header: iotop -q (or) iotop --quiet","title":"iotop, ss, netstat"},{"location":"networking/Linux Networking/","text":"Linux Networking and Troubleshooting Using the Network Environment Interface Tools (ip,nmcli etc..) MACAddress Unique fingerprint of the network interface IP Address Unique address on the network Subnet Separate the IP into network and host addresses Gateway The connection leading outside of the local network DNS Host Translate the host name into IP address DNS Domain The lookup for the host","title":"Networking"},{"location":"networking/Linux Networking/#linux-networking-and-troubleshooting","text":"","title":"Linux Networking and Troubleshooting"},{"location":"networking/Linux Networking/#using-the-network-environment-interface-tools-ipnmcli-etc","text":"MACAddress Unique fingerprint of the network interface IP Address Unique address on the network Subnet Separate the IP into network and host addresses Gateway The connection leading outside of the local network DNS Host Translate the host name into IP address DNS Domain The lookup for the host","title":"Using the Network Environment Interface Tools (ip,nmcli etc..)"},{"location":"networking/TCP connection backlog/","text":"Linux TCP Backlogs References: https://bunn.cc/2017/syn-backlog/ https://blog.cloudflare.com/syn-packet-handling-in-the-wild/ https://eklitzke.org/how-tcp-sockets-work Key Points 1 - The incoming connections get queued up, waiting for the application to accept(2) them. 2 - The queue that holds established connections is (of course) of finite length. 3 - Each bound socket, in the \"LISTENING\" TCP state has two separate queues. The SYN Queue: The SYN Queue stores inbound SYN packets. It's responsible for sending out SYN+ACK packets and retrying them on timeout. On Linux the number of retries is configured with: sysctl net.ipv4.tcp_synack_retries The Accept Queue: The Accept Queue contains fully established connections and ready to be picked up by the application. What will happen if Accept Queue is full? If the TCP implementation in Linux receives the ACK packet of the 3-way handshake and the accept queue is full, it will basically ignore that packet. Note: There is a timer associated with the SYN RECEIVED state. If the ACK packet is not received (or if it is ignored, as in the case considered here), then the TCP implementation will resend the SYN/ACK packet (with a certain number of retries specified by /proc/sys/net/ipv4/tcp_synack_retries and using an exponential backoff algorithm). After retries it will sent RST packet. What is Half Open Connections? If the client first waits for data from the server and the server never reduces the backlog, then the end result is that on the client side, the connection is in state ESTABLISHED, while on the server side, the connection is considered CLOSED. This means that we end up with a half-open connection! Note: If the accept queue is full, then the kernel will impose a limit on the rate at which SYN packets are accepted. If too many SYN packets are received, some of them will be dropped. Queue size limits The maximum allowed length of both the Accept and SYN Queues is taken from the backlog parameter passed to the listen(2) syscall by the application. For example, this sets the Accept and SYN Queue sizes to 1,024: listen(sfd, 1024) This SYN Queue cap used to be configured by the net.ipv4.tcp_max_syn_backlog toggle, but this isn't the case anymore. Nowadays net.core.somaxconn caps both queue sizes. Troubleshooting: To peek into the SYN Queue on Linux we can use the ss command and look for SYN-RECV sockets. $ ss -n state syn-recv sport = :80 | wc -l 119 $ ss -n state syn-recv sport = :443 | wc -l 78 Note: don't confuse Recv-Q and Send-Q fields with the SYN Queue and ACCEPT Queue. SYN Queue and ACCEPT Queues are used per application. Recv-Q and Send-Q are used per TCP connection.","title":"TCP Backlogs"},{"location":"networking/TCP connection backlog/#linux-tcp-backlogs","text":"References: https://bunn.cc/2017/syn-backlog/ https://blog.cloudflare.com/syn-packet-handling-in-the-wild/ https://eklitzke.org/how-tcp-sockets-work","title":"Linux TCP Backlogs"},{"location":"networking/TCP connection backlog/#key-points","text":"1 - The incoming connections get queued up, waiting for the application to accept(2) them. 2 - The queue that holds established connections is (of course) of finite length. 3 - Each bound socket, in the \"LISTENING\" TCP state has two separate queues. The SYN Queue: The SYN Queue stores inbound SYN packets. It's responsible for sending out SYN+ACK packets and retrying them on timeout. On Linux the number of retries is configured with: sysctl net.ipv4.tcp_synack_retries The Accept Queue: The Accept Queue contains fully established connections and ready to be picked up by the application.","title":"Key Points"},{"location":"networking/TCP connection backlog/#what-will-happen-if-accept-queue-is-full","text":"If the TCP implementation in Linux receives the ACK packet of the 3-way handshake and the accept queue is full, it will basically ignore that packet. Note: There is a timer associated with the SYN RECEIVED state. If the ACK packet is not received (or if it is ignored, as in the case considered here), then the TCP implementation will resend the SYN/ACK packet (with a certain number of retries specified by /proc/sys/net/ipv4/tcp_synack_retries and using an exponential backoff algorithm). After retries it will sent RST packet.","title":"What will happen if Accept Queue is full?"},{"location":"networking/TCP connection backlog/#what-is-half-open-connections","text":"If the client first waits for data from the server and the server never reduces the backlog, then the end result is that on the client side, the connection is in state ESTABLISHED, while on the server side, the connection is considered CLOSED. This means that we end up with a half-open connection! Note: If the accept queue is full, then the kernel will impose a limit on the rate at which SYN packets are accepted. If too many SYN packets are received, some of them will be dropped.","title":"What is Half Open Connections?"},{"location":"networking/TCP connection backlog/#queue-size-limits","text":"The maximum allowed length of both the Accept and SYN Queues is taken from the backlog parameter passed to the listen(2) syscall by the application. For example, this sets the Accept and SYN Queue sizes to 1,024: listen(sfd, 1024) This SYN Queue cap used to be configured by the net.ipv4.tcp_max_syn_backlog toggle, but this isn't the case anymore. Nowadays net.core.somaxconn caps both queue sizes.","title":"Queue size limits"},{"location":"networking/TCP connection backlog/#troubleshooting","text":"To peek into the SYN Queue on Linux we can use the ss command and look for SYN-RECV sockets. $ ss -n state syn-recv sport = :80 | wc -l 119 $ ss -n state syn-recv sport = :443 | wc -l 78 Note: don't confuse Recv-Q and Send-Q fields with the SYN Queue and ACCEPT Queue. SYN Queue and ACCEPT Queues are used per application. Recv-Q and Send-Q are used per TCP connection.","title":"Troubleshooting:"},{"location":"web/http/","text":"Fragments and Queries in HTTP url: Example: https://www.yahoo.com/search?q=tesla%20research Query: search?q=tesla%20research Example: https://www.yahoo.com/search?q=tesla%20research#space%20research Query: #space research Which means under \"q=tesla%20research\" resource scroll to \"#space\" section. HTTP Request Methods: [GET] Retreive a resource [POST] Update a resource [PUT] Store a resource [DELETE] Remove a resource [HEAD] Reterive the headers for a resource telnet yahoo.com 80 GET /yahoo.com/yahoomail.jpg HTTP/1.1 Host:yahoo.com When to use PUT and POST? The PUT method completely replaces whatever currently exists at the target URL with something else. With this method, you can create a new resource or overwrite an existing one given you know the exact Request-URI. In short, the PUT method is used to create or overwrite a resource at a particular URL that is known by the client. PUT method is idempotent because no matter how many times we send the same request, the results will always be the same. On the other hand, the POST method is not idempotent since if we send the same POST request multiple times, we will receive various results. When you know the URL of the thing you want to create or overwrite, a PUT method should be used. Alternatively, if you only know the URL of the category or sub-section of the thing you want to create something within, use the POST method. Safe and Unsafe: Safe - let's you to read and view resources. Unsafe - let's you to modify the resources. Post/Redirect/Get Operation: To avoid duplicate post requests signup (GET signup) signup page with forms (POST and update the data and REDIRECT to signed page) signed http Request Format: [method] [url] [version] [GET] [http://server.com/articles/741.html] [HTTP/1.1] [headers] [HOST]: [yahoo.com] [headers] [Accept-Language]: [fr-FR] [headers] [Date]: [Fri, 10 Aug 2002 21:12:00 GMT] Common Request Header: Referrer The URL of the referring page. The page where the URL originated. User-Agent Information about the browser. Accept Preffered media types. Accept-Language Preferred language. Cookie Cookie information. If-Modified-Since Date of last retrieval. Used for caching. Date Creation timestamp for the message. http Response Format: [version] [status] [reason] [HTTP/1.1] [200] [ok] [headers] [Server]: [nginx] [headers] [Content-Type]: [text/html] [body] Status Codes: [100-199] Informational [200-299] Successful [300-309] Redirection [400-499] Client Error [500-599] Server Error [200] [OK] Success. [301] [Moved Permanently] Resource Moved, don't check here again. [302] [Moved Temporarily] Resource Moved, but check here again. (Post Redirect Get Mechanism) [304] [Not Modified] Resource hasn't changed since last retrieval. [400] [Bad Request] Bad Syntax. [401] [Unauthorized] Client might need to authenticate. [403] [Forbidden] Refused Access. [404] [Not found] Resource doesn't exist. [500] [Internal server error] Something went wrong during processing. [503] [Service unavailable] Server will not service the request. HTTP Connections How does the messages actualy moves in the network? When are the network connections opened? When are the network connections closed? http (Application Layer) - tcp (Transport Layer) http Layer (Browser): 1 - Extract the host name and port number from the URL \"http://mail.yahoo.com/q?s=^mail\". 2 - Creates an HTTP socket and start writing the data to the socket. TCP Transport Layer: 1 - Accepts the data and ensures the data is getting delivered to the server without getting lost or duplicated. Error detection / flow control and takes care of the data reliablity. IP Network Layer: 1 - Responsible for taking these information and moving them in the network switch/router/gateway etc. 2 - IP is responsible for delivering the data to destination but doesn't gaurentee the delivery (TCP's job). Datalink Layer: 1 - Ethernet frames. TCP Handshake: Before starting the actual transmission, there is a 3 steps process followed to make sure the server and the client are in agreement to transfer the data. [SYN] Seq=0 [SYN, ACK] Seq=0 Ack=1 [ACK] Seq=1 Ack=1 Persistent Connections: Default type of connection in HTTP/1.1 1 - Persistent connection stays open after completion of one request response transaction. 2 - There is always a downside, each server has a limit in number of persistent connections as a security measure. 3 - Attackes will perform DOS attacks by opening number of persistent connections and makes servers un-responsive. 4 - Since servers only accepts only finite number of persistent connections, servers are configured to close the connections after certain intervals (if idle). Note: The server which does not allow the persistent connection must include a http connection header called \"Connection: close\" which will not allow the client to make another request on the same request. It has to re-open a new connection. Parallel connections: Making 2 different connection parallely at the same time. The server will return \"Connection: close\" header. Persistent Connections: Making more than one req/res transaction in a single connection. Proxies: Forward and Reverse Proxy: Forward Proxy forwards the client requests to the internet. Example: Specific set of users (clients) can access twitter from company via the Forward server. Reverse Proxy: Reverse Proxy sits at the server end accepts the request from internet and forwards them to servers (example: load balancing). Cache Controls: With HTTP/1.1 clients and proxies generally cache the response with 200 ok response code. (response to the http get request) It will not cache PUT, POST and DELETE transaction. Note: Application server can influence this cache settings by using appropriate cache headers. \"Cache-Control: private, max-age=0\" Cache-Control: public Public proxy servers can cache the response. Cache-Control: private Response targeted to single user, only web browser. Cache-Control: no-cache Should not be cached. Cache-Control: no-store HTTP Security: The stateful and stateless web: HTTP is designed as a stateless protocol, each request/response transaction is independent of any previous or future transactions. cookie are used for tracking / differentiate one user from another user. Non-Persistent Cookies (Session Cookies): These cookies will be used only for particular session. Example: set-cookie: GUID=07hfhjebhbwb76, domain=.search.yahoo.com, // Send this cookie only to yahoo.com and not to other websites. path=/ // Restrict cookie to specific path. Persistent Cookies (Will have a validity and stored to client filesystem). set-cookie: GUID=07hfhjebhbwb76, domain=.search.yahoo.com, path=/, expires=Monday, 09-July-2019 21:20:00 GMT Authentication Types: Basic Type: 1. GET /account HTTP/1.1 Host: starlingbank.com ... 2. HTPP/1.1 401 Unauthorized WWW-Authenticate: Basic realm=\"starlingbank.com\" 3. GET /account HTTP/1.1 Host: starlingbank.com Authorization: Basic Z3fnjnjnsflr \"Authorization\" [header type] \"Basic\" [AUTH type] \"Z3fnjnjnsflr\" [base64 encoded value of username and password] Digest: uses md5 digets instead of base64 Form Authentication: When you try to access a secured resource, the user will be temporarily redirected to a web page with authentication form. If authentication is successful, the user will be redirected again to the secured resource. Open ID","title":"HTTP"},{"location":"web/http/#fragments-and-queries-in-http-url","text":"Example: https://www.yahoo.com/search?q=tesla%20research Query: search?q=tesla%20research Example: https://www.yahoo.com/search?q=tesla%20research#space%20research Query: #space research Which means under \"q=tesla%20research\" resource scroll to \"#space\" section.","title":"Fragments and Queries in HTTP url:"},{"location":"web/http/#http-request-methods","text":"[GET] Retreive a resource [POST] Update a resource [PUT] Store a resource [DELETE] Remove a resource [HEAD] Reterive the headers for a resource telnet yahoo.com 80 GET /yahoo.com/yahoomail.jpg HTTP/1.1 Host:yahoo.com When to use PUT and POST? The PUT method completely replaces whatever currently exists at the target URL with something else. With this method, you can create a new resource or overwrite an existing one given you know the exact Request-URI. In short, the PUT method is used to create or overwrite a resource at a particular URL that is known by the client. PUT method is idempotent because no matter how many times we send the same request, the results will always be the same. On the other hand, the POST method is not idempotent since if we send the same POST request multiple times, we will receive various results. When you know the URL of the thing you want to create or overwrite, a PUT method should be used. Alternatively, if you only know the URL of the category or sub-section of the thing you want to create something within, use the POST method.","title":"HTTP Request Methods:"},{"location":"web/http/#safe-and-unsafe","text":"Safe - let's you to read and view resources. Unsafe - let's you to modify the resources.","title":"Safe and Unsafe:"},{"location":"web/http/#postredirectget-operation-to-avoid-duplicate-post-requests","text":"signup (GET signup) signup page with forms (POST and update the data and REDIRECT to signed page) signed","title":"Post/Redirect/Get Operation: To avoid duplicate post requests"},{"location":"web/http/#http-request-format","text":"[method] [url] [version] [GET] [http://server.com/articles/741.html] [HTTP/1.1] [headers] [HOST]: [yahoo.com] [headers] [Accept-Language]: [fr-FR] [headers] [Date]: [Fri, 10 Aug 2002 21:12:00 GMT]","title":"http Request Format:"},{"location":"web/http/#common-request-header","text":"Referrer The URL of the referring page. The page where the URL originated. User-Agent Information about the browser. Accept Preffered media types. Accept-Language Preferred language. Cookie Cookie information. If-Modified-Since Date of last retrieval. Used for caching. Date Creation timestamp for the message.","title":"Common Request Header:"},{"location":"web/http/#http-response-format","text":"[version] [status] [reason] [HTTP/1.1] [200] [ok] [headers] [Server]: [nginx] [headers] [Content-Type]: [text/html] [body]","title":"http Response Format:"},{"location":"web/http/#status-codes","text":"[100-199] Informational [200-299] Successful [300-309] Redirection [400-499] Client Error [500-599] Server Error [200] [OK] Success. [301] [Moved Permanently] Resource Moved, don't check here again. [302] [Moved Temporarily] Resource Moved, but check here again. (Post Redirect Get Mechanism) [304] [Not Modified] Resource hasn't changed since last retrieval. [400] [Bad Request] Bad Syntax. [401] [Unauthorized] Client might need to authenticate. [403] [Forbidden] Refused Access. [404] [Not found] Resource doesn't exist. [500] [Internal server error] Something went wrong during processing. [503] [Service unavailable] Server will not service the request.","title":"Status Codes:"},{"location":"web/http/#http-connections","text":"How does the messages actualy moves in the network? When are the network connections opened? When are the network connections closed? http (Application Layer) - tcp (Transport Layer)","title":"HTTP Connections"},{"location":"web/http/#http-layer-browser","text":"1 - Extract the host name and port number from the URL \"http://mail.yahoo.com/q?s=^mail\". 2 - Creates an HTTP socket and start writing the data to the socket.","title":"http Layer (Browser):"},{"location":"web/http/#tcp-transport-layer","text":"1 - Accepts the data and ensures the data is getting delivered to the server without getting lost or duplicated. Error detection / flow control and takes care of the data reliablity.","title":"TCP Transport Layer:"},{"location":"web/http/#ip-network-layer","text":"1 - Responsible for taking these information and moving them in the network switch/router/gateway etc. 2 - IP is responsible for delivering the data to destination but doesn't gaurentee the delivery (TCP's job).","title":"IP Network Layer:"},{"location":"web/http/#datalink-layer","text":"1 - Ethernet frames.","title":"Datalink Layer:"},{"location":"web/http/#tcp-handshake","text":"Before starting the actual transmission, there is a 3 steps process followed to make sure the server and the client are in agreement to transfer the data. [SYN] Seq=0 [SYN, ACK] Seq=0 Ack=1 [ACK] Seq=1 Ack=1","title":"TCP Handshake:"},{"location":"web/http/#persistent-connections-default-type-of-connection-in-http11","text":"1 - Persistent connection stays open after completion of one request response transaction. 2 - There is always a downside, each server has a limit in number of persistent connections as a security measure. 3 - Attackes will perform DOS attacks by opening number of persistent connections and makes servers un-responsive. 4 - Since servers only accepts only finite number of persistent connections, servers are configured to close the connections after certain intervals (if idle). Note: The server which does not allow the persistent connection must include a http connection header called \"Connection: close\" which will not allow the client to make another request on the same request. It has to re-open a new connection. Parallel connections: Making 2 different connection parallely at the same time. The server will return \"Connection: close\" header. Persistent Connections: Making more than one req/res transaction in a single connection.","title":"Persistent Connections: Default type of connection in HTTP/1.1"},{"location":"web/http/#proxies","text":"Forward and Reverse Proxy: Forward Proxy forwards the client requests to the internet. Example: Specific set of users (clients) can access twitter from company via the Forward server.","title":"Proxies:"},{"location":"web/http/#reverse-proxy","text":"Reverse Proxy sits at the server end accepts the request from internet and forwards them to servers (example: load balancing).","title":"Reverse Proxy:"},{"location":"web/http/#cache-controls","text":"With HTTP/1.1 clients and proxies generally cache the response with 200 ok response code. (response to the http get request) It will not cache PUT, POST and DELETE transaction. Note: Application server can influence this cache settings by using appropriate cache headers. \"Cache-Control: private, max-age=0\" Cache-Control: public Public proxy servers can cache the response. Cache-Control: private Response targeted to single user, only web browser. Cache-Control: no-cache Should not be cached. Cache-Control: no-store","title":"Cache Controls:"},{"location":"web/http/#http-security","text":"The stateful and stateless web: HTTP is designed as a stateless protocol, each request/response transaction is independent of any previous or future transactions. cookie are used for tracking / differentiate one user from another user. Non-Persistent Cookies (Session Cookies): These cookies will be used only for particular session. Example: set-cookie: GUID=07hfhjebhbwb76, domain=.search.yahoo.com, // Send this cookie only to yahoo.com and not to other websites. path=/ // Restrict cookie to specific path. Persistent Cookies (Will have a validity and stored to client filesystem). set-cookie: GUID=07hfhjebhbwb76, domain=.search.yahoo.com, path=/, expires=Monday, 09-July-2019 21:20:00 GMT","title":"HTTP Security:"},{"location":"web/http/#authentication-types","text":"Basic Type: 1. GET /account HTTP/1.1 Host: starlingbank.com ... 2. HTPP/1.1 401 Unauthorized WWW-Authenticate: Basic realm=\"starlingbank.com\" 3. GET /account HTTP/1.1 Host: starlingbank.com Authorization: Basic Z3fnjnjnsflr \"Authorization\" [header type] \"Basic\" [AUTH type] \"Z3fnjnjnsflr\" [base64 encoded value of username and password] Digest: uses md5 digets instead of base64","title":"Authentication Types:"},{"location":"web/http/#form-authentication","text":"When you try to access a secured resource, the user will be temporarily redirected to a web page with authentication form. If authentication is successful, the user will be redirected again to the secured resource. Open ID","title":"Form Authentication:"},{"location":"web/nginx/","text":"Notes: The basic nginx architecture consists of a master process and its workers. Developed to solve c10k problems. Which means handling 10,000 concurrent connections. Applications: High Performance Web Server Reverse Proxy (SSL Termination and Contnet Caching and Termination) Load Balancer The master is supposed to read the configuration file and maintain worker processes, while workers do the actual processing of requests. References: To get info about core context and directive blocks, go to http://nginx.org/en/docs/ngx_core_module.html Commands: While your nginx instance is running, you can manage it by sending signals: sudo nginx -s signal stop: fast shutdown quit: graceful shutdown (wait for workers to finish their processes) reload: reload the configuration file reopen: reopen the log files To verify configuration: nginx -t To view selinux context: semanage fcontext -l | grep -i /usr/share/nginx/html To add selinux context: semanage fcontext -a -t httpd_sys_content_t '/var/www' To restore context back to default: restorecon -R -v '/var/www' Curl with host header: curl --header \"Host: www.example.com\" localhost Configuration files location: /etc/nginx/nginx.conf /usr/local/etc/nginx/nginx.conf /usr/local/nginx/conf/nginx.conf Directive and Context: Directive: The option that consists of name and parameters; it should end with a semicolon Example: gzip on; Context: Section where you can declare directives (similar to scope in programming languages) Example: http { # http context gzip on; # directive in http context } Directive types: You have to pay attention when using the same directive in multiple contexts, as the inheritance model differs for different directives. There are 3 types of directives, each with its own inheritance model. Normal: Has one value per context. Also, it can be defined only once in the context. Subcontexts can override the parent directive, but this override will be valid only in a given subcontext. gzip on; gzip off; # illegal to have 2 normal directives in same context server { # server context location /downloads { # location subcontext under server context } location /assets { } } Array: Adding multiple directives in the same context will add to the values instead of overwriting them altogether. Defining a directive in a subcontext will override ALL parent values in the given subcontext. error_log /var/log/nginx/error.log; error_log /var/log/nginx/error_native.log notice; error_log /var/log/nginx/error_debug.log debug; server { location /downloads { # this will override all the parent directives error_log /var/log/nginx/error_downloads.log; } } Action: Actions are directives that change things. Their inheritance behaviour will depend on the module. For example, in the case of the rewrite directive, every matching directive will be executed: server { rewrite ^ /foobar; location /foobar { rewrite ^ /foo; rewrite ^ /bar; } } Custom Error Pages: Syntax: error_page list of error codes error page Example: error_page 404 /404.html Basic Auth: https://nginx.org/en/docs/http/ngx_http_auth_basic_module.html location /admin.html { auth_basic \"Login Required\"; auth_basic_user_file /etc/nginx/.htpasswd; } nginx SSL: https://nginx.org/en/docs/http/ngx_http_ssl_module.html Self Singed Certs: mkdir /etc/nginx/ssl openssl -req x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/nginx/ssl/keys/private.key -out /etc/nginx/ssl/public.pem req - We\u2019re making a certificate request to OpenSSL -x509 - Specifying the structure that our certificate should have. Conforms to the X.509 standard -nodes - Do not encrypt the output key -days 365 - Set the key to be valid for 365 days -newkey rsa:2048 - Generate an RSA key that is 2048 bits in size -keyout /etc/nginx/ssl/private.key - File to write the private key to -out /etc/nginx/ssl/public.pem - Output file for public portion of key server { listen 443 ssl; root /usr/share/nginx/html; ssl_certificate /etc/nginx/ssl/home.v01.openhouse.cert.pem; ssl_certificate_key /etc/nginx/ssl/keys/home.v01.openhouse.key.pem; server_name _; location = /admin.html { auth_basic \"Login Required\"; auth_basic_user_file /etc/nginx/.htpasswd; } error_page 404 /404.html; }","title":"Nginx"},{"location":"web/nginx/#notes","text":"The basic nginx architecture consists of a master process and its workers. Developed to solve c10k problems. Which means handling 10,000 concurrent connections.","title":"Notes:"},{"location":"web/nginx/#applications","text":"High Performance Web Server Reverse Proxy (SSL Termination and Contnet Caching and Termination) Load Balancer The master is supposed to read the configuration file and maintain worker processes, while workers do the actual processing of requests.","title":"Applications:"},{"location":"web/nginx/#references","text":"To get info about core context and directive blocks, go to http://nginx.org/en/docs/ngx_core_module.html","title":"References:"},{"location":"web/nginx/#commands","text":"While your nginx instance is running, you can manage it by sending signals: sudo nginx -s signal stop: fast shutdown quit: graceful shutdown (wait for workers to finish their processes) reload: reload the configuration file reopen: reopen the log files To verify configuration: nginx -t To view selinux context: semanage fcontext -l | grep -i /usr/share/nginx/html To add selinux context: semanage fcontext -a -t httpd_sys_content_t '/var/www' To restore context back to default: restorecon -R -v '/var/www' Curl with host header: curl --header \"Host: www.example.com\" localhost","title":"Commands:"},{"location":"web/nginx/#configuration-files-location","text":"/etc/nginx/nginx.conf /usr/local/etc/nginx/nginx.conf /usr/local/nginx/conf/nginx.conf","title":"Configuration files location:"},{"location":"web/nginx/#directive-and-context","text":"Directive: The option that consists of name and parameters; it should end with a semicolon Example: gzip on; Context: Section where you can declare directives (similar to scope in programming languages) Example: http { # http context gzip on; # directive in http context }","title":"Directive and Context:"},{"location":"web/nginx/#directive-types","text":"You have to pay attention when using the same directive in multiple contexts, as the inheritance model differs for different directives. There are 3 types of directives, each with its own inheritance model.","title":"Directive types:"},{"location":"web/nginx/#normal","text":"Has one value per context. Also, it can be defined only once in the context. Subcontexts can override the parent directive, but this override will be valid only in a given subcontext. gzip on; gzip off; # illegal to have 2 normal directives in same context server { # server context location /downloads { # location subcontext under server context } location /assets { } }","title":"Normal:"},{"location":"web/nginx/#array","text":"Adding multiple directives in the same context will add to the values instead of overwriting them altogether. Defining a directive in a subcontext will override ALL parent values in the given subcontext. error_log /var/log/nginx/error.log; error_log /var/log/nginx/error_native.log notice; error_log /var/log/nginx/error_debug.log debug; server { location /downloads { # this will override all the parent directives error_log /var/log/nginx/error_downloads.log; } }","title":"Array:"},{"location":"web/nginx/#action","text":"Actions are directives that change things. Their inheritance behaviour will depend on the module. For example, in the case of the rewrite directive, every matching directive will be executed: server { rewrite ^ /foobar; location /foobar { rewrite ^ /foo; rewrite ^ /bar; } }","title":"Action:"},{"location":"web/nginx/#custom-error-pages","text":"Syntax: error_page list of error codes error page Example: error_page 404 /404.html","title":"Custom Error Pages:"},{"location":"web/nginx/#basic-auth","text":"https://nginx.org/en/docs/http/ngx_http_auth_basic_module.html location /admin.html { auth_basic \"Login Required\"; auth_basic_user_file /etc/nginx/.htpasswd; } nginx SSL: https://nginx.org/en/docs/http/ngx_http_ssl_module.html","title":"Basic Auth:"},{"location":"web/nginx/#self-singed-certs","text":"mkdir /etc/nginx/ssl openssl -req x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/nginx/ssl/keys/private.key -out /etc/nginx/ssl/public.pem req - We\u2019re making a certificate request to OpenSSL -x509 - Specifying the structure that our certificate should have. Conforms to the X.509 standard -nodes - Do not encrypt the output key -days 365 - Set the key to be valid for 365 days -newkey rsa:2048 - Generate an RSA key that is 2048 bits in size -keyout /etc/nginx/ssl/private.key - File to write the private key to -out /etc/nginx/ssl/public.pem - Output file for public portion of key server { listen 443 ssl; root /usr/share/nginx/html; ssl_certificate /etc/nginx/ssl/home.v01.openhouse.cert.pem; ssl_certificate_key /etc/nginx/ssl/keys/home.v01.openhouse.key.pem; server_name _; location = /admin.html { auth_basic \"Login Required\"; auth_basic_user_file /etc/nginx/.htpasswd; } error_page 404 /404.html; }","title":"Self Singed Certs:"},{"location":"web/notes/","text":"What is HTTP Keep Alive? https://www.imperva.com/learn/performance/http-keep-alive/","title":"Notes"},{"location":"web/notes/#what-is-http-keep-alive","text":"https://www.imperva.com/learn/performance/http-keep-alive/","title":"What is HTTP Keep Alive?"}]}