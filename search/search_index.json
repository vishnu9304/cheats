{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"MY CHEATS"},{"location":"chef/chef/","text":"Documentation Cookbooks: https://docs.chef.io/cookbooks.html Recipes: https://docs.chef.io/recipes.html Resource: https://docs.chef.io/resource.html Package: https://docs.chef.io/resource_package.html Service: https://docs.chef.io/resource_service.html Chef Server Installation wget https://packages.chef.io/files/stable/chef-server/13.1.13/el/6/chef-server-core-13.1.13-1.el6.x86_64.rpm pm -Uvh chef-server-core-13.1.13-1.el6.x86_64.rpm chef-server-ctl reconfigure chef-server-ctl service-list (Note: The lines with (*) means the services are running.) chef-server-ctl user-create vishnu vishnu pradeep vishnu@yahoo.com 'password' --filename /home/vishnu/vishnu.pem chef-server-ctl org-create yahoo 'yahoo, Inc.' --association_user vishnu --filename /home/vishnu/yahoo.pem chef-server-ctl install chef-manage chef-server-ctl reconfigure chef-manage-ctl reconfigure Note: we have two keys, orgkey and personal key. \"Chef Manage\" is an ruby and rails application that provides an web interface for chef server. Ched DK installation wget https://packages.chef.io/files/stable/chefdk/2.5.3/el/6/chefdk-2.5.3-1.el6.x86_64.rpm rpm -Uvh chef-server-core-13.1.13-1.el6.x86_64.rpm chef shell-init bash eval \"$(chef shell-init bash)\" (or) chef shell-init bash ~/.bash_profile chef generate repo chef-repo knife configure [input the values, go to ~/.chef, you will find 2 files credentials and key file] knife node list Note: Before start working with chef make sure the above variables are set. what happens when a chef-client run occurs? Get configuration data - Read information from client.rb file and Ohai attributes. /etc/chef client.pem client.rb first-boot.json Authenticate w/ Chef server - Utilizes RSA key node name to authenticate with Chef server. Will generate a new RSA key if this is the first connection. Get/rebuild the node object - Pull node object from Chef server if this isn't the first chef-client run. After the pull, the node object is rebuilt based on the node's current state. Expand the run-list - Compiles the list of roles and recipes to be applied. Synchronize cookbooks - Request and download all of the files from cookbooks on the Chef server that are necessary to converge the run list and are different from the files already existing on the node. Reset node attributes - Rebuild the attributes on the node object. Compile the resource collection - Load the necessary Ruby code to converge the run-list. Converge the node - Execute the run-list. Update the node object, process exception report handlers - Update the node object on the Chef server after the chef-client run finishes successfully. Also executing the exception and report handlers in the proper order. Stop, wait for the next run - The chef-client waits until the next time it is executed. What is Chef Supermarket? The Chef Supermarket is a searchable repository for cookbooks.Most modern programming languages have repositories for open source packages (Pypi for Python, Rubygems.org for Ruby, etc.) and that is what the Chef Supermarket is for cookbooks. There are a few ways that you can interact with the Chef Supermarket: Using the public supermarket Deploying a private supermarket The public supermarket includes a lot of open source cookbooks, some of them officially maintained by Chef the company and many more that are published by enthusiastic Chef users. Beware: not all of the cookbooks on the public supermarket are up to date and secure, you\u2019ll want to vet anything open source cookbook that you use. Super Market Related Tools: Berkshelf - used to install and manage cookbook dependencies and versions. Stove - used to version and publish cookbooks to a supermarket (either public or private). What is Test Kitchen? Test Kitchen (or just \u201cKitchen\u201d) is a testing harness that allows us to easily provision environments using a variety of platforms and backends so that we can test our infrastructure code. We can easily create a brand new \u201cserver\u201d that is running a specific operating system, install Chef, run a specific run list, and then execute automated tests to ensure that the configuration that we\u2019re expecting is configured properly. This process might sound tedious, but we can use VM tools like Vagrant or containers through Docker to rapidly create, configure, test, and destroy these environments. Cookbooks chef generate cookbook path / cookbook name - To create a cookbook Runlists https://docs.chef.io/run_lists.html knife upload cookbook path/cookbook name - To upload a cookbook to chef server knife node run_list add [FQDN] -r 'recipe[COOKBOOK::RECIPE], COOKBOOK:RECIPE, role[NAME]' knife node run_list set FQDN COOKBOOK - This will set the run list for the host to default recipe of the given cookbook knife ssh 'name:NODENAME' 'sudo chef-client' -x v01 -A - To execute chef client run remotely knife node edit FQDN -a - To edit the node object stored in the chef server Roles In simple words, Roles are nothing but collection of run lists with some default attributes. We first create a role and add set of run lists and assign the roles to the nodes. Exampe: Create set of cookbooks (more generic), which needs to be installed universally across the infrastructure and packaged them as ROLES. First export EDITOR=\"vim\" knife role create [ROLENAME] It will open vim editor with a role template, add the run lists there and save It will create the role in chef server knife role show knife node run_list add FQDN 'role[ROLENAME]' knife node show FQDN 'role[ROLENAME]' --before 'recipe[COOKBOOK::RECIPE]' - If you want to run your recipes in your role before any other recipes knife ssh 'name:NODENAME' 'sudo chef-client' -x v01 -A - Where 'name:NODENAME' is the search query Environments It\u2019s pretty common to have nodes that are seemingly identical in what they install, but used for slightly different purposes. A great example of this is a staging application server vs. a production application server. knife environment create staging --description \"pre-prodduction staging environment\" - to create an environment knife node environment_set [NODE] [ENVIRONMENT] - to set environment to a node knife node show [NODE] -a environment - to get a particular attribute Databags https://docs.chef.io/data_bags.html Data bags are pieces of JSON data that are stored in the Chef Server. They're searchable and also available within recipes. Where attributes can be a little complicated because of the precedence order, data bags are relatively simple. knife data bag create [DATABAG NAME] - to create databag Create a databag for Jenkins user: jenkins.json { \"id\": \"jenkins\", \"comment\": \"A user for CI/CD\", \"password\": \"$1$6eMmUKIy$8QdyDIyDnwmNTkjKKWbvq.\", \"groups\": [\"ci\"], \"uid\": 2001 } knife data bag create [DATABAG NAME] jenkins Command References Knife Commands: knife cookbook list - to list the available cookbooks knife data bag list - to list data bags knife environment list - to list environments knife node list - to list nodes knife role list - to list roles knife node list -VV - for verbose output Command to bootstrap a node: knife bootstrap NODE-FQDN -N NODE-NAME -x vishnu --sudo -A https://docs.chef.io/install_bootstrap.html","title":"chef"},{"location":"chef/chef/#documentation","text":"Cookbooks: https://docs.chef.io/cookbooks.html Recipes: https://docs.chef.io/recipes.html Resource: https://docs.chef.io/resource.html Package: https://docs.chef.io/resource_package.html Service: https://docs.chef.io/resource_service.html","title":"Documentation"},{"location":"chef/chef/#chef-server-installation","text":"wget https://packages.chef.io/files/stable/chef-server/13.1.13/el/6/chef-server-core-13.1.13-1.el6.x86_64.rpm pm -Uvh chef-server-core-13.1.13-1.el6.x86_64.rpm chef-server-ctl reconfigure chef-server-ctl service-list (Note: The lines with (*) means the services are running.) chef-server-ctl user-create vishnu vishnu pradeep vishnu@yahoo.com 'password' --filename /home/vishnu/vishnu.pem chef-server-ctl org-create yahoo 'yahoo, Inc.' --association_user vishnu --filename /home/vishnu/yahoo.pem chef-server-ctl install chef-manage chef-server-ctl reconfigure chef-manage-ctl reconfigure Note: we have two keys, orgkey and personal key. \"Chef Manage\" is an ruby and rails application that provides an web interface for chef server.","title":"Chef Server Installation"},{"location":"chef/chef/#ched-dk-installation","text":"wget https://packages.chef.io/files/stable/chefdk/2.5.3/el/6/chefdk-2.5.3-1.el6.x86_64.rpm rpm -Uvh chef-server-core-13.1.13-1.el6.x86_64.rpm chef shell-init bash eval \"$(chef shell-init bash)\" (or) chef shell-init bash ~/.bash_profile chef generate repo chef-repo knife configure [input the values, go to ~/.chef, you will find 2 files credentials and key file] knife node list Note: Before start working with chef make sure the above variables are set.","title":"Ched DK installation"},{"location":"chef/chef/#what-happens-when-a-chef-client-run-occurs","text":"Get configuration data - Read information from client.rb file and Ohai attributes. /etc/chef client.pem client.rb first-boot.json Authenticate w/ Chef server - Utilizes RSA key node name to authenticate with Chef server. Will generate a new RSA key if this is the first connection. Get/rebuild the node object - Pull node object from Chef server if this isn't the first chef-client run. After the pull, the node object is rebuilt based on the node's current state. Expand the run-list - Compiles the list of roles and recipes to be applied. Synchronize cookbooks - Request and download all of the files from cookbooks on the Chef server that are necessary to converge the run list and are different from the files already existing on the node. Reset node attributes - Rebuild the attributes on the node object. Compile the resource collection - Load the necessary Ruby code to converge the run-list. Converge the node - Execute the run-list. Update the node object, process exception report handlers - Update the node object on the Chef server after the chef-client run finishes successfully. Also executing the exception and report handlers in the proper order. Stop, wait for the next run - The chef-client waits until the next time it is executed.","title":"what happens when a chef-client run occurs?"},{"location":"chef/chef/#what-is-chef-supermarket","text":"The Chef Supermarket is a searchable repository for cookbooks.Most modern programming languages have repositories for open source packages (Pypi for Python, Rubygems.org for Ruby, etc.) and that is what the Chef Supermarket is for cookbooks. There are a few ways that you can interact with the Chef Supermarket: Using the public supermarket Deploying a private supermarket The public supermarket includes a lot of open source cookbooks, some of them officially maintained by Chef the company and many more that are published by enthusiastic Chef users. Beware: not all of the cookbooks on the public supermarket are up to date and secure, you\u2019ll want to vet anything open source cookbook that you use. Super Market Related Tools: Berkshelf - used to install and manage cookbook dependencies and versions. Stove - used to version and publish cookbooks to a supermarket (either public or private).","title":"What is Chef Supermarket?"},{"location":"chef/chef/#what-is-test-kitchen","text":"Test Kitchen (or just \u201cKitchen\u201d) is a testing harness that allows us to easily provision environments using a variety of platforms and backends so that we can test our infrastructure code. We can easily create a brand new \u201cserver\u201d that is running a specific operating system, install Chef, run a specific run list, and then execute automated tests to ensure that the configuration that we\u2019re expecting is configured properly. This process might sound tedious, but we can use VM tools like Vagrant or containers through Docker to rapidly create, configure, test, and destroy these environments.","title":"What is Test Kitchen?"},{"location":"chef/chef/#cookbooks","text":"chef generate cookbook path / cookbook name - To create a cookbook","title":"Cookbooks"},{"location":"chef/chef/#runlists","text":"https://docs.chef.io/run_lists.html knife upload cookbook path/cookbook name - To upload a cookbook to chef server knife node run_list add [FQDN] -r 'recipe[COOKBOOK::RECIPE], COOKBOOK:RECIPE, role[NAME]' knife node run_list set FQDN COOKBOOK - This will set the run list for the host to default recipe of the given cookbook knife ssh 'name:NODENAME' 'sudo chef-client' -x v01 -A - To execute chef client run remotely knife node edit FQDN -a - To edit the node object stored in the chef server","title":"Runlists"},{"location":"chef/chef/#roles","text":"In simple words, Roles are nothing but collection of run lists with some default attributes. We first create a role and add set of run lists and assign the roles to the nodes. Exampe: Create set of cookbooks (more generic), which needs to be installed universally across the infrastructure and packaged them as ROLES. First export EDITOR=\"vim\" knife role create [ROLENAME] It will open vim editor with a role template, add the run lists there and save It will create the role in chef server knife role show knife node run_list add FQDN 'role[ROLENAME]' knife node show FQDN 'role[ROLENAME]' --before 'recipe[COOKBOOK::RECIPE]' - If you want to run your recipes in your role before any other recipes knife ssh 'name:NODENAME' 'sudo chef-client' -x v01 -A - Where 'name:NODENAME' is the search query","title":"Roles"},{"location":"chef/chef/#environments","text":"It\u2019s pretty common to have nodes that are seemingly identical in what they install, but used for slightly different purposes. A great example of this is a staging application server vs. a production application server. knife environment create staging --description \"pre-prodduction staging environment\" - to create an environment knife node environment_set [NODE] [ENVIRONMENT] - to set environment to a node knife node show [NODE] -a environment - to get a particular attribute","title":"Environments"},{"location":"chef/chef/#databags","text":"https://docs.chef.io/data_bags.html Data bags are pieces of JSON data that are stored in the Chef Server. They're searchable and also available within recipes. Where attributes can be a little complicated because of the precedence order, data bags are relatively simple. knife data bag create [DATABAG NAME] - to create databag Create a databag for Jenkins user: jenkins.json { \"id\": \"jenkins\", \"comment\": \"A user for CI/CD\", \"password\": \"$1$6eMmUKIy$8QdyDIyDnwmNTkjKKWbvq.\", \"groups\": [\"ci\"], \"uid\": 2001 } knife data bag create [DATABAG NAME] jenkins","title":"Databags"},{"location":"chef/chef/#command-references","text":"Knife Commands: knife cookbook list - to list the available cookbooks knife data bag list - to list data bags knife environment list - to list environments knife node list - to list nodes knife role list - to list roles knife node list -VV - for verbose output Command to bootstrap a node: knife bootstrap NODE-FQDN -N NODE-NAME -x vishnu --sudo -A https://docs.chef.io/install_bootstrap.html","title":"Command References"},{"location":"crypto/network_security/","text":"Basic Concepts Encryption: Conversion of plain text to cipher text. (Sender) Decryption: Conversion of cipher text to plain text. (Receiver) Cryptography: Study of Encryption Cryptanalysis: Study of Decryption Cryptology: Encryption + Decryption Encryption can be done in two ways: Stream Cipher: Conversion takes place bit by bit. (Recommended for short length messages Block Cipher: Conversion takes place block by block (grouping of bits), block size is determined by the type of algorithm. Two type of Encryption mechanism: Symmetric Encryption: Same key will be used for Encryption process and Decryption process (Secret key). Asymmetric Encryption: Two keys will be used that are mathematically linked (Public and Private keys). Note: It is possible to encrypt data with private key and decrypt it with public key. Types of Attacks Passive attacks: No modification will be done to the data by unauthorized user. Active attacks: Data will modified by unauthorized user. Passive attacks types: Eavesdropping - No modification will be done to the data, the attacker will just listen to the messages. Traffic Analysis - Just Analysing the traffic. Based on the traffic flow access will be gained. Active attacks types: Masquerade attacks - Receiver will receive the data from 3rd party in the name of sender. Replay attacks - Alice (A) sends her hashed password to Bob (B). Eve (E) sniffs the hash and replays it pretending like alice. Data Modification attacks - Sender sends the data, the attacker takes it and modifies it send it to receiver. Denial Of Service attacks - Disrupts the services provided by the server. Security Services Authentication - Acess control. Prevent un-authorized access. Authorization - Verification of user identity. Confidentiality - Securely sharing the data. Integrity - Make sure the data is not modified during transmission. Non-Repudiation. Substitution Techniques - The characters are replaced with corresponding cipher texts which are calulated mathematically (Vulnerable to attacks, suitable only for short length methods). Example: Caesar cipher Transposition Techniques - Re-arrange the order of bit positions. Example: Rail fence Cipher Columnar transposition FEISTEL STRUCTURE Most of the block cipher techniques follows a strcture called as FEISTEL STRUCTURE. FEISTEL STRUCTURE - Surf online for more info. Block ciper design priciples: block size key size number of rounds subkeys Symmentric Algorithms DES (Data Encyption Standard) AES (Advanced Encyption Standard) ASymmentric Algorithms RSA Diffe Hellman Key Exchange. This is not an Encryption algorithm. It is used only to exchange the secret/symmetric keys.","title":"Network Security"},{"location":"crypto/network_security/#basic-concepts","text":"Encryption: Conversion of plain text to cipher text. (Sender) Decryption: Conversion of cipher text to plain text. (Receiver) Cryptography: Study of Encryption Cryptanalysis: Study of Decryption Cryptology: Encryption + Decryption","title":"Basic Concepts"},{"location":"crypto/network_security/#encryption-can-be-done-in-two-ways","text":"Stream Cipher: Conversion takes place bit by bit. (Recommended for short length messages Block Cipher: Conversion takes place block by block (grouping of bits), block size is determined by the type of algorithm.","title":"Encryption can be done in two ways:"},{"location":"crypto/network_security/#two-type-of-encryption-mechanism","text":"Symmetric Encryption: Same key will be used for Encryption process and Decryption process (Secret key). Asymmetric Encryption: Two keys will be used that are mathematically linked (Public and Private keys). Note: It is possible to encrypt data with private key and decrypt it with public key.","title":"Two type of Encryption mechanism:"},{"location":"crypto/network_security/#types-of-attacks","text":"Passive attacks: No modification will be done to the data by unauthorized user. Active attacks: Data will modified by unauthorized user. Passive attacks types: Eavesdropping - No modification will be done to the data, the attacker will just listen to the messages. Traffic Analysis - Just Analysing the traffic. Based on the traffic flow access will be gained. Active attacks types: Masquerade attacks - Receiver will receive the data from 3rd party in the name of sender. Replay attacks - Alice (A) sends her hashed password to Bob (B). Eve (E) sniffs the hash and replays it pretending like alice. Data Modification attacks - Sender sends the data, the attacker takes it and modifies it send it to receiver. Denial Of Service attacks - Disrupts the services provided by the server.","title":"Types of Attacks"},{"location":"crypto/network_security/#security-services","text":"Authentication - Acess control. Prevent un-authorized access. Authorization - Verification of user identity. Confidentiality - Securely sharing the data. Integrity - Make sure the data is not modified during transmission. Non-Repudiation. Substitution Techniques - The characters are replaced with corresponding cipher texts which are calulated mathematically (Vulnerable to attacks, suitable only for short length methods). Example: Caesar cipher Transposition Techniques - Re-arrange the order of bit positions. Example: Rail fence Cipher Columnar transposition","title":"Security Services"},{"location":"crypto/network_security/#feistel-structure","text":"Most of the block cipher techniques follows a strcture called as FEISTEL STRUCTURE. FEISTEL STRUCTURE - Surf online for more info. Block ciper design priciples: block size key size number of rounds subkeys","title":"FEISTEL STRUCTURE"},{"location":"crypto/network_security/#symmentric-algorithms","text":"DES (Data Encyption Standard) AES (Advanced Encyption Standard)","title":"Symmentric Algorithms"},{"location":"crypto/network_security/#asymmentric-algorithms","text":"RSA Diffe Hellman Key Exchange. This is not an Encryption algorithm. It is used only to exchange the secret/symmetric keys.","title":"ASymmentric Algorithms"},{"location":"crypto/ssl/","text":"Certificate extensions An SSL Certificate is essentially an X.509 certificate, it defines the structure of the certificate. These certificate files will have different extensions based on the format and encoding they use. Types of encoding Binary Base64 ASCII Formats PEM uses Base64 ASCII encoding DER uses Binary encoding PEM file extensions (pem, crt, key, cer) The .pem file can include the server certificate, the intermediate certificate and the private key in a single file The server certificate and intermediate certificate can also be in a separate .crt or .cer file The private key can be in a .key file Each certificate in the PEM file is contained between the ---- BEGIN CERTIFICATE---- and ----END CERTIFICATE---- statements The private key is contained between the ---- BEGIN RSA PRIVATE KEY----- and -----END RSA PRIVATE KEY----- statements The CSR is contained between the -----BEGIN CERTIFICATE REQUEST----- and -----END CERTIFICATE REQUEST----- statements DER file extensions (der, cer) The DER certificates are in binary form, contained in .der or .cer files. These certificates are mainly used in Java-based web servers. OpenSSL commands Print the certificate in text form and don't print certificate output. openssl x509 -in server.crt -text -noout Verify Whether a Certificate and Private Key Match Verify Whether a Certificate and Private Key Match.To verify you need to print out md5 checksums and compare them. openssl x509 -noout -modulus -in server.crt| openssl md5 openssl rsa -noout -modulus -in server.key| openssl md5 Verify key and it's validity sudo openssl rsa -in server.key -noout -check RSA key ok Verify a Certificate was Signed by a CA openssl verify -verbose -CAFile ca.crt server.crt Convert PEM to DER openssl x509 -in server.crt -outform der -out server.der Convert DER to PEM openssl x509 -inform der -in server.der -out server.crt","title":"SSL"},{"location":"crypto/ssl/#certificate-extensions","text":"An SSL Certificate is essentially an X.509 certificate, it defines the structure of the certificate. These certificate files will have different extensions based on the format and encoding they use.","title":"Certificate extensions"},{"location":"crypto/ssl/#types-of-encoding","text":"Binary Base64 ASCII","title":"Types of encoding"},{"location":"crypto/ssl/#formats","text":"PEM uses Base64 ASCII encoding DER uses Binary encoding","title":"Formats"},{"location":"crypto/ssl/#pem-file-extensions-pem-crt-key-cer","text":"The .pem file can include the server certificate, the intermediate certificate and the private key in a single file The server certificate and intermediate certificate can also be in a separate .crt or .cer file The private key can be in a .key file Each certificate in the PEM file is contained between the ---- BEGIN CERTIFICATE---- and ----END CERTIFICATE---- statements The private key is contained between the ---- BEGIN RSA PRIVATE KEY----- and -----END RSA PRIVATE KEY----- statements The CSR is contained between the -----BEGIN CERTIFICATE REQUEST----- and -----END CERTIFICATE REQUEST----- statements","title":"PEM file extensions (pem, crt, key, cer)"},{"location":"crypto/ssl/#der-file-extensions-der-cer","text":"The DER certificates are in binary form, contained in .der or .cer files. These certificates are mainly used in Java-based web servers.","title":"DER file extensions (der, cer)"},{"location":"crypto/ssl/#openssl-commands","text":"Print the certificate in text form and don't print certificate output. openssl x509 -in server.crt -text -noout","title":"OpenSSL commands"},{"location":"crypto/ssl/#verify-whether-a-certificate-and-private-key-match","text":"Verify Whether a Certificate and Private Key Match.To verify you need to print out md5 checksums and compare them. openssl x509 -noout -modulus -in server.crt| openssl md5 openssl rsa -noout -modulus -in server.key| openssl md5","title":"Verify Whether a Certificate and Private Key Match"},{"location":"crypto/ssl/#verify-key-and-its-validity","text":"sudo openssl rsa -in server.key -noout -check RSA key ok","title":"Verify key and it's validity"},{"location":"crypto/ssl/#verify-a-certificate-was-signed-by-a-ca","text":"openssl verify -verbose -CAFile ca.crt server.crt","title":"Verify a Certificate was Signed by a CA"},{"location":"crypto/ssl/#convert-pem-to-der","text":"openssl x509 -in server.crt -outform der -out server.der","title":"Convert PEM to DER"},{"location":"crypto/ssl/#convert-der-to-pem","text":"openssl x509 -inform der -in server.der -out server.crt","title":"Convert DER to PEM"},{"location":"docker/docker_ca/","text":"Docker Edition Community Edition - Free Opensource Enterprise Edition - Paid version Installing Docker CE 1 - sudo yum install -y device-mapper-persistent-data lvm2 2 - sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 3 - sudo yum install -y docker-ce-18.09.5 docker-ce-cli-18.09.5 containerd.io 4 - sudo systemctl start docker 5 - sudo systemctl enable docker 6 - sudo useradd -a -G docker username // For allowing normal users to execute docker commands, add them under docker group. Selcting Storage Driver Command to find the storage driver: docker info Changing the default storage driver: 1 - Change it under \"/usr/lib/systemd/system/docker.service\" and add this flag: --storage-driver devicemapper ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ExecStart=/usr/bin/dockerd -H fd:// --storage-driver devicemapper --containerd=/run/container containerd.sock 2 - Daemon config file. (Recommended Way) /etc/docker/daemon.json { \"storage-driver\": \"devicemapper\" } When you start a container, a thin writable container layer is added on top of the other layers. Any changes the container makes to the filesystem are stored here. The major difference between a container and an image is the top writable layer. When the container is deleted, the writable layer is also deleted. The underlying image remains unchanged. Container Size On Disk: docker ps -s // https://docs.docker.com/storage/storagedriver/ Copy On Write: Copy the files to the writable layer only if the files are modifed. Running the docker Container: (docker run command) Some of the common falgs used with \"docker run\" command. Syntax: docker run [options] image[:tag] [command] [args] -d Run container in detached mode. --name For providing a descriptive name to the container. --restart options no - Never restart the container. on-failure - If container exits with non-zero exit code. always - Always restart the container whether it is successful or not. Also start the container on daemon restart. unless-stopped - It is similar to \"always\", Container will not be restarted, if you stop it explicitly. -p host_port : container_port Publish, expose a port inside the container by mapping it with host port. --rm Remove the container automatically when it exits. Not compatible with \"--restart\" option. --memory Memory Hard Limit. --memory-reservation A soft limit on memory usage. Container will be restricted within this memory if docker detects memory contention on the host. Logging Drivers https://docs.docker.com/config/containers/logging/configure/ Logging Drivers are a pluggable framework for accessing log data from services and container in docker. Configure the default logging driver under /etc/docker/daemon.json using options \"log-driver\" and \"log-opts\" (System Wide) { \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"15m\" } } How to override the settings at the container level. docker run --log-driver json-file --log-opt max-size=50m nginx Image Creation, Management, and Registry Docker Images: The image consists of one or more read-only layers, while the container adds one additional layer. The layered filesystem allows multiple images and containers to share the same layers. This results in 1 - Small overall footprint 2 - Faster image transfer 3 - Faster image build docker image pull IMAGE[:TAG] To pull the docker image. docker image history IMAGE To list the layers used by the image. Components Of Dockerfile https://docs.docker.com/engine/reference/builder/ A docker file is set of instructions which are used to construct docker image. These instructions are called directives. FROM Starts with a new build stage and sets the base image. ENV To set environment variables. RUN Create new filesystem layer by running a command. CMD Default command to run when the container is executed. EXPOSE Technically it will not expose any ports. Documents which ports are intended to be published when running a container. WORKDIR Sets the current working directory for subsequent directives such as ADD,COPY,CMD,ENTRYPOINT We can have multiple WORKDIR directives inside the docker file. WORKDIR /var WORKDIR www //Relative path WORKDIR html The above three WORKDIR directives are equivalent to /var/www/html //Absolute path COPY Copy files from local machine to the image ADD Similar to copy, but little advanced than COPY, like pulling files using URL and extract an archive into loose files in the image. STOPSIGNAL Specify the signal that will be used to stop the container. When you run docker container stop, this signal will be passed. HEALTHCHECK Used to specify a custom health check, to verify the container is running fine. HEALTHCHECK CMD curl localhost:80 docker build -t custom-nginx . docker run -d custom-nginx -p 8080:80 Example: FROM Centos7:latest ENV NGINX_VERSION=1.0.8 RUN yum update -y yum install -y curl RUN yum update -y yum install -y nginx=$NGINX_VERSION [ why \"yum update -y\" is added twice? When you rebuild the image by changing nginx version, it will first look for the line \"RUN yum update -y yum install -y curl\" since there is not change in the RUN directive it will use the same old layer. Note: Inorder to trigger a change to any layer, we should modify the RUN directive. ] CMD [\"nginx\", \"-g\", \"daemon off;\"] Building Efficient Images General tips: - Put things that are less likely to change on lower level layers. - Don't create unnecessary layers. - Avoid including any unnecessary files, packages, etc.. Docker Multistage builds: Docker supports ablity to perform multistage builds.It will have more than one FROM directive in the docker file with each FROM directive starting a new stage. Each stage begins with a completely new set of layers, allowing you to selectively copy only the files needed from previous layer. Example: Below steps will create an image size of 774MB FROM golang:1.12.4 WORKDIR /helloworld COPY helloworld.go . RUN GOOS=linux go build -a -installsuffix cgo -o helloworld . CMD [\"./helloworld\"] Example: Multistate build.This will produce only image size of 7MB. Idea is to keep only the required files not all, in our case we don't need the entire to go image to run our program. All we need is a binary. Create the binay in STAGE1 and move it to STAGE2 image with smaller size. FROM golang:1.12.4 AS compiler //STAGE1 WORKDIR /helloworld COPY helloworld.go . RUN GOOS=linux go build -a -installsuffix cgo -o helloworld . FROM alpine:3.9.3 //STAGE2 WORKDIR /root COPY --from=compiler /helloworld/helloworld . CMD [\"./helloworld\"] REPOSITORY TAG IMAGE ID CREATED SIZE gostage latest 3b3816104992 7 seconds ago 7.53MB go-custom latest 9802cc0d3ab8 10 minutes ago 776MB Managing Images docker pull To pull the images from registry, if not found locally. docker image ls To list images. docker image ls -a To list images including intermediate images. docker inspect image name To get more info about the images. Provides json output. docker inspect --format \"{{.Arch}} {{.Os}}\" --format (go template) to extract specific fields. docker image rm image name / docker rmi image name To remove the image. docker container ls -a / docker ps -a To list the containers. Dangling Images: Dangling Images are something which doesn't have tags and no containers referencing them. When we delete a container, it doesn't necessarily delete the uderlying image. It will delete only the tags not the image. So that image is called danglig image. Cleaning up the Dangling Images: docker image prune If you have any image which is not referenced by anything or any containers. This command will do a clean up. Flattening an Image: Docker doesn't provide an official way to do this. Run a container - docker export (export the container to an archive) - docker import (Import the archive as new image) docker export container flat.tar cat flat.tar | docker import - flat:latest Docker Storage: https://docs.docker.com/storage/ Storage drivers are also known as Graph Drivers. The proper storage driver to use often depends on your operating system. overlay2 Centos8 and RHEL versions aufs Ubuntu device mapper Centos7 and earlier Storage Models: Persistent data can be managed using several storage models. Filesystem storage: Data stored in the form of a file system. Used by overlay and aufs. Efficient use of memory. Inefficient with write-heavy workloads. Block storage: Stores data in block. Used by device mapper. Efficient with write-heavy workloads. Object storage: Stores data in an external object based store. Application must be designed to use object based storage. Flexible and scalable.","title":"Docker CA"},{"location":"docker/docker_ca/#docker-edition","text":"Community Edition - Free Opensource Enterprise Edition - Paid version","title":"Docker Edition"},{"location":"docker/docker_ca/#installing-docker-ce","text":"1 - sudo yum install -y device-mapper-persistent-data lvm2 2 - sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 3 - sudo yum install -y docker-ce-18.09.5 docker-ce-cli-18.09.5 containerd.io 4 - sudo systemctl start docker 5 - sudo systemctl enable docker 6 - sudo useradd -a -G docker username // For allowing normal users to execute docker commands, add them under docker group.","title":"Installing Docker CE"},{"location":"docker/docker_ca/#selcting-storage-driver","text":"Command to find the storage driver: docker info Changing the default storage driver: 1 - Change it under \"/usr/lib/systemd/system/docker.service\" and add this flag: --storage-driver devicemapper ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ExecStart=/usr/bin/dockerd -H fd:// --storage-driver devicemapper --containerd=/run/container containerd.sock 2 - Daemon config file. (Recommended Way) /etc/docker/daemon.json { \"storage-driver\": \"devicemapper\" } When you start a container, a thin writable container layer is added on top of the other layers. Any changes the container makes to the filesystem are stored here. The major difference between a container and an image is the top writable layer. When the container is deleted, the writable layer is also deleted. The underlying image remains unchanged. Container Size On Disk: docker ps -s // https://docs.docker.com/storage/storagedriver/ Copy On Write: Copy the files to the writable layer only if the files are modifed.","title":"Selcting Storage Driver"},{"location":"docker/docker_ca/#running-the-docker-container-docker-run-command","text":"Some of the common falgs used with \"docker run\" command. Syntax: docker run [options] image[:tag] [command] [args] -d Run container in detached mode. --name For providing a descriptive name to the container. --restart options no - Never restart the container. on-failure - If container exits with non-zero exit code. always - Always restart the container whether it is successful or not. Also start the container on daemon restart. unless-stopped - It is similar to \"always\", Container will not be restarted, if you stop it explicitly. -p host_port : container_port Publish, expose a port inside the container by mapping it with host port. --rm Remove the container automatically when it exits. Not compatible with \"--restart\" option. --memory Memory Hard Limit. --memory-reservation A soft limit on memory usage. Container will be restricted within this memory if docker detects memory contention on the host.","title":"Running the docker Container: (docker run command)"},{"location":"docker/docker_ca/#logging-drivers","text":"https://docs.docker.com/config/containers/logging/configure/ Logging Drivers are a pluggable framework for accessing log data from services and container in docker. Configure the default logging driver under /etc/docker/daemon.json using options \"log-driver\" and \"log-opts\" (System Wide) { \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"15m\" } } How to override the settings at the container level. docker run --log-driver json-file --log-opt max-size=50m nginx","title":"Logging Drivers"},{"location":"docker/docker_ca/#image-creation-management-and-registry","text":"Docker Images: The image consists of one or more read-only layers, while the container adds one additional layer. The layered filesystem allows multiple images and containers to share the same layers. This results in 1 - Small overall footprint 2 - Faster image transfer 3 - Faster image build docker image pull IMAGE[:TAG] To pull the docker image. docker image history IMAGE To list the layers used by the image.","title":"Image Creation, Management, and Registry"},{"location":"docker/docker_ca/#components-of-dockerfile","text":"https://docs.docker.com/engine/reference/builder/ A docker file is set of instructions which are used to construct docker image. These instructions are called directives. FROM Starts with a new build stage and sets the base image. ENV To set environment variables. RUN Create new filesystem layer by running a command. CMD Default command to run when the container is executed. EXPOSE Technically it will not expose any ports. Documents which ports are intended to be published when running a container. WORKDIR Sets the current working directory for subsequent directives such as ADD,COPY,CMD,ENTRYPOINT We can have multiple WORKDIR directives inside the docker file. WORKDIR /var WORKDIR www //Relative path WORKDIR html The above three WORKDIR directives are equivalent to /var/www/html //Absolute path COPY Copy files from local machine to the image ADD Similar to copy, but little advanced than COPY, like pulling files using URL and extract an archive into loose files in the image. STOPSIGNAL Specify the signal that will be used to stop the container. When you run docker container stop, this signal will be passed. HEALTHCHECK Used to specify a custom health check, to verify the container is running fine. HEALTHCHECK CMD curl localhost:80 docker build -t custom-nginx . docker run -d custom-nginx -p 8080:80 Example: FROM Centos7:latest ENV NGINX_VERSION=1.0.8 RUN yum update -y yum install -y curl RUN yum update -y yum install -y nginx=$NGINX_VERSION [ why \"yum update -y\" is added twice? When you rebuild the image by changing nginx version, it will first look for the line \"RUN yum update -y yum install -y curl\" since there is not change in the RUN directive it will use the same old layer. Note: Inorder to trigger a change to any layer, we should modify the RUN directive. ] CMD [\"nginx\", \"-g\", \"daemon off;\"]","title":"Components Of Dockerfile"},{"location":"docker/docker_ca/#building-efficient-images","text":"General tips: - Put things that are less likely to change on lower level layers. - Don't create unnecessary layers. - Avoid including any unnecessary files, packages, etc.. Docker Multistage builds: Docker supports ablity to perform multistage builds.It will have more than one FROM directive in the docker file with each FROM directive starting a new stage. Each stage begins with a completely new set of layers, allowing you to selectively copy only the files needed from previous layer. Example: Below steps will create an image size of 774MB FROM golang:1.12.4 WORKDIR /helloworld COPY helloworld.go . RUN GOOS=linux go build -a -installsuffix cgo -o helloworld . CMD [\"./helloworld\"] Example: Multistate build.This will produce only image size of 7MB. Idea is to keep only the required files not all, in our case we don't need the entire to go image to run our program. All we need is a binary. Create the binay in STAGE1 and move it to STAGE2 image with smaller size. FROM golang:1.12.4 AS compiler //STAGE1 WORKDIR /helloworld COPY helloworld.go . RUN GOOS=linux go build -a -installsuffix cgo -o helloworld . FROM alpine:3.9.3 //STAGE2 WORKDIR /root COPY --from=compiler /helloworld/helloworld . CMD [\"./helloworld\"] REPOSITORY TAG IMAGE ID CREATED SIZE gostage latest 3b3816104992 7 seconds ago 7.53MB go-custom latest 9802cc0d3ab8 10 minutes ago 776MB","title":"Building Efficient Images"},{"location":"docker/docker_ca/#managing-images","text":"docker pull To pull the images from registry, if not found locally. docker image ls To list images. docker image ls -a To list images including intermediate images. docker inspect image name To get more info about the images. Provides json output. docker inspect --format \"{{.Arch}} {{.Os}}\" --format (go template) to extract specific fields. docker image rm image name / docker rmi image name To remove the image. docker container ls -a / docker ps -a To list the containers.","title":"Managing Images"},{"location":"docker/docker_ca/#dangling-images","text":"Dangling Images are something which doesn't have tags and no containers referencing them. When we delete a container, it doesn't necessarily delete the uderlying image. It will delete only the tags not the image. So that image is called danglig image.","title":"Dangling Images:"},{"location":"docker/docker_ca/#cleaning-up-the-dangling-images","text":"docker image prune If you have any image which is not referenced by anything or any containers. This command will do a clean up.","title":"Cleaning up the Dangling Images:"},{"location":"docker/docker_ca/#flattening-an-image-docker-doesnt-provide-an-official-way-to-do-this","text":"Run a container - docker export (export the container to an archive) - docker import (Import the archive as new image) docker export container flat.tar cat flat.tar | docker import - flat:latest","title":"Flattening an Image: Docker doesn't provide an official way to do this."},{"location":"docker/docker_ca/#docker-storage-httpsdocsdockercomstorage","text":"Storage drivers are also known as Graph Drivers. The proper storage driver to use often depends on your operating system. overlay2 Centos8 and RHEL versions aufs Ubuntu device mapper Centos7 and earlier Storage Models: Persistent data can be managed using several storage models.","title":"Docker Storage: https://docs.docker.com/storage/"},{"location":"docker/docker_ca/#filesystem-storage","text":"Data stored in the form of a file system. Used by overlay and aufs. Efficient use of memory. Inefficient with write-heavy workloads.","title":"Filesystem storage:"},{"location":"docker/docker_ca/#block-storage","text":"Stores data in block. Used by device mapper. Efficient with write-heavy workloads.","title":"Block storage:"},{"location":"docker/docker_ca/#object-storage","text":"Stores data in an external object based store. Application must be designed to use object based storage. Flexible and scalable.","title":"Object storage:"},{"location":"golang/constants/","text":"Constants as the name indicate cannot be reassigned again to any other value. const a = 55 //allowed a = 89 //reassignment not allowed The value of a constant should be known at compile time. var a = math.Sqrt(4)//allowed const b = math.Sqrt(4)//not allowed a is a variable and hence it can be assigned to the result of the function math.Sqrt(4) b is a constant and the value of b needs to be know at compile time There are two types of constants. Typed Constant 1. const myConst int = 33 2. const myConst string = \"vishnu\" 3. const myConst bool = true 4. const myConst float32 = 675.09 Shadowing const a int16 = 27 // Package level func main() { const a int = 89 // Function level and this is vaild. fmt.Printf(\"%T, %v \\n\", a, a) } UnTyped constants are interoperable with similar type. func main() { const a = 10 // untyped constant var b int32 = 20 c := a + b // Compiler will see this as c := a + 10 fmt.Println(c) } Typed constants are interoperable only with the same type. func main() { const a int = 10 // typed constant var b int32 = 20 c := a + b fmt.Println(c) // ./prog.go:10:9: invalid operation: a + b (mismatched types int and int32) } What is iota? iota is a counter when we are creating enumerating constants. const ( a = iota b = iota c = iota ) The above block can also be written as: const ( a = iota b c ) The compiler learns the pattern and does the increment. Note: iota is scoped to the constant block. If we create a new constant block, iota will again start incrementing from '0'. Examples: package main import ( \"fmt\" ) func main() { const a = 5 var intVar int = a var int32Var int32 = a var float64Var float64 = a var complex64Var complex64 = a fmt.Println(\"intVar\",intVar, \"\\nint32Var\", int32Var, \"\\nfloat64Var\", float64Var, \"\\ncomplex64Var\",complex64Var) } The value of a is 5 and the syntax of a is generic (it can represent a float, integer or even a complex number with no imaginary part) and hence it is possible to be assigned to any compatible type. The default type of these kind of constants can be thought of as being generated on the fly depending on the context. The below program is perfectly valid: package main import ( \"fmt\" ) func main() { var a = 5.9/8 fmt.Printf(\"a's type %T value %v\",a, a) } The below program is not a valid one: package main import ( \"fmt\" ) func main() { a := 8.0 b := 9 var c = a/b // invalid operation: a / b (mismatched types float64 and int) fmt.Printf(\"a's type %T value %v\",a, a) }","title":"constants"},{"location":"golang/defer/","text":"Defer statement is used to execute a function call just before the function where the defer statement is present returns. Example: package main import \"fmt\" func Finished() { fmt.Println(\"Finished finding largest\") } func FindLargest(nums []int) { defer Finished() max := nums[0] for _, v := range nums { if v max { max = v } } fmt.Printf(\"The largest number in the given slice is %d\\n\", max) } func main() { var nums = []int{1, 2, 4, 5, 6} FindLargest(nums) } Stack of defers When a function has multiple defer calls, they are they are added on to a stack and executed in Last In First Out (LIFO) order. Example: package main import \"fmt\" func main() { name := \"vishnu\" fmt.Printf(\"Before Reversing string: %s\\n\", name) for _, v := range rune[](name) { defer fmt.Printf(\"%c\",v) } } The defere statement defer fmt.Printf(\"%c\",v) will be added to a stack like below and printed in LIFO order at the end of the main function execution. fmt.Printf(u) fmt.Printf(n) fmt.Printf(h) fmt.Printf(s) fmt.Printf(i) fmt.Printf(v) Hence the output of the above program will be a reversed string unshiv . Arguments evaluation The arguments of a deferred function are evaluated when the defer statement is executed and not when the actual function call is done. package main import ( \"fmt\" ) func printA(a int) { fmt.Println(\"value of a in deferred function\", a) } func main() { a := 5 defer printA(a) a = 10 fmt.Println(\"value of a before deferred function call\", a) } Output: value of a before deferred function call 10 value of a in deferred function 5 From the above output it can be understood that although the value of a changes to 10 after the defer statement is executed, the actual deferred function call printA(a) still prints 5.","title":"defer"},{"location":"golang/defer/#stack-of-defers","text":"When a function has multiple defer calls, they are they are added on to a stack and executed in Last In First Out (LIFO) order. Example: package main import \"fmt\" func main() { name := \"vishnu\" fmt.Printf(\"Before Reversing string: %s\\n\", name) for _, v := range rune[](name) { defer fmt.Printf(\"%c\",v) } } The defere statement defer fmt.Printf(\"%c\",v) will be added to a stack like below and printed in LIFO order at the end of the main function execution. fmt.Printf(u) fmt.Printf(n) fmt.Printf(h) fmt.Printf(s) fmt.Printf(i) fmt.Printf(v) Hence the output of the above program will be a reversed string unshiv .","title":"Stack of defers"},{"location":"golang/defer/#arguments-evaluation","text":"The arguments of a deferred function are evaluated when the defer statement is executed and not when the actual function call is done. package main import ( \"fmt\" ) func printA(a int) { fmt.Println(\"value of a in deferred function\", a) } func main() { a := 5 defer printA(a) a = 10 fmt.Println(\"value of a before deferred function call\", a) } Output: value of a before deferred function call 10 value of a in deferred function 5 From the above output it can be understood that although the value of a changes to 10 after the defer statement is executed, the actual deferred function call printA(a) still prints 5.","title":"Arguments evaluation"},{"location":"golang/error/","text":"Handling Errors The idiomatic way of handling error in Go is to compare the returned error to nil. A nil value indicates that no error has occurred and a non nil value indicates the presence of an error. Error type representation Error is an interface type with below definition. type error interface { Error() string } Note: To assert the underlying struct. Follow the below steps. When any method or function returns an error type. Check the underlying type to get more info fmt.Printf(\"%T\\n\", err) . package main import ( \"fmt\" \"os\" ) func main() { f, err := os.Open(\"/test.txt\") if err != nil { fmt.Println(err) fmt.Printf(\"%T\\n\", err) } fmt.Println(f.Name(), \"opened successfully\") }","title":"error"},{"location":"golang/error/#error-type-representation","text":"Error is an interface type with below definition. type error interface { Error() string } Note: To assert the underlying struct. Follow the below steps. When any method or function returns an error type. Check the underlying type to get more info fmt.Printf(\"%T\\n\", err) . package main import ( \"fmt\" \"os\" ) func main() { f, err := os.Open(\"/test.txt\") if err != nil { fmt.Println(err) fmt.Printf(\"%T\\n\", err) } fmt.Println(f.Name(), \"opened successfully\") }","title":"Error type representation"},{"location":"golang/go_tools/","text":"How to format the code? go fmt main.go go fmt -d main.go This command will show the difference. How to build a binary for windows (Cross compilation) architecture from Mac? GOOS=windows go build main.go What will go install do ? It will compile the code and install the resulting binary under $GOPATH/bin directory. What will go get do? go get github.com/golang/example/hello It will go to github.com download the code compliles it and place the binary under $GOPATH/bin Extracting info from go packages. go doc and go list Vishnus-Mac:test v01$ ls main.go Vishnus-Mac:test v01$ go list -f '{{ .Name }}: {{ .Doc }}: {{ .Imports }}' main: This is a demo!: [fmt] Vishnus-Mac:test v01$ go list -f '{{ join .Imports \"\\n\" }}' fmt errors internal/fmtsort io math os reflect strconv sync unicode/utf8 go list -f '{{ .Doc }}' fmt // To list the \"fmt\" package documentation. go doc go doc fmt // To check documentation for fmt package. godoc -http :6060 // To run http doc server. errcheck tool Run errcheck command under the directory where the go files are kept. It will display any unhandled errors. Refer: https://youtu.be/uBjoTxosSys","title":"gotools"},{"location":"golang/go_tools/#how-to-format-the-code","text":"go fmt main.go go fmt -d main.go This command will show the difference.","title":"How to format the code?"},{"location":"golang/go_tools/#how-to-build-a-binary-for-windows-cross-compilation-architecture-from-mac","text":"GOOS=windows go build main.go","title":"How to build a binary for windows (Cross compilation) architecture from Mac?"},{"location":"golang/go_tools/#what-will-go-install-do","text":"It will compile the code and install the resulting binary under $GOPATH/bin directory.","title":"What will go install do ?"},{"location":"golang/go_tools/#what-will-go-get-do","text":"go get github.com/golang/example/hello It will go to github.com download the code compliles it and place the binary under $GOPATH/bin","title":"What will go get do?"},{"location":"golang/go_tools/#extracting-info-from-go-packages-go-doc-and-go-list","text":"Vishnus-Mac:test v01$ ls main.go Vishnus-Mac:test v01$ go list -f '{{ .Name }}: {{ .Doc }}: {{ .Imports }}' main: This is a demo!: [fmt] Vishnus-Mac:test v01$ go list -f '{{ join .Imports \"\\n\" }}' fmt errors internal/fmtsort io math os reflect strconv sync unicode/utf8 go list -f '{{ .Doc }}' fmt // To list the \"fmt\" package documentation.","title":"Extracting info from go packages. go doc and go list"},{"location":"golang/go_tools/#go-doc","text":"go doc fmt // To check documentation for fmt package. godoc -http :6060 // To run http doc server.","title":"go doc"},{"location":"golang/go_tools/#errcheck-tool","text":"Run errcheck command under the directory where the go files are kept. It will display any unhandled errors. Refer: https://youtu.be/uBjoTxosSys","title":"errcheck tool"},{"location":"golang/gomod/","text":"What is a go module? A module is a way of packaging the software as simple as that. A go module is not atomic and it can contain many packages inside like a repository and it is versioned . Go follows something called Semantic Versioning to version the packages. Also called as semver. oh wait, then what is the difference between a package and a repository? A package cannot be sub divided in to working pieces. If you divide the package in to two different files, still you need those two files to represent the whole pacakge. A package is an atomic thing. A repository contains many packages that can move around independently. What is versioning? Why do we use it? How do we use it? Software changes over time. We need some kind of standard way of tracking these changes over period of time. That is where Semantic Versioning comes in to picture. It is a pre straight forward convention of how to name versions. Example: v1.0.0 v1 - Major Version for every single non backward compatible versions. v1.0 - Minor Version. We increase it when we add a new feature and not broken any backward Compatiblity. v1.0.0 - Patch updates. fixing things that has broken, like security bug fixes. Writing programs outside the GOPATH Command to check default gopath: go env GOPATH Example: package main import ( \"fmt\" \"log\" \"os\" \"github.com/sirupsen/logrus\" ) func main() { _, err := io.Copy(os.Stdout, os.Stdin) if err != { logrus.Fatal(err) } } Here github.com/sirupsen/logrus is a third party package. When you do go get it will complain that it is not able to find the gopath. It cannot store code outside gopath. Vishnus-Mac:mycat v01$ go get go get: no install location for directory /Users/v01/dev/mycat outside GOPATH For more details see: 'go help gopath' Go module comes to rescuse the above scenario. We are going to go from a directory having go code to directory having go module Vishnus-Mac:mycat v01$ go mod init go: cannot determine module path for source directory /Users/v01/dev/mycat (outside GOPATH, no import comments) Again it will complain as you are outside the gopath and it doesn't know what module you are initalizing. We need to provide the import path Vishnus-Mac:mycat v01$ go mod init github.com/vishnu9304/mycat go: creating new go.mod: module github.com/vishnu9304/mycat Vishnus-Mac:mycat v01$ cat go.mod module github.com/vishnu9304/mycat go 1.12 When you run go build it will pull all the dependencies and mentioned in our go code. Vishnus-Mac:mycat v01$ go build Vishnus-Mac:mycat v01$ ls go.mod go.sum main.go mycat Vishnus-Mac:mycat v01$ cat go.mod module github.com/vishnu9304/mycat go 1.12 require github.com/sirupsen/logrus v1.4.2 Vishnus-Mac:mycat v01$ cat go.sum github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38= github.com/konsorten/go-windows-terminal-sequences v1.0.1/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ= github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4= github.com/sirupsen/logrus v1.4.2 h1:SPIRibHv4MatM3XXNO2BJeFLZwZ2LvZgfQ5+UNI2im4= github.com/sirupsen/logrus v1.4.2/go.mod h1:tLMulIdttU9McNUspp0xgXVQah82FyeX6MwdIuYE2rE= github.com/stretchr/objx v0.1.1/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME= github.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs= golang.org/x/sys v0.0.0-20190422165155-953cdadca894 h1:Cz4ceDQGXuKRnVBDTS23GTn/pU5OE2C0WrNTOYK1Uuc= golang.org/x/sys v0.0.0-20190422165155-953cdadca894/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs= Under go.mod you can see a new line require github.com/sirupsen/logrus v1.4.2 . Our module depends on that module. You will see one more additional file go.sum which contains a lot of info. On top of the version we are using we also have a HASH J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38= It's a cryptographic hash to track if something that got changed over time. Commonly used commands Command to fetch a module with the latest tag: go get -u github.com/sirupsen/logrus Let's do some clean up: go mod tidy When you do a go ge t or go build your test files are ignored. go mod tidy will also find the modules that your tests are depending on. Where are these downloaded modules stores? It will be stored under GOPATH /go/pkg/mod How to identify why we are depending on some module? Vishnus-Mac:mycat v01$ go mod why github.com/pmezard/go-difflib # github.com/pmezard/go-difflib (main module does not need package github.com/pmezard/go-difflib) Actually we are not using github.com/pmezard/go-difflib directly. We are using some module inside it. To identify that we need to add -m flag. Find something that is in github.com/pmezard/go-difflib Vishnus-Mac:mycat v01$ go mod why -m github.com/pmezard/go-difflib # github.com/pmezard/go-difflib github.com/vishnu9304/mycat github.com/sirupsen/logrus github.com/sirupsen/logrus.test github.com/stretchr/testify/assert github.com/pmezard/go-difflib/difflib Migrating modules to use go mod. Step 1: Clone the repo. Step 2: go mod init Step 3: go mod tidy Step 4: go get ./...","title":"gomod"},{"location":"golang/gomod/#what-is-a-go-module","text":"A module is a way of packaging the software as simple as that. A go module is not atomic and it can contain many packages inside like a repository and it is versioned . Go follows something called Semantic Versioning to version the packages. Also called as semver.","title":"What is a go module?"},{"location":"golang/gomod/#oh-wait-then-what-is-the-difference-between-a-package-and-a-repository","text":"A package cannot be sub divided in to working pieces. If you divide the package in to two different files, still you need those two files to represent the whole pacakge. A package is an atomic thing. A repository contains many packages that can move around independently.","title":"oh wait, then what is the difference between a package and a repository?"},{"location":"golang/gomod/#what-is-versioning-why-do-we-use-it-how-do-we-use-it","text":"Software changes over time. We need some kind of standard way of tracking these changes over period of time. That is where Semantic Versioning comes in to picture. It is a pre straight forward convention of how to name versions.","title":"What is versioning? Why do we use it? How do we use it?"},{"location":"golang/gomod/#example","text":"v1.0.0 v1 - Major Version for every single non backward compatible versions. v1.0 - Minor Version. We increase it when we add a new feature and not broken any backward Compatiblity. v1.0.0 - Patch updates. fixing things that has broken, like security bug fixes.","title":"Example:"},{"location":"golang/gomod/#writing-programs-outside-the-gopath","text":"Command to check default gopath: go env GOPATH","title":"Writing programs outside the GOPATH"},{"location":"golang/gomod/#example_1","text":"package main import ( \"fmt\" \"log\" \"os\" \"github.com/sirupsen/logrus\" ) func main() { _, err := io.Copy(os.Stdout, os.Stdin) if err != { logrus.Fatal(err) } } Here github.com/sirupsen/logrus is a third party package. When you do go get it will complain that it is not able to find the gopath. It cannot store code outside gopath. Vishnus-Mac:mycat v01$ go get go get: no install location for directory /Users/v01/dev/mycat outside GOPATH For more details see: 'go help gopath' Go module comes to rescuse the above scenario. We are going to go from a directory having go code to directory having go module Vishnus-Mac:mycat v01$ go mod init go: cannot determine module path for source directory /Users/v01/dev/mycat (outside GOPATH, no import comments) Again it will complain as you are outside the gopath and it doesn't know what module you are initalizing. We need to provide the import path Vishnus-Mac:mycat v01$ go mod init github.com/vishnu9304/mycat go: creating new go.mod: module github.com/vishnu9304/mycat Vishnus-Mac:mycat v01$ cat go.mod module github.com/vishnu9304/mycat go 1.12 When you run go build it will pull all the dependencies and mentioned in our go code. Vishnus-Mac:mycat v01$ go build Vishnus-Mac:mycat v01$ ls go.mod go.sum main.go mycat Vishnus-Mac:mycat v01$ cat go.mod module github.com/vishnu9304/mycat go 1.12 require github.com/sirupsen/logrus v1.4.2 Vishnus-Mac:mycat v01$ cat go.sum github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38= github.com/konsorten/go-windows-terminal-sequences v1.0.1/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ= github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4= github.com/sirupsen/logrus v1.4.2 h1:SPIRibHv4MatM3XXNO2BJeFLZwZ2LvZgfQ5+UNI2im4= github.com/sirupsen/logrus v1.4.2/go.mod h1:tLMulIdttU9McNUspp0xgXVQah82FyeX6MwdIuYE2rE= github.com/stretchr/objx v0.1.1/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME= github.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs= golang.org/x/sys v0.0.0-20190422165155-953cdadca894 h1:Cz4ceDQGXuKRnVBDTS23GTn/pU5OE2C0WrNTOYK1Uuc= golang.org/x/sys v0.0.0-20190422165155-953cdadca894/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs= Under go.mod you can see a new line require github.com/sirupsen/logrus v1.4.2 . Our module depends on that module. You will see one more additional file go.sum which contains a lot of info. On top of the version we are using we also have a HASH J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38= It's a cryptographic hash to track if something that got changed over time.","title":"Example:"},{"location":"golang/gomod/#commonly-used-commands","text":"Command to fetch a module with the latest tag: go get -u github.com/sirupsen/logrus Let's do some clean up: go mod tidy When you do a go ge t or go build your test files are ignored. go mod tidy will also find the modules that your tests are depending on.","title":"Commonly used commands"},{"location":"golang/gomod/#where-are-these-downloaded-modules-stores","text":"It will be stored under GOPATH /go/pkg/mod","title":"Where are these downloaded modules stores?"},{"location":"golang/gomod/#how-to-identify-why-we-are-depending-on-some-module","text":"Vishnus-Mac:mycat v01$ go mod why github.com/pmezard/go-difflib # github.com/pmezard/go-difflib (main module does not need package github.com/pmezard/go-difflib) Actually we are not using github.com/pmezard/go-difflib directly. We are using some module inside it. To identify that we need to add -m flag. Find something that is in github.com/pmezard/go-difflib Vishnus-Mac:mycat v01$ go mod why -m github.com/pmezard/go-difflib # github.com/pmezard/go-difflib github.com/vishnu9304/mycat github.com/sirupsen/logrus github.com/sirupsen/logrus.test github.com/stretchr/testify/assert github.com/pmezard/go-difflib/difflib","title":"How to identify why we are depending on some module?"},{"location":"golang/gomod/#migrating-modules-to-use-go-mod","text":"Step 1: Clone the repo. Step 2: go mod init Step 3: go mod tidy Step 4: go get ./...","title":"Migrating modules to use go mod."},{"location":"golang/types/","text":"Boolean true false Numeric Signed Integers - int8, int16, int32, int64, int UnSigned Integers - uint8, uint16, uint32, uint64, uint Floaring Point - float32, float64 Complex - complex64, complex128 byte is an alias of uint8 rune is an alias of int32 bool A bool type represents a boolean and is either true or false a := true b := false fmt.Println(\"a:\", a, \"b:\", b) c := a b fmt.Println(\"c:\", c) d := a || b fmt.Println(\"d:\", d) operator returns true only when both a and b are true. || operator returns true when either a or b is true. Signed integers int8: represents 8 bit signed integers size: 8 bits range: -128 to 127 int16: represents 16 bit signed integers size: 16 bits range: -32768 to 32767 int32: represents 32 bit signed integers size: 32 bits range: -2147483648 to 2147483647 int64: represents 64 bit signed integers size: 64 bits range: -9223372036854775808 to 9223372036854775807 Note: int represents 32 or 64 bit integers depending on the underlying platform. size: 32 bits in 32 bit systems and 64 bit in 64 bit systems. The type of a variable can be printed using %T format specifier in Printf function. Unsigned integers uint8: represents 8 bit unsigned integers size: 8 bits range: 0 to 255 uint16: represents 16 bit unsigned integers size: 16 bits range: 0 to 65535 uint32: represents 32 bit unsigned integers size: 32 bits range: 0 to 4294967295 uint64: represents 64 bit unsigned integers size: 64 bits range: 0 to 18446744073709551615 Note: uint represents 32 or 64 bit unsigned integers depending on the underlying platform. size : 32 bits in 32 bit systems and 64 bits in 64 bit systems. Sizeof(variable_name) method from unsafe package is used to check the size of the variables. Floating point types float32: 32 bit floating point numbers float64: 64 bit floating point numbers Complex types complex64: complex numbers which have float32 real and imaginary parts complex128: complex numbers with float64 real and imaginary parts Type Conversion The below code is not allowed in go i := 55 j := 67.8 sum := i + j To fix the above problem both i and j should be of the same type. i := 55 //int j := 67.8 //float64 sum := i + int(j) //j is converted to int","title":"types"},{"location":"golang/types/#bool","text":"A bool type represents a boolean and is either true or false a := true b := false fmt.Println(\"a:\", a, \"b:\", b) c := a b fmt.Println(\"c:\", c) d := a || b fmt.Println(\"d:\", d) operator returns true only when both a and b are true. || operator returns true when either a or b is true.","title":"bool"},{"location":"golang/types/#signed-integers","text":"int8: represents 8 bit signed integers size: 8 bits range: -128 to 127 int16: represents 16 bit signed integers size: 16 bits range: -32768 to 32767 int32: represents 32 bit signed integers size: 32 bits range: -2147483648 to 2147483647 int64: represents 64 bit signed integers size: 64 bits range: -9223372036854775808 to 9223372036854775807 Note: int represents 32 or 64 bit integers depending on the underlying platform. size: 32 bits in 32 bit systems and 64 bit in 64 bit systems. The type of a variable can be printed using %T format specifier in Printf function.","title":"Signed integers"},{"location":"golang/types/#unsigned-integers","text":"uint8: represents 8 bit unsigned integers size: 8 bits range: 0 to 255 uint16: represents 16 bit unsigned integers size: 16 bits range: 0 to 65535 uint32: represents 32 bit unsigned integers size: 32 bits range: 0 to 4294967295 uint64: represents 64 bit unsigned integers size: 64 bits range: 0 to 18446744073709551615 Note: uint represents 32 or 64 bit unsigned integers depending on the underlying platform. size : 32 bits in 32 bit systems and 64 bits in 64 bit systems. Sizeof(variable_name) method from unsafe package is used to check the size of the variables.","title":"Unsigned integers"},{"location":"golang/types/#floating-point-types","text":"float32: 32 bit floating point numbers float64: 64 bit floating point numbers","title":"Floating point types"},{"location":"golang/types/#complex-types","text":"complex64: complex numbers which have float32 real and imaginary parts complex128: complex numbers with float64 real and imaginary parts","title":"Complex types"},{"location":"golang/types/#type-conversion","text":"The below code is not allowed in go i := 55 j := 67.8 sum := i + j To fix the above problem both i and j should be of the same type. i := 55 //int j := 67.8 //float64 sum := i + int(j) //j is converted to int","title":"Type Conversion"},{"location":"golang/variables/","text":"Declaring a single variable: var [variable_name] type If a variable is not assigned any value, go automatically initialises it with the zero value of the variable's type. Declaring a variable with initial value var [variable_name] = [initial_value] Type inference If a variable has an initial value, Go will automatically be able to infer the type of that variable using that initial value. var [variable_name] = [initial_value] var age = 20 Multiple variable declaration var [variable_name1] [variable_name2] [variable_name3] = [initial_value1], [initial_value2], [initial_value3] Declaring variables belonging to different types in a single statement. Example: var ( [variable_name1] = initialvalue1 [variable_name2] = initialvalue2 ) Short hand declaration Note: This will work only inside function block. variable_name := initialvalue Short hand declaration requires initial values for all variables in the left hand side of the assignment. Bad Syntax: value for variable_name2 is missing. variable_name1, variable_name2 := initialvalue1 Short hand syntax can only be used when at least one of the variables in the left side of := is newly declared. a, b := 20, 30 a, b := 40, 50 it will throw error no new variables on left side of := This is because both the variables a and b have already been declared and there are no new variables in the left side of := Go is strongly typed, variables declared as belonging to one type cannot be assigned a value of another type. Bad Syntax: age := 29 // Age is of type int age = \"vishnu\" // Age is of type string","title":"variables"},{"location":"k8s/clustering&nodes/","text":"Kubernetes implements as clustered architecture. In a typical production environment, you will have multiple servers that are able to run your workloads (containers). These servers that actually run the containers are called nodes. A k8s cluster will have one or more control servers, which manage the cluster and hosts the k8's API. These control servers are separate from worker nodes, which runs the containers containing actual applications. Commands: kubectl get nodes - to get list of nodes. kubectl describe node [nodename] - to get more information about a particular node.","title":"k8s Clustering & nodes"},{"location":"k8s/deploymets/","text":"Deployments are a type of objects on k8s, which helps to run and maintain pods. It helps us to specify a desired state for set of pods. The cluster will then contantly work to maintain that state. Example: Scaling: With deployment you can specify the number of replicas you want, the deployment will create pods to meet that number of replicas. Rolling Updates: With a deployment, you can change the deployment container image to a new version of the image. The deployment will gradually replace the existing containers with new version. Self Healing: If one of the pods in the deployment is accidentally destroyed, the deployment will spin up the new pod immediately.","title":"k8s Deployment"},{"location":"k8s/k8s components/","text":"Kubernetes includes multiple components that work together to provide the functionality of a k8s cluster. The control plane components manage and control cluster: etcd : provides distributed, synchronized data storage for the cluster state. kube-apiserver : servers the k8's REST API, the primary interface for the cluster. kube-controller manager : bundles several components into one package. kube-scheduler : schedules pods to run on individual nodes. kubelet agents that runs on each nodes act's as middle men between kube api server and container runtime. kubeproxy handles network communication between pods across nodes.","title":"k8s Components"},{"location":"k8s/k8s/","text":"What is K8's? With containers, you can run a variety of software components across a cluster of generic servers. This can help to ensure availability and make it easier to scale resources. Oh! wait...! I have few questions. How can I ensure multiple instances of a piece of software are spread across multiple servers for high availability? How can I deploy new code changes and roll them out across the entire cluster? How can I create new containers to scale up to handle additional load? K8's (Orchestration Tool) is the answer for all the above questions. K8's Architecture List of components: docker - The container runtime to execute the containers on each nodes. kubeadm - Tool that simplifies the process of setting up the cluster. kubelet - Agent that manages the process of running the containers on each nodes. kubectl - Tool to interact with the cluster once it is up. Control Plane (only on k8's master) - Series of services that forms the k8's master. Note : The k8's control plane services will be running on containers under k8's master. Installing packages 1) Installing docker: https://docs.docker.com/install/linux/docker-ce/centos/#install-using-the-repository https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl How to lock a yum package at a particulat version? /etc/yum/pluginconf.d/versionlock.conf sudo yum versionlock packagename yum versionlock delete packagename sudo yum versionlock docker-ce.x86_64 0:17.12.1.ce-1.el7.centos 2) Installing K8s components (Kubeadm, Kubelet, and Kubectl): https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ Important ports: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#check-required-ports sudo yum install kubelet-1.15.6-0 kubeadm-1.15.6-0 kubectl-1.15.6-0 --disableexcludes=kubernetes Bootstrapping sudo setenforce 0 sudo vi /etc/selinux/config Change the line that says SELINUX=enforcing to SELINUX=permissive and save the file. sudo systemctl enable kubelet sudo systemctl start kubelet sudo vi /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 sudo sysctl --system sudo swapoff -a sudo kubeadm init --pod-network-cidr=10.244.0.0/16 You will get a below message. To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 10.82.219.85:6443 --token token --discovery-token-ca-cert-hash sha256: HASH kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml kubectl get nodes Container Runtimes Recommended Links: https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r http://alexander.holbreich.org/docker-components-explained/ https://medium.com/faun/docker-containerd-standalone-runtimes-heres-what-you-should-know-b834ef155426 https://opensource.com/article/18/1/history-low-level-container-runtimes How to identify which runtime the docker is using? sudo docker info | grep -i runtime Runtimes: runc Default Runtime: runc How to add another runtime? docker daemon --add-runtime \" runtime-name = runtime-path \" Example: docker daemon --add-runtime \"oci=/usr/local/sbin/runc\" What is containerd? containerd includes a daemon exposing gRPC API over a local UNIX socket. The API is a low-level one designed for higher layers to wrap and extend. Containerd uses RunC to run containers according to the OCI specification. Runc subsystem of containerd /usr/bin/docker-runc runc (OCI runtime) canbe seen as component of containerd. runc is a command line client for running applications packaged according to the OCI format and is a compliant implementation of the OCI spec. Containers are configured using bundles. A bundle for a container is a directory that includes a specification file named \"config.json\" and a root filesystem. The root filesystem contains the contents of the container. Assuming you have an OCI bundle you can execute the container: cd /mycontainer runc run mycontainerid containerd-shim It allows the runtimes, i.e. runc,to exit after it starts the container. This way we don't have to have the long running runtime processes for containers. https://www.ianlewis.org/en/container-runtimes-part-4-kubernetes-container-run OCI Specification: https://github.com/opencontainers/runtime-spec/blob/master/spec.md","title":"K8s"},{"location":"k8s/k8s/#what-is-k8s","text":"With containers, you can run a variety of software components across a cluster of generic servers. This can help to ensure availability and make it easier to scale resources. Oh! wait...! I have few questions. How can I ensure multiple instances of a piece of software are spread across multiple servers for high availability? How can I deploy new code changes and roll them out across the entire cluster? How can I create new containers to scale up to handle additional load? K8's (Orchestration Tool) is the answer for all the above questions.","title":"What is K8's?"},{"location":"k8s/k8s/#k8s-architecture","text":"List of components: docker - The container runtime to execute the containers on each nodes. kubeadm - Tool that simplifies the process of setting up the cluster. kubelet - Agent that manages the process of running the containers on each nodes. kubectl - Tool to interact with the cluster once it is up. Control Plane (only on k8's master) - Series of services that forms the k8's master. Note : The k8's control plane services will be running on containers under k8's master.","title":"K8's Architecture"},{"location":"k8s/k8s/#installing-packages","text":"1) Installing docker: https://docs.docker.com/install/linux/docker-ce/centos/#install-using-the-repository https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl How to lock a yum package at a particulat version? /etc/yum/pluginconf.d/versionlock.conf sudo yum versionlock packagename yum versionlock delete packagename sudo yum versionlock docker-ce.x86_64 0:17.12.1.ce-1.el7.centos 2) Installing K8s components (Kubeadm, Kubelet, and Kubectl): https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ Important ports: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#check-required-ports sudo yum install kubelet-1.15.6-0 kubeadm-1.15.6-0 kubectl-1.15.6-0 --disableexcludes=kubernetes","title":"Installing packages"},{"location":"k8s/k8s/#bootstrapping","text":"sudo setenforce 0 sudo vi /etc/selinux/config Change the line that says SELINUX=enforcing to SELINUX=permissive and save the file. sudo systemctl enable kubelet sudo systemctl start kubelet sudo vi /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 sudo sysctl --system sudo swapoff -a sudo kubeadm init --pod-network-cidr=10.244.0.0/16 You will get a below message. To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 10.82.219.85:6443 --token token --discovery-token-ca-cert-hash sha256: HASH kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml kubectl get nodes","title":"Bootstrapping"},{"location":"k8s/k8s/#container-runtimes","text":"Recommended Links: https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r http://alexander.holbreich.org/docker-components-explained/ https://medium.com/faun/docker-containerd-standalone-runtimes-heres-what-you-should-know-b834ef155426 https://opensource.com/article/18/1/history-low-level-container-runtimes How to identify which runtime the docker is using? sudo docker info | grep -i runtime Runtimes: runc Default Runtime: runc How to add another runtime? docker daemon --add-runtime \" runtime-name = runtime-path \" Example: docker daemon --add-runtime \"oci=/usr/local/sbin/runc\" What is containerd? containerd includes a daemon exposing gRPC API over a local UNIX socket. The API is a low-level one designed for higher layers to wrap and extend. Containerd uses RunC to run containers according to the OCI specification. Runc subsystem of containerd /usr/bin/docker-runc runc (OCI runtime) canbe seen as component of containerd. runc is a command line client for running applications packaged according to the OCI format and is a compliant implementation of the OCI spec. Containers are configured using bundles. A bundle for a container is a directory that includes a specification file named \"config.json\" and a root filesystem. The root filesystem contains the contents of the container. Assuming you have an OCI bundle you can execute the container: cd /mycontainer runc run mycontainerid containerd-shim It allows the runtimes, i.e. runc,to exit after it starts the container. This way we don't have to have the long running runtime processes for containers. https://www.ianlewis.org/en/container-runtimes-part-4-kubernetes-container-run OCI Specification: https://github.com/opencontainers/runtime-spec/blob/master/spec.md","title":"Container Runtimes"},{"location":"k8s/networking/","text":"The k8s networking model involves creating a virtual network across the whole cluster. Every pod on the cluster will have unique IP address, and can communicate with any other pod in the cluster, even that pod is running on a different node. K8s supports a variety of networking plugins that implement this model in various ways. Example: Flannel Flannel is the plugin that creates this virtual network and allows pods to communicate with each other reagardless of which node they are in. Example: Contacting one pod from another pod in the virtual network. apiVersion: apps/v1 kind: Deployment metadata: name: nginx labels: app: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 kubectl create -f deployment.yaml kubectl get pods -o wide kubectl exec busybox --curl $nginx_pod_ip","title":"k8s Networking"},{"location":"k8s/primitives/","text":"Pods are the smallest and most basic building block of the kubernetes model. A pod consist of one or more containers, storage resources and unique ip addresses in the k8's cluster network. In order to run containers, kubernetes schedules pods to run on servers in the cluster. When a pod is scheduled, the server will run the containers that are part of that pod. All objects in k8s will be having yaml definition. Example: nginx.yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx Note: While working with k8s, every object will be in a namespace. Commands: kubectl get pods --all-namespaces -o wide - To get pods from all the namespaces. kubectl create -f [yaml definition file] - To create k8s objects. kubectl get pods - To get list of running pods. If no \"namspace\" is specified \"default\" namespace will be considered. kubectl describe pod [podname] - To get bunch of information about a particulat pod. kubectl get pods -n kube-system - To list pods under \"kube-system\" namespace. kubectl delete pod [podname] - To delete a pod. Namespaces: Multiple virtual clusters are backed by the same physical cluster. Generally for large deployments. Provide scope for names. Easy way to divide cluster resources. Allows multiple team of users. Allows for resource quotas. Special \"kube-system\" namespace, used to differentiate pods from user pods. Note: we can use lables to distinguish between two slightly different objects (Namespaces not required here). Core Concepts: Inter cluster communications happens with a call to API server. Every single k8's system components communicate with API server and they do not talk to each other directly. API server is the only one that communicates with the \"etcd\" data store. The kubectl command line utility can be used to create,update,delete and modify the api objects. Note: The api server exposes a resource called as component status which will show all the components of the control plane and status of each. kubectl get componentstatus kubectl create -f pod.yaml kubectl get deployments kubectl get deployments nginx-deployment -o yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \"1\" creationTimestamp: \"2019-11-24T16:15:19Z\" generation: 1 name: nginx-deployment namespace: default resourceVersion: \"154730\" selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/nginx-deployment uid: fb187e22-5301-4cd3-91d3-d326f3861829 spec: progressDeadlineSeconds: 600 replicas: 2 revisionHistoryLimit: 10 selector: matchLabels: app: nginx strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: nginx spec: containers: - image: nginx:1.7.9 imagePullPolicy: IfNotPresent name: nginx ports: - containerPort: 80 protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 status: availableReplicas: 2 conditions: - lastTransitionTime: \"2019-11-24T16:15:37Z\" lastUpdateTime: \"2019-11-24T16:15:37Z\" message: Deployment has minimum availability. reason: MinimumReplicasAvailable status: \"True\" type: Available - lastTransitionTime: \"2019-11-24T16:15:19Z\" lastUpdateTime: \"2019-11-24T16:15:37Z\" message: ReplicaSet \"nginx-deployment-5754944d6c\" has successfully progressed. reason: NewReplicaSetAvailable status: \"True\" type: Progressing observedGeneration: 1 readyReplicas: 2 replicas: 2 updatedReplicas: 2 Labels: kubectl get pods --show-labels kubectl label pods [podname] env=label kubectl get pods -L [label name] kubectl get pods -o yaml kubectl get pods --field-selector status.phase=Running kubectl get pods --all-namespaces -L component kubectl get componentstatus","title":"K8s Basics"},{"location":"linux/CPU and Load Average/","text":"CPU's and Load Averages Key points about CPU's: CPU's comes with a rated speed and it is calculates in Hertz (1 Hertz=1 Cycles Per Second). Example: 1.6GHZ (1.6 Billons Per Second). Motherboard crystals play a major role in driving the CPU speed. Key point to note here is, the mother board crystal oscillates at some frequency (in MHZ mostly). CPU clock speed = cpu multiplier x Mother Board Clock Speed. Overclocking is the process of pushing the CPU beyond it's rated speed. -bash-4.1$ lscpu Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian CPU(s): 64 On-line CPU(s) list: 0-63 Thread(s) per core: 2 Core(s) per socket: 16 Socket(s): 2 NUMA node(s): 2 Vendor ID: GenuineIntel CPU family: 6 Model: 79 Model name: Intel(R) Xeon(R) CPU E5-2683 v4 @ 2.10GHz From above data, we have 2 cpu sockets , each processor is having 16 cores and 2 threads per core (hyperthreading enabled). Note: hyperthreading is only for intel processors. CPU Caches: Programs resides in Hard drive. When you double click on any program some of the files will be copied from Hard drive to RAM. Accessing data from RAM is lot faster than Hard drive but it is still quite slower than CPU itself. There is a static RAM inside the CPU itself, we call it as caches (L1, L2, L3). L1 caches are the smallest and fastest cache. Understanding Linux CPU Load averages: Source: https://scoutapm.com/blog/understanding-load-averages cpu load average: The number of processes currently running plus the number of processes yet to be processed. Unix refers to this as the run-queue length: the sum of the number of processes that are currently running plus the number that are waiting (queued) to run. \"Need to Look into it\" If your load average is staying above 0.70, it's time to investigate before things get worse. \"Fix this now\" If your load average stays above 1.00, find the problem and fix it now. \"Arrgh, it's 3AM WTF?\" If your load average is above 5.00, you could be in serious trouble, your box is either hanging or slowing way down, and this will (inexplicably) happen in the worst possible time like in the middle of the night or when you're presenting at a conference. On multi-processor system, the load is relative to the number of processor cores available. The \"100% utilization\" mark is 1.00 on a single-core system, 2.00, on a dual-core, 4.00 on a quad-core, etc. Rule of thumb: The \"number of cores = max load\" Rule of Thumb: on a multicore system, your load should not exceed the number of cores available. # of cores is important to interpreting load averages ... how do I know how many cores my system has? cat /proc/cpuinfo to get info on each processor in your system.","title":"CPU's"},{"location":"linux/CPU and Load Average/#cpus-and-load-averages","text":"","title":"CPU's and Load Averages"},{"location":"linux/CPU and Load Average/#key-points-about-cpus","text":"CPU's comes with a rated speed and it is calculates in Hertz (1 Hertz=1 Cycles Per Second). Example: 1.6GHZ (1.6 Billons Per Second). Motherboard crystals play a major role in driving the CPU speed. Key point to note here is, the mother board crystal oscillates at some frequency (in MHZ mostly). CPU clock speed = cpu multiplier x Mother Board Clock Speed. Overclocking is the process of pushing the CPU beyond it's rated speed. -bash-4.1$ lscpu Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian CPU(s): 64 On-line CPU(s) list: 0-63 Thread(s) per core: 2 Core(s) per socket: 16 Socket(s): 2 NUMA node(s): 2 Vendor ID: GenuineIntel CPU family: 6 Model: 79 Model name: Intel(R) Xeon(R) CPU E5-2683 v4 @ 2.10GHz From above data, we have 2 cpu sockets , each processor is having 16 cores and 2 threads per core (hyperthreading enabled). Note: hyperthreading is only for intel processors. CPU Caches: Programs resides in Hard drive. When you double click on any program some of the files will be copied from Hard drive to RAM. Accessing data from RAM is lot faster than Hard drive but it is still quite slower than CPU itself. There is a static RAM inside the CPU itself, we call it as caches (L1, L2, L3). L1 caches are the smallest and fastest cache.","title":"Key points about CPU's:"},{"location":"linux/CPU and Load Average/#understanding-linux-cpu-load-averages","text":"Source: https://scoutapm.com/blog/understanding-load-averages cpu load average: The number of processes currently running plus the number of processes yet to be processed. Unix refers to this as the run-queue length: the sum of the number of processes that are currently running plus the number that are waiting (queued) to run. \"Need to Look into it\" If your load average is staying above 0.70, it's time to investigate before things get worse. \"Fix this now\" If your load average stays above 1.00, find the problem and fix it now. \"Arrgh, it's 3AM WTF?\" If your load average is above 5.00, you could be in serious trouble, your box is either hanging or slowing way down, and this will (inexplicably) happen in the worst possible time like in the middle of the night or when you're presenting at a conference. On multi-processor system, the load is relative to the number of processor cores available. The \"100% utilization\" mark is 1.00 on a single-core system, 2.00, on a dual-core, 4.00 on a quad-core, etc. Rule of thumb: The \"number of cores = max load\" Rule of Thumb: on a multicore system, your load should not exceed the number of cores available. # of cores is important to interpreting load averages ... how do I know how many cores my system has? cat /proc/cpuinfo to get info on each processor in your system.","title":"Understanding Linux CPU Load averages:"},{"location":"linux/Filesystem/","text":"Display filesystem mounting information How to display mount information? mount /dev/vda1 on / type ext4 (rw) proc on /proc type proc (rw) sysfs on /sys type sysfs (rw) devpts on /dev/pts type devpts (rw,gid=5,mode=620) tmpfs on /dev/shm type tmpfs (rw) none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw) Where does the mount command gets the above information? I get the details from the file /etc/mtab This file will get updated whenever a mount point gets mounted and un-mounted. How to skip updating /etc/mtab file? mount -n (-n option) Note: /etc/mtab file will not have the up-to-date information about the mount points. The updated information about the mount points will be present under /proc/mounts Mounting and Un-Mounting Filesystems How to check which process is using files and directories on a mounted partitions? fuser -v / (-v verbose) fuser -k /mnt/data (identify and kill all the processes using the mount point \"/mnt/data\") How to un-mount and force un-mount a file system? unmount /mnt/data (Clean unmount) unmount -f /mnt/data (Force unmount) How to mount filesystem? mount -t ext3 /dev/xvdj1 /mnt/data How to remount a filesystem? mount -o remount,ro /mnt/data Mounting and Unmounting filesystems Automatically with /etc/fstab Entry inside fstab source destination fstype options /dev/xvdj1 /mnt/data ext3 defaults 0 0 How to label the drives? sudo e2label /dev/xvdj1 mydata go to /etc/fstab and add an entry like below. LABEL=mydata /mnt/data ext3 defaults 0 0 What is the most stable method of mounting device? Using blkid utility, which give us the UUID of the devices, which are unique for each devices. Two ways to get UUID, one is using blkid utility and the other one is ls -la /dev/disk/by-uuid/ Copy the UUID value and add it under /etc/fstab Swap space swapon -s (To list swap partitions) swapon -a (Enable swap) swapoff -a (disable swap) Filesystem Types and Creating Them Four key filesystems ext{2-4}, xfs Commands to create and format the file systems: Step1: Create the parition fdisk -l (list all the devices that are detected, whether they are currently partitioned or not) fdisk /dev/xvdj (format the device /dev/xvdj) Step 2: Create filesystem on top of partition mkfs -t ext4 /dev/xvdj1 mkfs -t xfs /dev/xvdj1 Step 3: Mount and start Writing filesystem checks Note: Don't run filesystem checks on a mounted filesystem How to print more detailes information about a given partition (only ext based)? dumpe2fs /dev/xvdf1 dumpe2fs -h /dev/xvdf1 dumpe2fs provides a lot of information about the physical filesystem, how to modify these values? (only ext based) tune2fs There is an one more utility which will give an interactive prompt debug2fs Maintaining a Linux Filesystem (Change and View XFS Based Filesystems) Install the xfs programs. yum install xfsprogs xfs_info /dev/xvdf1 provides information about the xfs filesystem. xfs_repair /dev/xvdf1 to check and fix xfs filesystem. (Makesure the filesystem is un-mounted) xfs_repair -n /dev/xvdf1 only check don't repair. (Makesure the filesystem is un-mounted) xfsdump -f /mnt/backup/backupfile /mnt/xfsdata to take backup of xfs filesystem. xfsrestore -f /mnt/backup/backupfile /mnt/restore to restore backup. xfsrestore -I provide information about the backup history. We can restore xfs backup to an non-xfs filesystem. SMART and smartd (Self Monitoring, Analysis and Reporting Technology) Built in to ATA, IDE, SCSI-3 hard drives Designed to monitor the reliablity of the drive What is smartd? A daemon that interacts with SMART functionality on drives. By default it queries every disk a 30mins interval and configurable. Errors are logged to syslog (/var/log/message, configurable) smartctl -i /dev/sda More at: https://www.thomas-krenn.com/en/wiki/SMART_tests_with_smartctl","title":"Filesystems"},{"location":"linux/Filesystem/#display-filesystem-mounting-information","text":"How to display mount information? mount /dev/vda1 on / type ext4 (rw) proc on /proc type proc (rw) sysfs on /sys type sysfs (rw) devpts on /dev/pts type devpts (rw,gid=5,mode=620) tmpfs on /dev/shm type tmpfs (rw) none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw) Where does the mount command gets the above information? I get the details from the file /etc/mtab This file will get updated whenever a mount point gets mounted and un-mounted. How to skip updating /etc/mtab file? mount -n (-n option) Note: /etc/mtab file will not have the up-to-date information about the mount points. The updated information about the mount points will be present under /proc/mounts","title":"Display filesystem mounting information"},{"location":"linux/Filesystem/#mounting-and-un-mounting-filesystems","text":"How to check which process is using files and directories on a mounted partitions? fuser -v / (-v verbose) fuser -k /mnt/data (identify and kill all the processes using the mount point \"/mnt/data\") How to un-mount and force un-mount a file system? unmount /mnt/data (Clean unmount) unmount -f /mnt/data (Force unmount) How to mount filesystem? mount -t ext3 /dev/xvdj1 /mnt/data How to remount a filesystem? mount -o remount,ro /mnt/data","title":"Mounting and Un-Mounting Filesystems"},{"location":"linux/Filesystem/#mounting-and-unmounting-filesystems-automatically-with-etcfstab","text":"Entry inside fstab source destination fstype options /dev/xvdj1 /mnt/data ext3 defaults 0 0 How to label the drives? sudo e2label /dev/xvdj1 mydata go to /etc/fstab and add an entry like below. LABEL=mydata /mnt/data ext3 defaults 0 0 What is the most stable method of mounting device? Using blkid utility, which give us the UUID of the devices, which are unique for each devices. Two ways to get UUID, one is using blkid utility and the other one is ls -la /dev/disk/by-uuid/ Copy the UUID value and add it under /etc/fstab","title":"Mounting and Unmounting filesystems Automatically with /etc/fstab"},{"location":"linux/Filesystem/#swap-space","text":"swapon -s (To list swap partitions) swapon -a (Enable swap) swapoff -a (disable swap)","title":"Swap space"},{"location":"linux/Filesystem/#filesystem-types-and-creating-them","text":"Four key filesystems ext{2-4}, xfs Commands to create and format the file systems: Step1: Create the parition fdisk -l (list all the devices that are detected, whether they are currently partitioned or not) fdisk /dev/xvdj (format the device /dev/xvdj) Step 2: Create filesystem on top of partition mkfs -t ext4 /dev/xvdj1 mkfs -t xfs /dev/xvdj1 Step 3: Mount and start Writing filesystem checks Note: Don't run filesystem checks on a mounted filesystem How to print more detailes information about a given partition (only ext based)? dumpe2fs /dev/xvdf1 dumpe2fs -h /dev/xvdf1 dumpe2fs provides a lot of information about the physical filesystem, how to modify these values? (only ext based) tune2fs There is an one more utility which will give an interactive prompt debug2fs Maintaining a Linux Filesystem (Change and View XFS Based Filesystems) Install the xfs programs. yum install xfsprogs xfs_info /dev/xvdf1 provides information about the xfs filesystem. xfs_repair /dev/xvdf1 to check and fix xfs filesystem. (Makesure the filesystem is un-mounted) xfs_repair -n /dev/xvdf1 only check don't repair. (Makesure the filesystem is un-mounted) xfsdump -f /mnt/backup/backupfile /mnt/xfsdata to take backup of xfs filesystem. xfsrestore -f /mnt/backup/backupfile /mnt/restore to restore backup. xfsrestore -I provide information about the backup history. We can restore xfs backup to an non-xfs filesystem.","title":"Filesystem Types and Creating Them"},{"location":"linux/Filesystem/#smart-and-smartd-self-monitoring-analysis-and-reporting-technology","text":"Built in to ATA, IDE, SCSI-3 hard drives Designed to monitor the reliablity of the drive What is smartd? A daemon that interacts with SMART functionality on drives. By default it queries every disk a 30mins interval and configurable. Errors are logged to syslog (/var/log/message, configurable) smartctl -i /dev/sda More at: https://www.thomas-krenn.com/en/wiki/SMART_tests_with_smartctl","title":"SMART and smartd (Self Monitoring, Analysis and Reporting Technology)"},{"location":"linux/Measure and Troubleshoot Resource Utilization/","text":"Measure and Troubleshoot Resource Utilization uptime The current time, how long the system has been running, how many users are currently logged on, and the system load averages for the past 1, 5, and 15 minutes. 05:25:34 up 0 min, 1 user, load average: 0.39, 0.10, 0.03 Note: Uptime depends on binary formatted file \"/var/run/utmp\". iostat Report Central Processing Unit (CPU) statistics and input/output statistics for devices, partitions and network filesystems (NFS). The iostat command generates three types of reports: CPU Utilization report Device Utilization report Network Filesystem report Display only CPU Utilization report: iostat -c Display only disk/block device stats: iostat -d Display CPU Utilization report at regular interval for 'n' number of times: iostat -c 2 5 (Every 2 seconds for 5 times) Note: refer man pages if you don't understand the keywords displayed in the output. Example: %iowait, the percentage of time that the CPU or CPUs were idle during which the system had an outstanding disk I/O request. sar sar - Collect, report, or save system activity information. sar provides historical stats on disk utilization for block devices and CPU. Display CPU stats: sar | head Display block devices stats: sar -d Note: sar command is provided by sysstat package (provides iostat aswell). When you install sysstat you have to start and enable it. When you start sysstat utility it creates a file under /var/log/sa/sa[num] . This is where it stores all the stats for every 10mins. [num] two digit number represents the date. It will be recycled every 30days . /var/log/sa/sa[num] is binay formatted. sar is desined to read this file and generate reports. Memory: free free Display amount of free and used memory in the system. free -m -h -s 1 Display the output in MB / Shorten / Display the results for seconds until three iterations. free -m [v01@equatescrates ~]$ free -m total used free shared buffers cached Mem: 3705 807 2897 0 61 438 -/+ buffers/cache: 306 3398 Swap: 0 0 0 According to above output we have only have 2897 mb of memory free. But, actual available free memory is free(2897) + cached(438) + buffers(61) = 3398 Major Page Fault: is a request done to fetch pages from the hard disk and buffer it to RAM. Minor Page Fault: If the data is present in the RAM buffer cache, the processor can issue a Minor Page fault. https://www.slashroot.in/linux-system-io-monitoring article explains the logic behind the free command. Memory: vmstat vmstat provides full summary of all memory utilization (idle memory/buffer memory etc..) on our system. procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 1991612 262060 871784 0 0 35 30 178 263 2 1 95 2 0 Display active/inactive memory: vmstat -a Active Memory: The memory that is being used by a particular process. Inactive Memory: The pages which have not been accessed \"recently\". Run every 5 seconds: vmstat 5 Run every 5 seconds with a timestamp: vmstat 5 -t Display a table of various event counters and memory statistics: vmstat -s Disk and Files: lsof Example: Let's say you have identified busy disk or high cpu utilization using above discussed command. To find what files, utilities or applications are open and might be using that. We can look at that by system and end users. Display number of file currently open: lsof | wc -l Display number of files that are opened by a particular user: lsof -u [user] | wc -l ps, pstree, top How do I get detailed information about process running on our system? Display all running processes on the system: ps -e Diplay full format: ps -ef Display the process hierarchy: pstree Display the process hierarchy and highlight the current process: pstree -h Display the process hierarchy along with pid's and highlight the current process: pstree -h -p Display the process hierarchy along with pid's and highlight the current process along with command line parameters: pstree -h -p -a Note: All of the commands are great. But they are not interactive. top command used to get process information in an interactive way. Before that we need to understand what a load average is? and what are tasks? Notes for top command : There are flags that are used to change the behaviour of the top command. Search online for more flags. Press \"f\" to add or remove fields \"shift + R\" to sort it in reverse, the process with less cpu time first. Press \"n\" then 5 for top five process. Press \"d\" to change the update duration. Press \"k\" to kill and enter the \"PID\" number to kill a process. You can pass any valid linux kill signal (default is 15). iotop, ss, netstat ss command is similar to netstat, the ss command is used to dump the socket statistics. netstat uses /proc/net/tcp to gather system network information, while ss queries the kernel directly using the Netlink socket interface.s iotop: The top command shows the Memory and CPU usage for open programs, iotop shows disk usage information for open processes. To run on non-interactive mode: iotop -b (or) iotop --batch To set the number of interactions before quitting: iotop -N 10 (or) iotop --iter=10 To set the delay between interactions: iotop -d 10 (or) iotop --delay=10 To monitor particular processes: iotop -p 1234 2345 3456 (or) iotop --pid=1234 To monitor a list of users: iotop -u mike mark (or) iotop --user=mike To show only processes: iotop -P (or) iotop --processes To Show accumulated I/O instead of bandwidth: iotop -a (or) iotop --accumulated To Use kilobytes instead of a human friendly unit: iotop -k (or) iotop --kilobytes To add a time stamp on each line: iotop -t (or) iotop --time To suppress some lines of header: iotop -q (or) iotop --quiet","title":"Troubleshoot Resource Utilization"},{"location":"linux/Measure and Troubleshoot Resource Utilization/#measure-and-troubleshoot-resource-utilization","text":"","title":"Measure and Troubleshoot Resource Utilization"},{"location":"linux/Measure and Troubleshoot Resource Utilization/#uptime","text":"The current time, how long the system has been running, how many users are currently logged on, and the system load averages for the past 1, 5, and 15 minutes. 05:25:34 up 0 min, 1 user, load average: 0.39, 0.10, 0.03 Note: Uptime depends on binary formatted file \"/var/run/utmp\".","title":"uptime"},{"location":"linux/Measure and Troubleshoot Resource Utilization/#iostat","text":"Report Central Processing Unit (CPU) statistics and input/output statistics for devices, partitions and network filesystems (NFS). The iostat command generates three types of reports: CPU Utilization report Device Utilization report Network Filesystem report Display only CPU Utilization report: iostat -c Display only disk/block device stats: iostat -d Display CPU Utilization report at regular interval for 'n' number of times: iostat -c 2 5 (Every 2 seconds for 5 times) Note: refer man pages if you don't understand the keywords displayed in the output. Example: %iowait, the percentage of time that the CPU or CPUs were idle during which the system had an outstanding disk I/O request.","title":"iostat"},{"location":"linux/Measure and Troubleshoot Resource Utilization/#sar","text":"sar - Collect, report, or save system activity information. sar provides historical stats on disk utilization for block devices and CPU. Display CPU stats: sar | head Display block devices stats: sar -d Note: sar command is provided by sysstat package (provides iostat aswell). When you install sysstat you have to start and enable it. When you start sysstat utility it creates a file under /var/log/sa/sa[num] . This is where it stores all the stats for every 10mins. [num] two digit number represents the date. It will be recycled every 30days . /var/log/sa/sa[num] is binay formatted. sar is desined to read this file and generate reports.","title":"sar"},{"location":"linux/Measure and Troubleshoot Resource Utilization/#memory-free","text":"free Display amount of free and used memory in the system. free -m -h -s 1 Display the output in MB / Shorten / Display the results for seconds until three iterations. free -m [v01@equatescrates ~]$ free -m total used free shared buffers cached Mem: 3705 807 2897 0 61 438 -/+ buffers/cache: 306 3398 Swap: 0 0 0 According to above output we have only have 2897 mb of memory free. But, actual available free memory is free(2897) + cached(438) + buffers(61) = 3398 Major Page Fault: is a request done to fetch pages from the hard disk and buffer it to RAM. Minor Page Fault: If the data is present in the RAM buffer cache, the processor can issue a Minor Page fault. https://www.slashroot.in/linux-system-io-monitoring article explains the logic behind the free command.","title":"Memory: free"},{"location":"linux/Measure and Troubleshoot Resource Utilization/#memory-vmstat","text":"vmstat provides full summary of all memory utilization (idle memory/buffer memory etc..) on our system. procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 1991612 262060 871784 0 0 35 30 178 263 2 1 95 2 0 Display active/inactive memory: vmstat -a Active Memory: The memory that is being used by a particular process. Inactive Memory: The pages which have not been accessed \"recently\". Run every 5 seconds: vmstat 5 Run every 5 seconds with a timestamp: vmstat 5 -t Display a table of various event counters and memory statistics: vmstat -s","title":"Memory: vmstat"},{"location":"linux/Measure and Troubleshoot Resource Utilization/#disk-and-files-lsof","text":"Example: Let's say you have identified busy disk or high cpu utilization using above discussed command. To find what files, utilities or applications are open and might be using that. We can look at that by system and end users. Display number of file currently open: lsof | wc -l Display number of files that are opened by a particular user: lsof -u [user] | wc -l","title":"Disk and Files: lsof"},{"location":"linux/Measure and Troubleshoot Resource Utilization/#ps-pstree-top","text":"How do I get detailed information about process running on our system? Display all running processes on the system: ps -e Diplay full format: ps -ef Display the process hierarchy: pstree Display the process hierarchy and highlight the current process: pstree -h Display the process hierarchy along with pid's and highlight the current process: pstree -h -p Display the process hierarchy along with pid's and highlight the current process along with command line parameters: pstree -h -p -a Note: All of the commands are great. But they are not interactive. top command used to get process information in an interactive way. Before that we need to understand what a load average is? and what are tasks? Notes for top command : There are flags that are used to change the behaviour of the top command. Search online for more flags. Press \"f\" to add or remove fields \"shift + R\" to sort it in reverse, the process with less cpu time first. Press \"n\" then 5 for top five process. Press \"d\" to change the update duration. Press \"k\" to kill and enter the \"PID\" number to kill a process. You can pass any valid linux kill signal (default is 15).","title":"ps, pstree, top"},{"location":"linux/Measure and Troubleshoot Resource Utilization/#iotop-ss-netstat","text":"ss command is similar to netstat, the ss command is used to dump the socket statistics. netstat uses /proc/net/tcp to gather system network information, while ss queries the kernel directly using the Netlink socket interface.s iotop: The top command shows the Memory and CPU usage for open programs, iotop shows disk usage information for open processes. To run on non-interactive mode: iotop -b (or) iotop --batch To set the number of interactions before quitting: iotop -N 10 (or) iotop --iter=10 To set the delay between interactions: iotop -d 10 (or) iotop --delay=10 To monitor particular processes: iotop -p 1234 2345 3456 (or) iotop --pid=1234 To monitor a list of users: iotop -u mike mark (or) iotop --user=mike To show only processes: iotop -P (or) iotop --processes To Show accumulated I/O instead of bandwidth: iotop -a (or) iotop --accumulated To Use kilobytes instead of a human friendly unit: iotop -k (or) iotop --kilobytes To add a time stamp on each line: iotop -t (or) iotop --time To suppress some lines of header: iotop -q (or) iotop --quiet","title":"iotop, ss, netstat"},{"location":"networking/Linux Networking/","text":"Linux Networking and Troubleshooting Using the Network Environment Interface Tools (ip,nmcli etc..) MACAddress Unique fingerprint of the network interface IP Address Unique address on the network Subnet Separate the IP into network and host addresses Gateway The connection leading outside of the local network DNS Host Translate the host name into IP address DNS Domain The lookup for the host","title":"Networking"},{"location":"networking/Linux Networking/#linux-networking-and-troubleshooting","text":"","title":"Linux Networking and Troubleshooting"},{"location":"networking/Linux Networking/#using-the-network-environment-interface-tools-ipnmcli-etc","text":"MACAddress Unique fingerprint of the network interface IP Address Unique address on the network Subnet Separate the IP into network and host addresses Gateway The connection leading outside of the local network DNS Host Translate the host name into IP address DNS Domain The lookup for the host","title":"Using the Network Environment Interface Tools (ip,nmcli etc..)"},{"location":"networking/TCP connection backlog/","text":"Linux TCP Backlogs References: https://bunn.cc/2017/syn-backlog/ https://blog.cloudflare.com/syn-packet-handling-in-the-wild/ https://eklitzke.org/how-tcp-sockets-work Key Points 1 - The incoming connections get queued up, waiting for the application to accept(2) them. 2 - The queue that holds established connections is (of course) of finite length. 3 - Each bound socket, in the \"LISTENING\" TCP state has two separate queues. The SYN Queue: The SYN Queue stores inbound SYN packets. It's responsible for sending out SYN+ACK packets and retrying them on timeout. On Linux the number of retries is configured with: sysctl net.ipv4.tcp_synack_retries The Accept Queue: The Accept Queue contains fully established connections and ready to be picked up by the application. What will happen if Accept Queue is full? If the TCP implementation in Linux receives the ACK packet of the 3-way handshake and the accept queue is full, it will basically ignore that packet. Note: There is a timer associated with the SYN RECEIVED state. If the ACK packet is not received (or if it is ignored, as in the case considered here), then the TCP implementation will resend the SYN/ACK packet (with a certain number of retries specified by /proc/sys/net/ipv4/tcp_synack_retries and using an exponential backoff algorithm). After retries it will sent RST packet. What is Half Open Connections? If the client first waits for data from the server and the server never reduces the backlog, then the end result is that on the client side, the connection is in state ESTABLISHED, while on the server side, the connection is considered CLOSED. This means that we end up with a half-open connection! Note: If the accept queue is full, then the kernel will impose a limit on the rate at which SYN packets are accepted. If too many SYN packets are received, some of them will be dropped. Queue size limits The maximum allowed length of both the Accept and SYN Queues is taken from the backlog parameter passed to the listen(2) syscall by the application. For example, this sets the Accept and SYN Queue sizes to 1,024: listen(sfd, 1024) This SYN Queue cap used to be configured by the net.ipv4.tcp_max_syn_backlog toggle, but this isn't the case anymore. Nowadays net.core.somaxconn caps both queue sizes. Troubleshooting: To peek into the SYN Queue on Linux we can use the ss command and look for SYN-RECV sockets. $ ss -n state syn-recv sport = :80 | wc -l 119 $ ss -n state syn-recv sport = :443 | wc -l 78 Note: don't confuse Recv-Q and Send-Q fields with the SYN Queue and ACCEPT Queue. SYN Queue and ACCEPT Queues are used per application. Recv-Q and Send-Q are used per TCP connection.","title":"TCP Backlogs"},{"location":"networking/TCP connection backlog/#linux-tcp-backlogs","text":"References: https://bunn.cc/2017/syn-backlog/ https://blog.cloudflare.com/syn-packet-handling-in-the-wild/ https://eklitzke.org/how-tcp-sockets-work","title":"Linux TCP Backlogs"},{"location":"networking/TCP connection backlog/#key-points","text":"1 - The incoming connections get queued up, waiting for the application to accept(2) them. 2 - The queue that holds established connections is (of course) of finite length. 3 - Each bound socket, in the \"LISTENING\" TCP state has two separate queues. The SYN Queue: The SYN Queue stores inbound SYN packets. It's responsible for sending out SYN+ACK packets and retrying them on timeout. On Linux the number of retries is configured with: sysctl net.ipv4.tcp_synack_retries The Accept Queue: The Accept Queue contains fully established connections and ready to be picked up by the application.","title":"Key Points"},{"location":"networking/TCP connection backlog/#what-will-happen-if-accept-queue-is-full","text":"If the TCP implementation in Linux receives the ACK packet of the 3-way handshake and the accept queue is full, it will basically ignore that packet. Note: There is a timer associated with the SYN RECEIVED state. If the ACK packet is not received (or if it is ignored, as in the case considered here), then the TCP implementation will resend the SYN/ACK packet (with a certain number of retries specified by /proc/sys/net/ipv4/tcp_synack_retries and using an exponential backoff algorithm). After retries it will sent RST packet.","title":"What will happen if Accept Queue is full?"},{"location":"networking/TCP connection backlog/#what-is-half-open-connections","text":"If the client first waits for data from the server and the server never reduces the backlog, then the end result is that on the client side, the connection is in state ESTABLISHED, while on the server side, the connection is considered CLOSED. This means that we end up with a half-open connection! Note: If the accept queue is full, then the kernel will impose a limit on the rate at which SYN packets are accepted. If too many SYN packets are received, some of them will be dropped.","title":"What is Half Open Connections?"},{"location":"networking/TCP connection backlog/#queue-size-limits","text":"The maximum allowed length of both the Accept and SYN Queues is taken from the backlog parameter passed to the listen(2) syscall by the application. For example, this sets the Accept and SYN Queue sizes to 1,024: listen(sfd, 1024) This SYN Queue cap used to be configured by the net.ipv4.tcp_max_syn_backlog toggle, but this isn't the case anymore. Nowadays net.core.somaxconn caps both queue sizes.","title":"Queue size limits"},{"location":"networking/TCP connection backlog/#troubleshooting","text":"To peek into the SYN Queue on Linux we can use the ss command and look for SYN-RECV sockets. $ ss -n state syn-recv sport = :80 | wc -l 119 $ ss -n state syn-recv sport = :443 | wc -l 78 Note: don't confuse Recv-Q and Send-Q fields with the SYN Queue and ACCEPT Queue. SYN Queue and ACCEPT Queues are used per application. Recv-Q and Send-Q are used per TCP connection.","title":"Troubleshooting:"},{"location":"web/http/","text":"Fragments and Queries in HTTP url: Example: https://www.yahoo.com/search?q=tesla%20research Query: search?q=tesla%20research Example: https://www.yahoo.com/search?q=tesla%20research#space%20research Query: #space research Which means under \"q=tesla%20research\" resource scroll to \"#space\" section. HTTP Request Methods: [GET] Retreive a resource [POST] Update a resource [PUT] Store a resource [DELETE] Remove a resource [HEAD] Reterive the headers for a resource telnet yahoo.com 80 GET /yahoo.com/yahoomail.jpg HTTP/1.1 Host:yahoo.com When to use PUT and POST? The PUT method completely replaces whatever currently exists at the target URL with something else. With this method, you can create a new resource or overwrite an existing one given you know the exact Request-URI. In short, the PUT method is used to create or overwrite a resource at a particular URL that is known by the client. PUT method is idempotent because no matter how many times we send the same request, the results will always be the same. On the other hand, the POST method is not idempotent since if we send the same POST request multiple times, we will receive various results. When you know the URL of the thing you want to create or overwrite, a PUT method should be used. Alternatively, if you only know the URL of the category or sub-section of the thing you want to create something within, use the POST method. Safe and Unsafe: Safe - let's you to read and view resources. Unsafe - let's you to modify the resources. Post/Redirect/Get Operation: To avoid duplicate post requests signup (GET signup) signup page with forms (POST and update the data and REDIRECT to signed page) signed http Request Format: [method] [url] [version] [GET] [http://server.com/articles/741.html] [HTTP/1.1] [headers] [HOST]: [yahoo.com] [headers] [Accept-Language]: [fr-FR] [headers] [Date]: [Fri, 10 Aug 2002 21:12:00 GMT] Common Request Header: Referrer The URL of the referring page. The page where the URL originated. User-Agent Information about the browser. Accept Preffered media types. Accept-Language Preferred language. Cookie Cookie information. If-Modified-Since Date of last retrieval. Used for caching. Date Creation timestamp for the message. http Response Format: [version] [status] [reason] [HTTP/1.1] [200] [ok] [headers] [Server]: [nginx] [headers] [Content-Type]: [text/html] [body] Status Codes: [100-199] Informational [200-299] Successful [300-309] Redirection [400-499] Client Error [500-599] Server Error [200] [OK] Success. [301] [Moved Permanently] Resource Moved, don't check here again. [302] [Moved Temporarily] Resource Moved, but check here again. (Post Redirect Get Mechanism) [304] [Not Modified] Resource hasn't changed since last retrieval. [400] [Bad Request] Bad Syntax. [401] [Unauthorized] Client might need to authenticate. [403] [Forbidden] Refused Access. [404] [Not found] Resource doesn't exist. [500] [Internal server error] Something went wrong during processing. [503] [Service unavailable] Server will not service the request. HTTP Connections How does the messages actualy moves in the network? When are the network connections opened? When are the network connections closed? http (Application Layer) - tcp (Transport Layer) http Layer (Browser): 1 - Extract the host name and port number from the URL \"http://mail.yahoo.com/q?s=^mail\". 2 - Creates an HTTP socket and start writing the data to the socket. TCP Transport Layer: 1 - Accepts the data and ensures the data is getting delivered to the server without getting lost or duplicated. Error detection / flow control and takes care of the data reliablity. IP Network Layer: 1 - Responsible for taking these information and moving them in the network switch/router/gateway etc. 2 - IP is responsible for delivering the data to destination but doesn't gaurentee the delivery (TCP's job). Datalink Layer: 1 - Ethernet frames. TCP Handshake: Before starting the actual transmission, there is a 3 steps process followed to make sure the server and the client are in agreement to transfer the data. [SYN] Seq=0 [SYN, ACK] Seq=0 Ack=1 [ACK] Seq=1 Ack=1 Persistent Connections: Default type of connection in HTTP/1.1 1 - Persistent connection stays open after completion of one request response transaction. 2 - There is always a downside, each server has a limit in number of persistent connections as a security measure. 3 - Attackes will perform DOS attacks by opening number of persistent connections and makes servers un-responsive. 4 - Since servers only accepts only finite number of persistent connections, servers are configured to close the connections after certain intervals (if idle). Note: The server which does not allow the persistent connection must include a http connection header called \"Connection: close\" which will not allow the client to make another request on the same request. It has to re-open a new connection. Parallel connections: Making 2 different connection parallely at the same time. The server will return \"Connection: close\" header. Persistent Connections: Making more than one req/res transaction in a single connection. Proxies: Forward and Reverse Proxy: Forward Proxy forwards the client requests to the internet. Example: Specific set of users (clients) can access twitter from company via the Forward server. Reverse Proxy: Reverse Proxy sits at the server end accepts the request from internet and forwards them to servers (example: load balancing). Cache Controls: With HTTP/1.1 clients and proxies generally cache the response with 200 ok response code. (response to the http get request) It will not cache PUT, POST and DELETE transaction. Note: Application server can influence this cache settings by using appropriate cache headers. \"Cache-Control: private, max-age=0\" Cache-Control: public Public proxy servers can cache the response. Cache-Control: private Response targeted to single user, only web browser. Cache-Control: no-cache Should not be cached. Cache-Control: no-store HTTP Security: The stateful and stateless web: HTTP is designed as a stateless protocol, each request/response transaction is independent of any previous or future transactions. cookie are used for tracking / differentiate one user from another user. Non-Persistent Cookies (Session Cookies): These cookies will be used only for particular session. Example: set-cookie: GUID=07hfhjebhbwb76, domain=.search.yahoo.com, // Send this cookie only to yahoo.com and not to other websites. path=/ // Restrict cookie to specific path. Persistent Cookies (Will have a validity and stored to client filesystem). set-cookie: GUID=07hfhjebhbwb76, domain=.search.yahoo.com, path=/, expires=Monday, 09-July-2019 21:20:00 GMT Authentication Types: Basic Type: 1. GET /account HTTP/1.1 Host: starlingbank.com ... 2. HTPP/1.1 401 Unauthorized WWW-Authenticate: Basic realm=\"starlingbank.com\" 3. GET /account HTTP/1.1 Host: starlingbank.com Authorization: Basic Z3fnjnjnsflr \"Authorization\" [header type] \"Basic\" [AUTH type] \"Z3fnjnjnsflr\" [base64 encoded value of username and password] Digest: uses md5 digets instead of base64 Form Authentication: When you try to access a secured resource, the user will be temporarily redirected to a web page with authentication form. If authentication is successful, the user will be redirected again to the secured resource. Open ID","title":"HTTP"},{"location":"web/http/#fragments-and-queries-in-http-url","text":"Example: https://www.yahoo.com/search?q=tesla%20research Query: search?q=tesla%20research Example: https://www.yahoo.com/search?q=tesla%20research#space%20research Query: #space research Which means under \"q=tesla%20research\" resource scroll to \"#space\" section.","title":"Fragments and Queries in HTTP url:"},{"location":"web/http/#http-request-methods","text":"[GET] Retreive a resource [POST] Update a resource [PUT] Store a resource [DELETE] Remove a resource [HEAD] Reterive the headers for a resource telnet yahoo.com 80 GET /yahoo.com/yahoomail.jpg HTTP/1.1 Host:yahoo.com When to use PUT and POST? The PUT method completely replaces whatever currently exists at the target URL with something else. With this method, you can create a new resource or overwrite an existing one given you know the exact Request-URI. In short, the PUT method is used to create or overwrite a resource at a particular URL that is known by the client. PUT method is idempotent because no matter how many times we send the same request, the results will always be the same. On the other hand, the POST method is not idempotent since if we send the same POST request multiple times, we will receive various results. When you know the URL of the thing you want to create or overwrite, a PUT method should be used. Alternatively, if you only know the URL of the category or sub-section of the thing you want to create something within, use the POST method.","title":"HTTP Request Methods:"},{"location":"web/http/#safe-and-unsafe","text":"Safe - let's you to read and view resources. Unsafe - let's you to modify the resources.","title":"Safe and Unsafe:"},{"location":"web/http/#postredirectget-operation-to-avoid-duplicate-post-requests","text":"signup (GET signup) signup page with forms (POST and update the data and REDIRECT to signed page) signed","title":"Post/Redirect/Get Operation: To avoid duplicate post requests"},{"location":"web/http/#http-request-format","text":"[method] [url] [version] [GET] [http://server.com/articles/741.html] [HTTP/1.1] [headers] [HOST]: [yahoo.com] [headers] [Accept-Language]: [fr-FR] [headers] [Date]: [Fri, 10 Aug 2002 21:12:00 GMT]","title":"http Request Format:"},{"location":"web/http/#common-request-header","text":"Referrer The URL of the referring page. The page where the URL originated. User-Agent Information about the browser. Accept Preffered media types. Accept-Language Preferred language. Cookie Cookie information. If-Modified-Since Date of last retrieval. Used for caching. Date Creation timestamp for the message.","title":"Common Request Header:"},{"location":"web/http/#http-response-format","text":"[version] [status] [reason] [HTTP/1.1] [200] [ok] [headers] [Server]: [nginx] [headers] [Content-Type]: [text/html] [body]","title":"http Response Format:"},{"location":"web/http/#status-codes","text":"[100-199] Informational [200-299] Successful [300-309] Redirection [400-499] Client Error [500-599] Server Error [200] [OK] Success. [301] [Moved Permanently] Resource Moved, don't check here again. [302] [Moved Temporarily] Resource Moved, but check here again. (Post Redirect Get Mechanism) [304] [Not Modified] Resource hasn't changed since last retrieval. [400] [Bad Request] Bad Syntax. [401] [Unauthorized] Client might need to authenticate. [403] [Forbidden] Refused Access. [404] [Not found] Resource doesn't exist. [500] [Internal server error] Something went wrong during processing. [503] [Service unavailable] Server will not service the request.","title":"Status Codes:"},{"location":"web/http/#http-connections","text":"How does the messages actualy moves in the network? When are the network connections opened? When are the network connections closed? http (Application Layer) - tcp (Transport Layer)","title":"HTTP Connections"},{"location":"web/http/#http-layer-browser","text":"1 - Extract the host name and port number from the URL \"http://mail.yahoo.com/q?s=^mail\". 2 - Creates an HTTP socket and start writing the data to the socket.","title":"http Layer (Browser):"},{"location":"web/http/#tcp-transport-layer","text":"1 - Accepts the data and ensures the data is getting delivered to the server without getting lost or duplicated. Error detection / flow control and takes care of the data reliablity.","title":"TCP Transport Layer:"},{"location":"web/http/#ip-network-layer","text":"1 - Responsible for taking these information and moving them in the network switch/router/gateway etc. 2 - IP is responsible for delivering the data to destination but doesn't gaurentee the delivery (TCP's job).","title":"IP Network Layer:"},{"location":"web/http/#datalink-layer","text":"1 - Ethernet frames.","title":"Datalink Layer:"},{"location":"web/http/#tcp-handshake","text":"Before starting the actual transmission, there is a 3 steps process followed to make sure the server and the client are in agreement to transfer the data. [SYN] Seq=0 [SYN, ACK] Seq=0 Ack=1 [ACK] Seq=1 Ack=1","title":"TCP Handshake:"},{"location":"web/http/#persistent-connections-default-type-of-connection-in-http11","text":"1 - Persistent connection stays open after completion of one request response transaction. 2 - There is always a downside, each server has a limit in number of persistent connections as a security measure. 3 - Attackes will perform DOS attacks by opening number of persistent connections and makes servers un-responsive. 4 - Since servers only accepts only finite number of persistent connections, servers are configured to close the connections after certain intervals (if idle). Note: The server which does not allow the persistent connection must include a http connection header called \"Connection: close\" which will not allow the client to make another request on the same request. It has to re-open a new connection. Parallel connections: Making 2 different connection parallely at the same time. The server will return \"Connection: close\" header. Persistent Connections: Making more than one req/res transaction in a single connection.","title":"Persistent Connections: Default type of connection in HTTP/1.1"},{"location":"web/http/#proxies","text":"Forward and Reverse Proxy: Forward Proxy forwards the client requests to the internet. Example: Specific set of users (clients) can access twitter from company via the Forward server.","title":"Proxies:"},{"location":"web/http/#reverse-proxy","text":"Reverse Proxy sits at the server end accepts the request from internet and forwards them to servers (example: load balancing).","title":"Reverse Proxy:"},{"location":"web/http/#cache-controls","text":"With HTTP/1.1 clients and proxies generally cache the response with 200 ok response code. (response to the http get request) It will not cache PUT, POST and DELETE transaction. Note: Application server can influence this cache settings by using appropriate cache headers. \"Cache-Control: private, max-age=0\" Cache-Control: public Public proxy servers can cache the response. Cache-Control: private Response targeted to single user, only web browser. Cache-Control: no-cache Should not be cached. Cache-Control: no-store","title":"Cache Controls:"},{"location":"web/http/#http-security","text":"The stateful and stateless web: HTTP is designed as a stateless protocol, each request/response transaction is independent of any previous or future transactions. cookie are used for tracking / differentiate one user from another user. Non-Persistent Cookies (Session Cookies): These cookies will be used only for particular session. Example: set-cookie: GUID=07hfhjebhbwb76, domain=.search.yahoo.com, // Send this cookie only to yahoo.com and not to other websites. path=/ // Restrict cookie to specific path. Persistent Cookies (Will have a validity and stored to client filesystem). set-cookie: GUID=07hfhjebhbwb76, domain=.search.yahoo.com, path=/, expires=Monday, 09-July-2019 21:20:00 GMT","title":"HTTP Security:"},{"location":"web/http/#authentication-types","text":"Basic Type: 1. GET /account HTTP/1.1 Host: starlingbank.com ... 2. HTPP/1.1 401 Unauthorized WWW-Authenticate: Basic realm=\"starlingbank.com\" 3. GET /account HTTP/1.1 Host: starlingbank.com Authorization: Basic Z3fnjnjnsflr \"Authorization\" [header type] \"Basic\" [AUTH type] \"Z3fnjnjnsflr\" [base64 encoded value of username and password] Digest: uses md5 digets instead of base64","title":"Authentication Types:"},{"location":"web/http/#form-authentication","text":"When you try to access a secured resource, the user will be temporarily redirected to a web page with authentication form. If authentication is successful, the user will be redirected again to the secured resource. Open ID","title":"Form Authentication:"},{"location":"web/nginx/","text":"Notes: The basic nginx architecture consists of a master process and its workers. Developed to solve c10k problems. Which means handling 10,000 concurrent connections. Applications: High Performance Web Server Reverse Proxy (SSL Termination and Contnet Caching and Termination) Load Balancer The master is supposed to read the configuration file and maintain worker processes, while workers do the actual processing of requests. References: To get info about core context and directive blocks, go to http://nginx.org/en/docs/ngx_core_module.html Commands: While your nginx instance is running, you can manage it by sending signals: sudo nginx -s signal stop: fast shutdown quit: graceful shutdown (wait for workers to finish their processes) reload: reload the configuration file reopen: reopen the log files To verify configuration: nginx -t To view selinux context: semanage fcontext -l | grep -i /usr/share/nginx/html To add selinux context: semanage fcontext -a -t httpd_sys_content_t '/var/www' To restore context back to default: restorecon -R -v '/var/www' Curl with host header: curl --header \"Host: www.example.com\" localhost Configuration files location: /etc/nginx/nginx.conf /usr/local/etc/nginx/nginx.conf /usr/local/nginx/conf/nginx.conf Directive and Context: Directive: The option that consists of name and parameters; it should end with a semicolon Example: gzip on; Context: Section where you can declare directives (similar to scope in programming languages) Example: http { # http context gzip on; # directive in http context } Directive types: You have to pay attention when using the same directive in multiple contexts, as the inheritance model differs for different directives. There are 3 types of directives, each with its own inheritance model. Normal: Has one value per context. Also, it can be defined only once in the context. Subcontexts can override the parent directive, but this override will be valid only in a given subcontext. gzip on; gzip off; # illegal to have 2 normal directives in same context server { # server context location /downloads { # location subcontext under server context } location /assets { } } Array: Adding multiple directives in the same context will add to the values instead of overwriting them altogether. Defining a directive in a subcontext will override ALL parent values in the given subcontext. error_log /var/log/nginx/error.log; error_log /var/log/nginx/error_native.log notice; error_log /var/log/nginx/error_debug.log debug; server { location /downloads { # this will override all the parent directives error_log /var/log/nginx/error_downloads.log; } } Action: Actions are directives that change things. Their inheritance behaviour will depend on the module. For example, in the case of the rewrite directive, every matching directive will be executed: server { rewrite ^ /foobar; location /foobar { rewrite ^ /foo; rewrite ^ /bar; } } Custom Error Pages: Syntax: error_page list of error codes error page Example: error_page 404 /404.html Basic Auth: https://nginx.org/en/docs/http/ngx_http_auth_basic_module.html location /admin.html { auth_basic \"Login Required\"; auth_basic_user_file /etc/nginx/.htpasswd; } nginx SSL: https://nginx.org/en/docs/http/ngx_http_ssl_module.html Self Singed Certs: mkdir /etc/nginx/ssl openssl -req x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/nginx/ssl/keys/private.key -out /etc/nginx/ssl/public.pem req - We\u2019re making a certificate request to OpenSSL -x509 - Specifying the structure that our certificate should have. Conforms to the X.509 standard -nodes - Do not encrypt the output key -days 365 - Set the key to be valid for 365 days -newkey rsa:2048 - Generate an RSA key that is 2048 bits in size -keyout /etc/nginx/ssl/private.key - File to write the private key to -out /etc/nginx/ssl/public.pem - Output file for public portion of key server { listen 443 ssl; root /usr/share/nginx/html; ssl_certificate /etc/nginx/ssl/home.v01.openhouse.cert.pem; ssl_certificate_key /etc/nginx/ssl/keys/home.v01.openhouse.key.pem; server_name _; location = /admin.html { auth_basic \"Login Required\"; auth_basic_user_file /etc/nginx/.htpasswd; } error_page 404 /404.html; }","title":"Nginx"},{"location":"web/nginx/#notes","text":"The basic nginx architecture consists of a master process and its workers. Developed to solve c10k problems. Which means handling 10,000 concurrent connections.","title":"Notes:"},{"location":"web/nginx/#applications","text":"High Performance Web Server Reverse Proxy (SSL Termination and Contnet Caching and Termination) Load Balancer The master is supposed to read the configuration file and maintain worker processes, while workers do the actual processing of requests.","title":"Applications:"},{"location":"web/nginx/#references","text":"To get info about core context and directive blocks, go to http://nginx.org/en/docs/ngx_core_module.html","title":"References:"},{"location":"web/nginx/#commands","text":"While your nginx instance is running, you can manage it by sending signals: sudo nginx -s signal stop: fast shutdown quit: graceful shutdown (wait for workers to finish their processes) reload: reload the configuration file reopen: reopen the log files To verify configuration: nginx -t To view selinux context: semanage fcontext -l | grep -i /usr/share/nginx/html To add selinux context: semanage fcontext -a -t httpd_sys_content_t '/var/www' To restore context back to default: restorecon -R -v '/var/www' Curl with host header: curl --header \"Host: www.example.com\" localhost","title":"Commands:"},{"location":"web/nginx/#configuration-files-location","text":"/etc/nginx/nginx.conf /usr/local/etc/nginx/nginx.conf /usr/local/nginx/conf/nginx.conf","title":"Configuration files location:"},{"location":"web/nginx/#directive-and-context","text":"Directive: The option that consists of name and parameters; it should end with a semicolon Example: gzip on; Context: Section where you can declare directives (similar to scope in programming languages) Example: http { # http context gzip on; # directive in http context }","title":"Directive and Context:"},{"location":"web/nginx/#directive-types","text":"You have to pay attention when using the same directive in multiple contexts, as the inheritance model differs for different directives. There are 3 types of directives, each with its own inheritance model.","title":"Directive types:"},{"location":"web/nginx/#normal","text":"Has one value per context. Also, it can be defined only once in the context. Subcontexts can override the parent directive, but this override will be valid only in a given subcontext. gzip on; gzip off; # illegal to have 2 normal directives in same context server { # server context location /downloads { # location subcontext under server context } location /assets { } }","title":"Normal:"},{"location":"web/nginx/#array","text":"Adding multiple directives in the same context will add to the values instead of overwriting them altogether. Defining a directive in a subcontext will override ALL parent values in the given subcontext. error_log /var/log/nginx/error.log; error_log /var/log/nginx/error_native.log notice; error_log /var/log/nginx/error_debug.log debug; server { location /downloads { # this will override all the parent directives error_log /var/log/nginx/error_downloads.log; } }","title":"Array:"},{"location":"web/nginx/#action","text":"Actions are directives that change things. Their inheritance behaviour will depend on the module. For example, in the case of the rewrite directive, every matching directive will be executed: server { rewrite ^ /foobar; location /foobar { rewrite ^ /foo; rewrite ^ /bar; } }","title":"Action:"},{"location":"web/nginx/#custom-error-pages","text":"Syntax: error_page list of error codes error page Example: error_page 404 /404.html","title":"Custom Error Pages:"},{"location":"web/nginx/#basic-auth","text":"https://nginx.org/en/docs/http/ngx_http_auth_basic_module.html location /admin.html { auth_basic \"Login Required\"; auth_basic_user_file /etc/nginx/.htpasswd; } nginx SSL: https://nginx.org/en/docs/http/ngx_http_ssl_module.html","title":"Basic Auth:"},{"location":"web/nginx/#self-singed-certs","text":"mkdir /etc/nginx/ssl openssl -req x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/nginx/ssl/keys/private.key -out /etc/nginx/ssl/public.pem req - We\u2019re making a certificate request to OpenSSL -x509 - Specifying the structure that our certificate should have. Conforms to the X.509 standard -nodes - Do not encrypt the output key -days 365 - Set the key to be valid for 365 days -newkey rsa:2048 - Generate an RSA key that is 2048 bits in size -keyout /etc/nginx/ssl/private.key - File to write the private key to -out /etc/nginx/ssl/public.pem - Output file for public portion of key server { listen 443 ssl; root /usr/share/nginx/html; ssl_certificate /etc/nginx/ssl/home.v01.openhouse.cert.pem; ssl_certificate_key /etc/nginx/ssl/keys/home.v01.openhouse.key.pem; server_name _; location = /admin.html { auth_basic \"Login Required\"; auth_basic_user_file /etc/nginx/.htpasswd; } error_page 404 /404.html; }","title":"Self Singed Certs:"},{"location":"web/notes/","text":"What is HTTP Keep Alive? https://www.imperva.com/learn/performance/http-keep-alive/","title":"Notes"},{"location":"web/notes/#what-is-http-keep-alive","text":"https://www.imperva.com/learn/performance/http-keep-alive/","title":"What is HTTP Keep Alive?"}]}